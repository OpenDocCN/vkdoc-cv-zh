# 1.机器学习导论

机器学习被定义为通过机器执行一项任务的一套技术，而这项任务并不是明确为它编写的。它有时被视为动态编程的子集。如果你以前有过一些传统编程的经验，你就会知道构建一个软件需要明确地为机器提供一组明确的指令，这些指令可以顺序或并行执行，以完成某项任务。如果你的软件的目的是计算购买的佣金，或者向用户显示一个仪表板，或者向一个连接的设备读写数据，这就很好了。这些类型的问题通常涉及有限数量的明确定义的步骤，以便执行它们的任务。然而，如果你的软件的任务是识别一张图片是否包含一只猫呢？即使您构建了一个软件，能够在一些特定的样本图片上正确识别猫的形状(例如，通过检查样本图片中的一些特定像素是否到位)，如果您为该软件提供不同的猫图片，或者甚至是您自己的样本图片的稍微编辑版本，该软件也可能无法执行其任务。如果你必须开发一个软件来检测垃圾邮件呢？当然，你可能仍然可以用传统的编程来做到这一点——例如，你可以建立一个垃圾邮件中常见的大量单词或短语的列表——但是如果你的软件提供了与你的列表中相似但不在你的列表中的单词，那么它可能会失败。

后一类包括传统上人类被认为比机器更擅长完成的任务:在执行有限的步骤序列甚至解决高等数学问题方面，机器比人快一百万倍，但它会可耻地失败(至少在传统编程中)，无法分辨某张图片是描绘猫还是交通灯。在这些任务中，人类的大脑通常比机器更好，因为他们已经接触了很多年的例子和基于感觉的经验。我们可以在几分之一秒内判断出一张照片中是否有一只猫，即使没有对所有可能的品种、它们的特征和它们所有可能的姿势的完整经验。这是因为我们可能以前见过其他的猫，我们可以很快执行一个心理*分类*的过程，将图片的主题标记为我们过去已经见过的东西。换句话说，我们的大脑经过多年的*训练*，或*连线*，变得非常擅长识别模糊世界中的*模式*，而不是在虚拟世界中快速执行一系列复杂但确定的任务。

机器学习是一套试图模仿我们大脑执行任务的方式的技术——通过反复试验，直到我们可以从获得的经验中推断出模式，而不是通过明确的步骤声明。

有必要快速澄清一下*机器学习*和*人工智能* (AI)之间的区别。虽然这两个术语今天经常被用作同义词，但机器学习是一套技术，通过接触(通常是许多)例子，可以指导机器解决它没有专门编程的问题。人工智能是一个更广泛的分类，包括任何擅长执行通常人类更擅长的任务的机器或算法，或者根据一些人的说法，显示某种形式的类人智能的任务。人工智能的实际定义实际上非常模糊——一些人可能会认为，能够检测到图片中的物体或两个城市之间的最短路径是否真的是一种“智能”——机器学习可能只是实现这一点的一种可能工具(例如，专家系统在 21 世纪初非常流行)。因此，通过这本书，我通常会谈论工具(机器学习算法)，而不是这种算法可能要实现的哲学目标(人工智能)。

在我们深入研究机器学习的具体细节之前，可能有必要提供一些背景和历史，以了解该学科多年来是如何发展的，以及我们现在所处的位置。

## 1.1 历史

尽管机器学习在过去的十年里非常流行，但它的存在时间可能和数字计算机一样长。建造一台能够模仿人类行为和特征及其所有细微差别的机器的梦想比计算机科学本身还要古老。然而，在经历今天的爆炸之前，这门学科在过去的半个世纪里经历了一系列的起伏。

今天最流行的机器学习技术利用了唐纳德·赫布在 1949 年首次理论化的概念[1]。在他的书《行为的组织》中，他首次提出理论，认为人类大脑中的神经元通过加强或削弱它们的相互连接来对外部环境的刺激做出反应。赫布写道，*“*当一个细胞重复协助另一个细胞放电时，第一个细胞的轴突在与第二个细胞的胞体接触时，会发展出突触旋钮(或者扩大它们，如果它们已经存在的话)。”这种模型(fire together，wire together)激发了人们对如何构建一个人工神经元的研究，该人工神经元可以通过动态调整其与其他神经元的链接的*权重*(*突触*)来响应其收集的经验。这个概念是现代神经网络背后的理论基础。

一年后的 1950 年，著名的英国数学家(计算机科学之父)艾伦·图灵提出了可能是第一个已知的人工智能定义。他提出了一个实验，要求一个人与藏在屏幕后面的“某人/某物”进行对话。如果在对话结束时，受试者不能说出他/她是与人还是与机器交谈，那么机器就通过了“人工智能”测试。这种测试如今被称为著名的图灵测试。

1951 年，Christopher Strachey 编写了一个可以下跳棋的程序，Dietrich Prinz 编写了一个可以下国际象棋的程序。后来在 20 世纪 50 年代的改进导致了可以有效挑战业余选手的程序的发展。这样的早期发展导致游戏经常被用作衡量机器学习进展的标准基准——直到 IBM 的深蓝在国际象棋中击败卡斯帕罗夫，AlphaGo 在围棋中击败李·塞多尔。

与此同时，20 世纪 50 年代中期数字计算机的出现引领了一股乐观主义浪潮，这就是所谓的象征性人工智能。一些研究人员认识到，可以处理数字的机器也可以处理符号，如果符号是人类思维的基础，那么就有可能设计出会思考的机器。1955 年，艾伦·纽厄尔和未来的诺奖得主司马贺创造了*逻辑理论家*，一个可以通过推理证明数学定理的程序给出了一组逻辑公理。它成功地证明了伯特兰·罗素的*数学原理*的前 52 个定理中的 38 个。

这样的理论背景导致了早期研究者的热情。它引发了乐观情绪，并在 1956 年达特茅斯学院举行的研讨会上达到高潮，一些学者预测，与人类一样智能的机器将在一代人内出现，并获得数百万美元来实现这一愿景。今天，这个会议被认为是人工智能作为一门学科的基础。

1957 年，弗兰克·罗森布拉特设计了*感知机*。他应用 Hebb 的神经模型设计了一台可以进行图像识别的机器。该软件最初是为 IBM 704 设计的，安装在一台名为 *Mark 1 感知器*的定制机器上。它的主要目标是从照片中识别特征——尤其是面部特征。感知器在功能上就像一个单个神经元，可以从提供的例子中*学习*(即调整其突触权重)，并对其从未见过的例子进行预测或猜测。感知器基础上的数学过程(*逻辑回归*)是神经网络的构建模块，我们将在本章稍后介绍。

尽管方向肯定是好的，但网络本身相对简单，1957 年的硬件肯定不能让今天的机器创造奇迹。每当你想知道 Raspberry Pi 是否是运行机器学习模型的正确选择时，请记住，你正在处理的机器几乎比 Frank Rosenblatt 用来训练第一个可以识别人脸的模型的机器强大一百万倍[4，5]。

感知器实验后的失望导致了对我们今天所知的机器学习领域的兴趣下降(直到上世纪 90 年代末，当改进的硬件开始显示出该理论的潜力时，该领域才再次兴起)，而更多的注意力则放在了人工智能的其他分支上。在 20 世纪 60 年代和 70 年代，特别是出现了作为搜索的推理，这种方法将找到特定解决方案的问题基本上转化为在表示可用知识的连通图中搜索路径的问题。寻找两个单词的意思有多“接近”变成了在语义图中寻找两个相关节点之间的最短路径的问题。在一盘国际象棋中寻找最佳走法变成了在所有可能场景的图形中寻找具有最小*成本*或最大*利润*的路径的问题。证明一个定理是真还是假变成了从它的命题加上相关的公理构建一个*决策树*的问题，并找到一条可以导致真或假陈述的路径。这些领域的进展导致了令人印象深刻的早期成就，例如今天被认为是聊天机器人的第一个例子的 ELIZA。它于 1964 年至 1966 年间在麻省理工学院开发，用于模拟人类对话，它可能会欺骗用户(至少在最初的几次互动中)，认为对方是人类。事实上，早期版本背后的算法相对简单，因为它只是重复或重新表述了用户提出的一些问题(对许多人来说，这给人一种与心理医生交谈的印象)，但请记住，我们仍然在谈论第一个视频游戏诞生前的几年。这样的成就导致了当时对人工智能的极度乐观。这种早期乐观的几个例子:

*   1958 年:“十年内数字计算机将成为世界象棋冠军，数字计算机将发现并证明一个重要的新数学定理”[6]。

*   1965 年:“二十年内，机器将能做任何人能做的工作”。

*   1967 年:“在一代人的时间内，创造‘人工智能’的问题将得到实质性的解决”[8]。

*   1970 年:“在三到八年内，我们将拥有一台具有普通人一般智力的机器”[9]。

当然，事情并不完全是那样的。大约在 20 世纪 70 年代中期，大多数研究人员意识到他们肯定低估了这个问题。当然，主要问题在于当时的计算能力。到 20 世纪 60 年代末，研究人员意识到训练多层感知机网络比训练单个感知机效果更好，到 20 世纪 70 年代中期，*反向传播*(网络如何“学习”的构建模块)被理论化。换句话说，现代神经网络的基本形状在 20 世纪 70 年代中期就已经理论化了。然而，训练一个类似神经的模型需要大量的 CPU 能力来执行收敛到最优解所需的计算，而这样的硬件能力在接下来的 25-30 年内是不可用的。

与此同时，*推理作为搜索*方法面临组合爆炸问题。将决策过程转化为图搜索问题对于下棋、证明几何定理或寻找同义词来说是可以的，但更复杂的现实世界问题很容易导致庞大的图，因为它们的复杂性将随着输入的数量呈指数增长——这使得人工智能主要成为研究实验室内的玩具项目，而不是现实世界的应用。

最后，研究人员了解到了被称为*莫拉维克悖论*的东西:对于一台确定性机器来说，证明一个定理或解决一个几何问题确实很容易，但要执行更多“模糊”的任务就困难得多，比如识别人脸或在不撞到物体的情况下行走。当研究成果未能实现时，研究经费就耗尽了。

人工智能在 20 世纪 80 年代以专家系统的形式复兴。专家系统是一种软件，它应用从人类专家的知识中得出的推理规则，在特定的知识领域内回答问题或解释文本的内容。20 世纪 70 年代末引入的通过关系和基于图形的数据库对知识的正式表示导致了这场新的人工智能革命，这场革命专注于如何最好地表示人类知识以及如何从中推断决策。

专家系统经历了又一波乐观浪潮，随后又是一次崩溃。虽然它们在提供简单的特定领域问题的答案方面相对较好，但它们与人类专家提供的知识一样好。这使得它们的维护和更新非常昂贵，并且每当输入看起来与知识库中提供的略有不同时，就非常容易出错。它们在特定的环境中是有用的，但是它们不能被放大来解决更通用的问题。基于逻辑的人工智能的整个框架在 20 世纪 90 年代受到越来越多的批评。许多研究人员认为，真正智能的机器应该是自下而上而不是自上而下设计的。如果机器不知道雨和伞的实际含义或样子，它就不能对这些东西或概念做出逻辑推理——换句话说，如果它不能基于直觉和获得的经验进行某种形式的类似人类的分类。

这种思考逐渐导致了对人工智能机器学习方面的新兴趣，而不是基于知识或符号的方法。此外，20 世纪 90 年代末的硬件比 20 世纪 60 年代麻省理工学院研究人员可用的硬件好得多，在当时被证明具有难以置信的挑战性的简单计算机视觉任务，如识别手写字母或检测简单物体，可以通过摩尔定律得到解决，该定律指出，芯片中晶体管的数量大约每 18 个月翻一番。感谢网络和这些年来它提供的大量数据，以及分享这些数据的日益便利。

今天，神经网络是机器学习和人工智能中普遍存在的组件。然而，需要注意的是，在某些情况下，其他方法可能仍然适用。在你家和办公室之间骑自行车寻找最快的路径在很大程度上仍然是一个图形搜索问题。非结构化文本中的意图检测仍然依赖于语言模型。其他问题，如一些游戏或真实世界的模拟，可能会使用遗传算法。一些特定的领域仍然可以利用专家系统。然而，即使采用了其他算法，现在的神经网络也常常是连接所有组件的“粘合剂”。如今，不同的算法或网络通常是通过*数据管道*连接在一起的模块化模块。

在过去的十年里，深度学习已经变得越来越受欢迎，因为更好的硬件和更多的数据使得将更多的数据训练到更大的网络和更多的层成为可能。深度学习，在引擎盖下，是通过添加更多的神经元和更多的层来解决早期网络的一些常见问题(如过度拟合)的过程。通常，当你增加网络的层数和节点数时，网络的准确性也会增加，因为网络更善于发现非线性问题中的模式。然而，深度学习也可能受到一些问题的困扰。其中之一是*消失渐变问题*，当渐变穿过越来越多的层时，渐变会慢慢缩小。另一个更具体的问题与其环境影响有关:尽管向网络中投入更多的数据和更多的神经元以及运行更多的训练迭代似乎可以使网络更加准确，但这也是一个非常耗电的解决方案，无法持续长期增长。

## 1.2 监督和非监督学习

既然人工智能和机器学习之间的区别已经很明显，并且我们已经了解了我们是如何走到现在这一步的，那么让我们将注意力转移到机器学习和两个主要的“学习”类别上:有监督的和无监督的学习:

*   我们将**监督学习**定义为一组算法，其中模型在*数据集*上训练，该数据集包括示例输入数据和相关的预期输出。

*   另一方面，在**无监督学习**中，我们在不包括预期输出的数据集上训练模型。在这些算法中，我们期望模型自己“找出”模式。

当谈到监督学习时，训练模型通常包括计算函数![$$ \overline{y}=f\left(\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq1.png)，该函数将给定的输入向量![$$ \overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq2.png)映射到输出向量![$$ \overline{y} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq3.png)，使得预测值和期望值之间的平均误差最小化。监督学习算法的一些应用包括

*   给定一个包含一百万张猫的图片和一百万张没有猫的图片的训练集，建立一个在以前看不到的图片中识别猫的模型。在这种情况下，训练集通常会包含一个真/假标签来判断图片中是否包含一只猫。这通常被认为是一个**分类**问题——也就是说，给定一些输入值和它们的标签(统称为*训练集*，你希望你的模型预测正确的*类*，或者标签——例如，“包含/不包含一只猫。”或者，如果您为模型提供许多被标记为垃圾邮件/非垃圾邮件的电子邮件示例，您可以训练您的分类器来检测以前未见过的电子邮件中的垃圾邮件。

*   给定包含城市中大量公寓的特征(大小、位置、建造年份等)的训练集。)连同它们的价格，建立一个可以预测市场上新公寓价值的模型。这通常被认为是一个**回归**问题——也就是说，给定一个训练集，您想要训练一个模型来预测新提供的输入的最佳数值近似值，例如，美元、米或千克。

另一方面，无监督学习通常用于解决问题，其目标是找到数据中的潜在结构、分布或模式。由于没有预期的标签，这些类型的问题既没有确切/正确的答案，也无法与预期值进行比较。无监督学习问题的一些例子包括

*   给定电子商务网站上具有相关输入特征(年龄、性别、地址、过去一年的购买清单等)的大量客户列表。)，找到细分客户群的最佳方式，以便策划几个广告活动。这通常被认为是一个**聚类**问题——也就是说，给定一组输入，找到将它们分组在一起的最佳方式。

*   给定音乐流平台上的用户及其相关特征(年龄、性别、上个月收听的曲目列表等。)，构建一个可以推荐音乐品味相近的用户简档的模型。这通常被认为是一个**推荐系统**，或者**关联**问题——也就是说，一个寻找特定节点最近邻居的模型。

最后，在监督学习和非监督学习之间可能存在一些问题。想象一个大型生产数据库，其中一些图像被标记(例如，“猫”、“狗”等)。)但有些则不然——例如，因为雇佣足够多的人来手动标记所有记录的成本很高，或者因为问题非常大，很难提供所有可能标签的全貌。在这种情况下，您可能希望依靠混合实现，使用监督学习从可用的标记数据中学习，并利用非监督学习在未标记的数据中查找模式。

在本书的其余部分，我们将主要关注*监督学习*，因为这一类别包括当今使用的大多数神经架构，以及所有的回归问题。然而，一些流行的无监督学习算法值得一提，因为它们可能经常与监督算法共生使用。

## 1.3 准备工具

在谈论了这么多机器学习之后，让我们来介绍一下我们将在旅程中使用的工具。在本章中，你还不需要树莓派:当我们讨论用于机器学习的算法和软件工具时，你自己的笔记本电脑将会完成这项工作。

在接下来的几节中，我将假设您对以下方面有一些知识/经验

*   任何编程语言(如果有 Python 的经验就更好了)。在过去的几年中，Python 已经成为机器学习最流行的选择，但是即使你没有太多的经验，也不要担心——它相对简单，我会尽可能多地对代码进行注释。

*   高中(或更高)水平的数学。如果你熟悉微积分、统计学和线性代数，那就更好了。如果没有，也不用担心。虽然需要一些微积分和线性代数概念来掌握机器学习如何在引擎盖下工作，但我会尽量不要过多地挖掘理论，每当我提到梯度、张量或回忆时，我都会确保更多地关注它们的直观含义，而不是仅仅关注它们的形式定义。

### 软件工具

我们将在旅程中使用以下软件工具:

*   **Python 编程语言**(3.6 或更高版本)。

*   TensorFlow ，可能是当今最流行的构建、训练和查询机器学习模型的框架。

*   Keras ，一个非常受欢迎的神经网络和回归模型库，可以轻松集成到 TensorFlow 之上。

*   **numpy** 和 **pandas** ，最常用的 Python 库，分别用于数值操作和数据分析。

*   **matplotlib** ，一个用于绘制数据和图像的 Python 库。

*   **seaborn** ，一个经常用于统计数据可视化的 Python 库。

*   jupyter ，这是一个非常流行的 Python 解决方案，用于通过笔记本进行原型开发(对于 Python 中的数据科学来说，这基本上是一个事实上的标准)。

*   git ，我们将使用它从 GitHub 下载样本数据集和笔记本。

### 1.3.2 设置环境

*   下载 git 并安装到您的系统上，如果它还不可用的话。

*   从 [`https://python.org/downloads`](https://python.org/downloads) 下载并安装最新版本的 Python，如果你的系统上还没有的话。请确保您使用的 Python 版本高于 3(不推荐使用 Python 2)。打开一个终端，检查`python`和`pip`命令是否存在。

*   (可选)创建新的 Python 虚拟环境。虚拟环境允许您将机器学习设置与主 Python 安装分开，而不会干扰任何系统范围的依赖关系，并且它还允许您在非特权用户空间中安装 Python 包。如果您希望在系统范围内安装依赖项，可以跳过这一步(尽管您可能需要 root/administrator 权限)。

*   安装依赖项(根据您的连接和 CPU 能力，可能需要一段时间):

```
# Create a virtual environment under your home folder
python -m venv $HOME/venv

# Activate the environment
cd $HOME/venv
source bin/activate

# Whenever you are done, run 'deactivate'
# to go back to your standard environment
deactivate

```

*   下载“mlbook-code”存储库，其中包含我们将在本书中用到的一些数据集和代码片段。我们称`<REPO_DIR>`为您克隆存储库的目录。

*   启动 Jupyter 服务器:

```
pip install tensorflow
pip install keras
pip install numpy
pip install pandas
pip install matplotlib
pip install jupyter
pip install seaborn

```

*   在浏览器中打开`http://localhost:8888`。您应该会看到如图 [1-1](#Fig1) 所示的登录屏幕。您可以决定是使用令牌进行鉴定还是设置密码。

*   选择您想要存储笔记本的文件夹(从现在开始，我们将其标识为`<NOTEBOOK_DIR>`)，并创建您的第一个笔记本。Jupyter 笔记本是单元格列表；每个单元格都可以包含 Python 代码、markdown 元素、图像等等。开始熟悉环境，尝试运行一些 Python 命令，确保一切正常。

```
jupyter notebook

```

```
git clone https://github.com/BlackLight/mlbook-code

```

现在所有的工具都准备好了，让我们动手做一些算法。

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig2_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig2_HTML.jpg)

图 1-2

作为面积函数的房价分布

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig1_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig1_HTML.jpg)

图 1-1

`http://localhost:8888`处的 Jupyter 登录屏幕

## 1.4 线性回归

线性回归是我们旅途中遇到的第一个机器学习算法，也是最重要的。正如我们将看到的，大多数监督机器学习算法是线性回归的变体或线性回归在规模上的应用。线性回归适用于解决这样的问题:您有一些由 *n* 维表示的输入数据，并且您希望从这些数据的分布中学习，以便对未来的数据点进行预测，例如，在给定一系列历史数据的情况下，预测某个社区的房屋或股票的价格。线性回归也被广泛用作金融、物流和经济学中的统计工具，以预测商品的价格、特定时期的需求或其他宏观经济变量。它是其他类型回归的构建模块，如逻辑回归，而逻辑回归又是神经网络的构建模块。

如果输入数据由单个变量( *n* = 1)表示，则回归模型称为*简单回归*模型；如果回归模型对由多个维度定义的输入数据进行操作，则称为*多元回归*。如果它的输出是一条最接近输入数据的线，它被称为*线性回归*。也存在其他类型的回归——例如，如果您试图通过数据拟合抛物线而不是直线，您将得到一个*二次回归*，如果您试图通过数据拟合三次多项式曲线，您将得到一个*三次回归*，等等。我们将从简单的线性回归开始，并将其基本机制扩展到其他回归问题。

### 1.4.1 加载和绘制数据集

首先，在您的`<NOTEBOOK_DIR>`下创建一个新的 Jupyter 笔记本并加载`<REPO_DIR>/datasets/house-size-price-1.csv`。这是一个 CSV 文件，其中包含某个城市的房价(以千美元计)与房价大小(以平方米计)的函数关系。现在让我们假设大小是我们得到的唯一参数，我们希望创建一个基于此数据训练的模型，该模型可以预测给定大小的新房子的价格。

在开始定义任何机器学习模型之前，你应该做的第一件事是可视化你的数据，并试图了解什么是最好的模型。这就是我们如何使用`pandas`加载 CSV 并使用`matplotlib`可视化它:

```
import pandas as pd
import matplotlib.pyplot as plt

# Download the CSV file from GitHub
csv_url = 'https://raw.githubusercontent.com/' +
          'BlackLight/mlbook-code/master/' +
          'datasets/house-size-price-1.csv'

data = pd.read_csv(csv_url)

# The first column contains the size in m2
size = data[columns[0]]

# The second column contains the price in thousands of dollars
price = data[columns[1]]

# Create a new figure and plot the data
fig = plt.figure()
plot = fig.add_subplot()
plot.set_xlabel(columns[0])
plot.set_ylabel(columns[1])
points = plot.scatter(size, price)

```

运行完单元格的内容后，您应该会看到如图 [1-2](#Fig2) 所示的图表。你会注意到数据有点分散，但如果我们能拟合一条直线通过它，它仍然可以很好地近似。我们现在的目标是找到最接近我们数据的直线。

### 1.4.2 回归背后的思想

让我们暂时把笔记本放在一边，试着想想这样一个功能应该有什么特点。第一，必须是一条线；因此，它必须有这样的形式:

![$$ {h}_{\theta }(x)={\theta}_0+{\theta}_1x $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ1.png)

(1.1)

在等式 [1.1](#Equ1) 中， *x* 表示输入数据，不包括输出标签。在房子大小-价格模型的情况下，我们有一个输入变量(大小)和一个输出变量(价格)；因此，输入和输出向量都具有单一的大小，但是其他模型可以具有多个输入变量和/或多个输出变量。 *θ* <sub>0</sub> 和 *θ* <sub>1</sub> 是该行的数值系数。特别是， *θ* <sub>0</sub> 告诉我们直线与 *y* 轴的交叉点，而 *θ* <sub>1</sub> 告诉我们直线的方向以及它有多“陡”——它通常被称为直线的*斜率*。*h*<sub>*θ*</sub>(*x*)是我们的模型将用于基于向量![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq4.png)的值进行预测的函数，也称为模型的*权重*。在我们的问题中，输入![$$ \overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq5.png)和期望输出![$$ \overline{y} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq6.png)是通过训练集提供的；因此，线性回归问题是在前面的方程中寻找![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq7.png)参数的问题，使得*h*<sub>*θ*</sub>(*x*)是 *x* 和 *y* 之间线性相关性的良好近似。*h*<sub>*θ*</sub>(*x*)常被表示为*假设函数*，或简称为*模型*。阶为 *n* 的单变量模型的通用公式(线性为 *n* = 1，二次为 *n* = 2，以此类推。)将

![$$ {h}_{\theta }(x)={\theta}_0+{\theta}_1x+{\theta}_2{x}^2+\dots +{\theta}_n{x}^n=\sum \limits_{i=0}^n{\theta}_i{x}^i $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ2.png)

(1.2)

另外，请注意，本书中符号顶部的“条”或“上标”将表示一个*向量*。vector 是一个固定大小的数值元素列表，所以![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq8.png)实际上是一种更简洁的写法[ * θ * <sub>0</sub> ， *θ* <sub>1</sub> 或者[*θ*<sub>0</sub>……*θ*<sub>*n*</sub>]。

那么如何才能将“足够好的线性逼近”这一直观概念形式化为算法呢？直觉是选择![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq9.png)参数，使得它们相关的*h*<sub>*θ*</sub>(*x*)函数对于给定的 *x* 值“足够接近”所提供的样本 *y* 。更正式地说，对于训练集中提供的所有 *m* 个数据点，我们希望最小化采样值 *y* 和预测值*h*<sub>*θ*</sub>(*x*)之间的*均方误差*——如果预测值和实际值之间的误差较低，则模型表现良好:

![$$ \underset{\theta_0,{\theta}_1}{\min}\frac{1}{2m}\sum \limits_{i=1}^m{\left({h}_{\theta}\left({x}_i\right)-{y}_i\right)}^2 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ3.png)

(1.3)

让我们将前面公式的论点重新表述为参数![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq10.png) :

![$$ J\left(\overline{\theta}\right)=\frac{1}{2m}\sum \limits_{i=1}^m{\left({h}_{\theta}\left({x}_i\right)-{y}_i\right)}^2 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ4.png)

(1.4)的函数

函数 *J* 也被称为**成本函数**(或**损失函数**)，因为它表达了与![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq11.png)参数的特定选择相关联的线路的成本(或，在这种情况下，近似误差)。因此，为您的数据寻找最佳线性近似就是寻找使前面的成本函数(即样本和预测之间的均方误差之和)最小化的![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq12.png)值的问题。注意*h*<sub>*θ*</sub>(*x*)和![$$ J\left(\overline{\theta}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq13.png)的区别:前者是我们的*模型假设*，即以其参数![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq14.png)的向量表示的、以 *x* 为变量的模型的预测函数。![$$ J\left(\overline{\theta}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq15.png)是成本函数，参数![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq16.png)是变量，我们的目标是找到最小化该函数的![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq17.png)的值，以便计算最佳的*h*<sub>*θ*</sub>(*x*)。如果我们想要开始形式化该过程，我们可以说找到最佳回归模型的问题可以表达如下:

*   从一组初始参数![$$ \overline{\theta}=\left[{\theta}_0,{\theta}_1,\dots, {\theta}_n\right] $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq18.png)开始，用 *n* 作为你回归的阶数( *n* = 1 表示线性， *n* = 2 表示二次，以此类推。).

*   使用这些值来公式化假设*h*<sub>*θ*</sub>(*x*)如等式 [1.2](#Equ2) 所示。

*   如等式 [1.4](#Equ4) 所示，计算与该假设相关的成本函数![$$ J\left(\overline{\theta}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq19.png)。

*   不断改变![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq20.png)的值，直到我们收敛到最小化成本函数的点。

既然已经清楚了回归算法是如何建模的，以及如何衡量它对数据的逼近程度，那么让我们来看看如何实现前面列表中的最后一点，即实际的“学习”阶段。

### 梯度下降

如果你对微积分有一些记忆的话，希望这么多提到的“误差最小化”已经敲响了警钟！*微分*(或*导数*)是大多数最小化/最大化问题中的数学工具。特别是:

*   函数的*一阶导数*告诉我们该函数是在某个点附近增加还是减少，或者它是“静止的”(即该点是局部最小值/最大值或拐点)。几何上可以形象化为*切线*对函数在某一点的斜率。如果我们命名*f*<sup>’</sup>(*x*)一个函数的一阶导数 *f* ( *x* )(暂且用![$$ x\mathfrak{\in}\mathfrak{R} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq21.png))，那么它在一个点上的值 *x* <sub>0</sub> 就会是(假设函数在 *x* <sub>0</sub> 中*可微)*

![$$ {f}^{\prime}\left({x}_0\right)\left\{\begin{array}{l}&gt;0\ \mathrm{if}\ \mathrm{the}\ \mathrm{curve}\ \mathrm{is}\ \mathrm{increasing}\\ {}&lt;0\ \mathrm{if}\ \mathrm{the}\ \mathrm{curve}\ \mathrm{is}\ \mathrm{decreasing}\\ {}=0\ \mathrm{if}\ {x}_0\ \mathrm{is}\ \mathrm{a}\ \min /\max /\mathrm{flex}\end{array}\right. $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ5.png)

(1.5)

*   函数的*二阶导数*告诉我们该函数在某一点周围的“凹度”，即曲线在该点周围是面向“上”还是“下”。围绕给定点*x*T10】0 的二阶导数*f*T4】(*x*)将为

![$$ {f}^{{\prime\prime}}\left({x}_0\right)\left\{\begin{array}{l}&gt;0\ \mathrm{if}\ \mathrm{the}\ \mathrm{curve}\ \mathrm{faces}\ \mathrm{up}\\ {}&lt;0\ \mathrm{if}\ \mathrm{the}\ \mathrm{curve}\ \mathrm{faces}\ \mathrm{down}\\ {}=0\ \mathrm{if}\ {x}_0\ \mathrm{is}\ \mathrm{an}\ \mathrm{inflection}\ \mathrm{point}\end{array}\right. $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ6.png)

(1.6)

通过结合这两种工具，我们可以找出，给定曲面上的某一点，该函数的最小值在哪个方向。换句话说，最小化成本函数![$$ J\left(\overline{\theta}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq22.png)是寻找值![$$ \overline{\theta^{\ast }}=\left[{\theta}_0,{\theta}_1,\dots, {\theta}_n\right] $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq23.png)的向量的问题，使得 *J* 对![$$ \overline{\theta^{\ast }} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq24.png)的一阶导数为零(或者足够接近零)，并且其二阶导数为正(即，该点是局部最小值)。在本书的大多数算法中，我们实际上不需要二阶导数(当今许多流行的机器学习模型都是围绕*凸*成本函数构建的，即具有一个单点最小值的成本函数)，但具有更高多项式模型的应用程序可能会利用二阶导数背后的凹度来判断一个点是最小值、最大值还是拐点。

这种直觉适用于单变量的情况。但是， *J* 是一个参数向量![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq25.png)的函数，线性回归的长度为 2，二次的长度为 3，以此类推。对于多元函数，使用*梯度*的概念，而不是用于一元函数的导数。多元函数的梯度(通常用符号𝛻表示)是针对每个变量计算的该函数的*偏导数*(通常用 *∂* 符号表示)的向量。在我们的成本函数的例子中，它的梯度将是

![$$ \nabla J\left(\overline{\theta}\right)=\left[\begin{array}{c}\frac{\partial }{\partial {\theta}_0}J\left(\overline{\theta}\right)\\ {}\vdots \\ {}\frac{\partial }{\partial {\theta}_n}J\left(\overline{\theta}\right)\end{array}\right] $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ7.png)

(1.7)

在实践中，偏导数是计算多元函数导数的过程，假设它的变量中只有一个是实际变量，其他都是常数。

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig3_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig3_HTML.jpg)

图 1-3

线性回归模型的成本函数的典型形状:三维空间中的抛物面

梯度向量表示一条 *n* 维曲线围绕某一点增加的方向以及增加的“速度”。在具有一个输入变量的线性回归的情况下，我们已经在方程 [1.4](#Equ4) 中看到，成本函数是两个变量的函数( *θ* <sub>0</sub> 和 *θ* <sub>1</sub> )，并且是二次函数。事实上，通过组合方程 [1.1](#Equ1) 和 [1.4](#Equ4) ，我们可以导出单变量输入的线性回归的成本函数:

![$$ J\left({\theta}_0,{\theta}_1\right)=\frac{1}{2m}\sum \limits_{i=1}^m{\left({\theta}_0+{\theta}_1{x}_i-{y}_i\right)}^2 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ8.png)

(1.8)

这个表面可以像抛物面一样在 3D 空间中表示(如果你将抛物线绕其轴旋转 360 度，就会得到一个碗的形状，如图 [1-3](#Fig3) 所示)。这让我们处于一个相对幸运的位置。就像抛物线一样，抛物面只有一个最小值——也就是说，只有一个点的矢量梯度为零，而这个点也恰好是全局最小值。这意味着，如果我们从![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq26.png)的一些随机值开始，然后开始在该点的梯度的相反方向上“行走”，我们应该最终在最优点结束——也就是说，我们可以插入我们的假设函数*h*<sub>*θ*</sub>(*x*)的参数![$$ \overline{\theta^{\ast }} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq27.png)的向量，以获得良好的预测。记住，给定曲面上的一个点，梯度向量会告诉你函数“向上”的方向如果你朝相反的方向走，你就会下去。如果你的表面是碗的形状，如果你从表面的任何地方继续向下，你最终会到达底部。如果你使用更复杂的回归类型(二次、三次等)。)，您可能并不总是那么幸运——您可能会陷入局部最小值，这并不代表函数的整体最小值。因此，假设我们一开始向最小值收敛就很高兴，那么*梯度下降*算法可以被表达为这样一个过程，在该过程中，我们首先为![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq28.png)选取随机值，然后在每一步 *k* 中，我们通过下面的公式更新这些值:

![$$ {\theta}_i^{\left(k+1\right)}={\theta}_i^{(k)}-\alpha \frac{\partial }{\partial {\theta}_i}J\left({\theta}_0^{(k)},{\theta}_1^{(k)}\right)\ \mathrm{for}\ i=0,1 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ9.png)

(1.9)

或者，以向量形式:

![$$ {\overline{\theta}}^{\left(k+1\right)}={\overline{\theta}}^{(k)}-\alpha \nabla J\left({\overline{\theta}}^{(k)}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ10.png)

(1.10)

*α* 是一个介于零和一之间的参数，被称为机器学习模型的**学习速率**，它表达了*该模型从新数据中学习的速度*——换句话说，沿着梯度向量的方向应该“跳跃”多大。较大的值 *α* 将导致模型在开始时可以快速学习，但可能会超过最小值或“错过停止点”——在某个点上，它可能会在成本函数的最小值附近着陆，并可能会因采取更长的步骤而错过它，从而导致在收敛之前来回“反弹”。另一方面，较小的值 *α* 可能在开始时学习速度较慢，可能需要更多的迭代，但基本上可以保证在最小值附近不会有太多的反弹。

打个比喻，通过梯度下降进行学习就像蒙着眼睛把球推下山谷。首先，你必须用梯度向量的方向作为指南针，找出谷底的方向。然后，你要找出你要对球施加多大的力才能让它触底。如果你用很大的力推它，它可能会更快地到达底部，但它可能会来回几次才能沉淀下来。相反，如果你只让重力发挥作用，球可能需要更长时间才能到达底部，但一旦到达底部，它就不太可能摆动太多。在当今的大多数应用程序中，学习率是动态的:在第一阶段(当您的模型还不太了解数据时)，您可能通常想要更高的值 *α* ，并在接近结束时(当您的模型已经看到大量数据点和/或成本函数正在收敛时)降低它。现在一些流行的算法也执行某种学习率*洗牌*以防卡住。

我们可能还想为算法设置一个*退出条件*:例如，如果它没有收敛到使成本函数无效的参数向量上，如果成本函数的梯度足够接近零，或者从上一步没有显著的改进，或者相应的模型已经足够好地逼近我们的问题，我们可能仍然想退出。

通过组合方程 [1.8](#Equ8) 和 [1.9](#Equ9) 并应用微分规则，我们可以导出线性回归情况下*θ*T6】0 和*θ*T10】1 的精确更新步骤:

![$$ {\theta}_0^{\left(k+1\right)}={\theta}_0^{(k)}-\frac{\alpha }{m}\sum \limits_{i=1}^m\left({\theta}_0^{(k)}+{\theta}_1^{(k)}{x}_i-{y}_i\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ11.png)

(1.11)

![$$ {\theta}_1^{\left(k+1\right)}={\theta}_1^{(k)}-\frac{\alpha }{m}\sum \limits_{i=1}^m\left({\theta}_0^{(k)}+{\theta}_1^{(k)}{x}_i-{y}_i\right){x}_i $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ12.png)

(1.12)

因此，线性回归的完整算法可以总结如下:

1.  为 *θ* <sub>0</sub> 和 *θ* <sub>1</sub> 选取随机值。

2.  分别通过方程 [1.11](#Equ11) 和 [1.12](#Equ12) 保持更新*θ*T2 0 和*θ*T6】1。

3.  在执行预设数量的训练迭代后(通常称为*时期*)或达到收敛时(即，与前一步骤相比，某一步骤未测量到显著改善)终止。

有许多工具和库可以执行有效的回归，所以您不得不从头开始实现算法是不常见的。然而，由于前面的步骤是已知的，所以用 Python 编写一个仅含`numpy`的单变量线性回归算法相对简单:

```
import numpy as np

def gradient_descent(x, y, theta, alpha):
    m = len(x)
    new_theta = np.zeros(2)

    # Perform the gradient descent on theta
    for i in range(m):
        new_theta[0] += theta[0] + theta[1]*x[i] - y[i]
        new_theta[1] += (theta[0] + theta[1]*x[i] - y[i]) * x[i]

    return theta - (alpha/m) * new_theta

    def train(x, y, steps, alpha=0.001):
        # Initialize theta randomly
        theta = np.random.randint(low=0, high=10, size=2)

        # Perform the gradient descent <steps> times
        for i in range(steps):
            theta = gradient_descent(x, y, theta, alpha)

        # Return the linear function associated to theta
        return lambda x: theta[0] + theta[1] * x

```

### 输入标准化

让我们从离开的地方捡起我们的笔记本。我们现在应该有一个清晰的想法，如何训练一个回归模型来预测给定大小的房子的价格。我们将使用 TensorFlow+Keras 来定义和训练模型。

然而，在继续之前，重要的是花点时间了解一下输入规范化(或**特性缩放**)。非常重要的一点是，即使某些数据以不同于训练集中使用的单位提供，或者某些任意常数被添加或乘以您的输入，您的模型也要足够稳健。这就是为什么在将输入输入到任何机器学习模型之前，对其进行标准化非常重要。不仅如此，如果输入在以原点为中心的特定范围内分布良好，模型的收敛速度将比提供没有特定范围的原始输入快得多。更糟糕的是，非标准化的训练集通常会导致成本函数根本不收敛。

输入规范化通常通过应用以下变换来完成:

![$$ {\hat{x}}_i=\frac{x_i-\mu }{\sigma }\ \mathrm{for}\ i=1,\dots, m $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ13.png)

(1.13)

其中*x*<sub>T3】IT5】是你的 *m* -long 输入向量的第 *i* 个元素， *μ* 是输入的向量![$ \overline{x} $](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq29.png)的算术平均值， *σ* 是其标准差:</sub>

![$$ \mu =\frac{1}{m}\sum \limits_{i=1}^m{x}_i $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ14.png)

(1.14)

![$$ \sigma =\sqrt{\frac{1}{m}\sum \limits_{i=1}^m{\left({x}_i-\mu \right)}^2} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ15.png)

(1.15)

通过将等式 [1.13](#Equ13) 应用于我们的输入，我们基本上在零点附近转置输入值，并在[*σ*、 *σ* 范围内对大部分输入值进行分组。在预测值时，我们反而希望对提供的输出进行反规格化，这可以通过从等式 1.13:

![$$ {x}_i=\sigma {\hat{x}}_i+\mu $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ16.png)

(1.16)推导出 *x* <sub>*i*</sub> 来轻松实现

用 Python 编写这些函数并把它们插入到我们的笔记本中是相当容易的。首先，使用`pandas describe`方法获取数据集统计信息:

```
dataset_stats = data.describe()

```

然后定义规范化和反规范化数据的函数:

```
def normalize(x, stats):
    return (x - stats['mean']) / stats['std']

def denormalize(x, stats):
    return stats['std'] * x + stats['mean']

norm_size = normalize(size, dataset_stats['size'])
norm_price = normalize(price, dataset_stats['price'])

```

### 1.4.5 定义和训练模型

使用 TensorFlow+Keras 定义和训练线性回归模型非常简单:

```
from tensorflow.keras.experimental import LinearModel

model = LinearModel(units=1, activation="linear", dtype="float32")
model.compile(optimizer='rmsprop', loss="mse", metrics=['mse'])
history = model.fit(norm_size, norm_price, epochs=1000, verbose=0)

```

这里发生了很多事情:

*   首先，我们用一个输入变量(`units=1`)、`linear`激活函数(即模型的输出值不经过非线性输出函数的转换直接返回)和`float32`数值类型定义一个`LinearModel`。

*   然后，我们`compile`模型，让它准备好接受训练。Keras 中的`optimizer`做很多事情。对优化器的深入理解需要一个专门的章节，但是为了保持简洁，我们将快速介绍我们使用它们时它们做了什么。`rmsprop`初始化学习率，并根据最近的梯度在训练迭代中逐渐调整学习率[10]。默认情况下，`rmsprop`用`learning_rate=0.001`初始化。但是，您可以尝试调整它，看看它如何/是否会影响您的模型:

*   其他常见的优化器包括
    *   `SGD`或*随机梯度下降*，它实现了等式 [1.9](#Equ9) 中描述的梯度下降算法，并进行一些优化和调整，如学习率衰减和内斯特罗夫动量[11]

    *   `adam`，一种基于一阶梯度优化算法，最近获得了相当大的发展势头，特别是在深度学习中[12]

    *   `nadam`，它在`adam`算法之上实现了对内斯特罗夫动量的支持【13】

*   随意试验不同的优化器和不同的学习率，看看它如何影响你的模型的性能。

*   `loss`参数定义了要优化的*损失/成本函数*。`mse`是指*均方误差*，我们已经在等式 [1.4](#Equ4) 中定义了它。其他常见的损失函数包括
    *   `mae`，或*表示绝对误差*—类似于`mse`，但它使用的是*h*(*x*<sub>T9】I</sub>)*y*<sub>*I*</sub>的绝对值，而不是等式 [1.4](#Equ4) 中的平方值。

    *   `mape`或*表示绝对百分比误差*——类似于`mae`，但它使用与前一次迭代相比的绝对误差百分比作为目标度量。

    *   `mean_squared_logarithmic_error`—与`mse`类似，但它使用均方误差的对数(如果您的曲线具有指数特征，这很有用)。

    *   几种*交叉熵*损失函数(如`categorical_crossentropy`、`sparse_categorical_crossentropy`、`binary_crossentropy`，常用于分类问题。

*   `metrics`属性是一个列表，它标识了用于评估模型性能的指标。在本例中，我们使用与损失/成本函数相同的指标(均方误差)，但也可以使用其他指标。度量在概念上类似于损失/成本函数，只是度量函数的结果仅用于评估模型，而不是训练模型。如果希望根据多个特征评估模型，也可以使用多个度量。其他常见指标包括
    *   `mae`或*表示绝对误差*。

    *   `accuracy`及其派生的度量标准(`binary_accuracy`、`categorical_accuracy`、`sparse_categorical_accuracy`、`top_k_categorical_accuracy`等)。).准确性通常用于分类问题，它表示训练集中正确标记的项目占训练集中项目总数的比例。

    *   还可以定义自定义指标。正如我们稍后处理分类问题时将会看到的，精确度、召回率和 F1 分数是非常流行的评估指标。这些还不是核心框架的一部分(还不是？)，但它们很容易被定义。

*   最后，我们*使用`fit`函数根据上一步得到的标准化数据训练*我们的模型。该函数的第一个参数将是输入值的向量，第二个参数将是预期输出值的向量，然后我们指定**历元**的数量，即我们希望对该数据执行的训练迭代。

```
from tensorflow.keras.optimizers import RMSprop
rmsprop = RMSprop(learning_rate=0.005)
model.compile(optimizer=rmsprop, loss="mse", metrics=['mse'])

```

**历元**值在很大程度上取决于您的数据集。您将呈现给模型的输入样本的累积数量由×个时期给出，其中 *m* 是数据集的大小。在我们的例子中，我们有一个相对较小的数据集，这意味着您想要运行更多的训练时期，以确保您的模型已经看到了足够的数据。但是，如果您有较大的训练集，您可能希望运行较少的训练迭代。在同一批数据上运行太多训练迭代的风险，正如我们将在后面看到的，是*过度拟合*你的数据，也就是说，创建一个模型，如果提供的值足够接近它已经被训练过的值，那么它将紧密模拟预期的输出，但在没有被训练过的数据点上不准确。

### 1.4.6 评估模型

现在，我们已经定义并训练了我们的模型，并对如何衡量其性能有了清晰的想法，让我们看看它的主要指标(均方差)在训练阶段是如何改进的:

```
epochs = history.epoch
loss = history.history['loss']

fig = plt.figure()
plot = fig.add_subplot()
plot.set_xlabel('epoch')
plot.set_ylabel('loss')
plot.plot(epochs, loss)

```

您应该会看到类似于图 [1-4](#Fig4) 所示的图。你会注意到损耗曲线急剧下降，这很好；这意味着当我们输入数据点时，我们的模型实际上是在学习，而不会陷入梯度“山谷”这也意味着学习率得到了很好的校准——如果你的学习率太高，模型不一定会收敛，如果太低，那么在许多训练时期之后，它可能仍在朝着收敛的方向前进。还有，0.07 左右有一个短尾。这也是好的:这意味着我们的模型在最近的迭代中已经收敛了。

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig4_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig4_HTML.jpg)

图 1-4

线性回归模型损失随训练的演变

如果尾部太长，这意味着您已经为模型训练了太多的时期，您可能希望减少时期的数量或训练集的大小，以防止过度拟合。如果尾部太短或根本没有尾部，则您的模型没有在足够的数据点上进行训练，您可能需要增加训练集的大小或时期的数量来获得准确性。

您还可以(read: should)在一些不一定属于您的训练集的数据上评估您的模型，以检查模型在新数据点上的表现如何。稍后，我们将深入探讨如何拆分您的*训练集*和*测试集*。现在，给定相对较小的数据集，我们可以通过`evaluate`函数在数据集本身上评估模型:

```
model.evaluate(norm_size, norm_price)

```

您应该会看到如下输出:

```
0s 2ms/step - loss: 0.0733 - mse: 0.0733
[0.0733143612742424, 0.0733143612742424]

```

返回的向量分别包含损失函数和度量函数的值——在我们的例子中，因为我们对两者都使用了`mse`,所以它们是相同的。

最后，让我们看看线性模型相对于数据集的实际情况，并开始使用它进行预测。首先，定义一个`predict`函数

1.  将一个房屋列表`sizes`作为输入，并根据您的训练集的平均值和标准偏差对其进行标准化

2.  查询线性模型以获得预测价格

3.  使用训练集的平均值和标准差对输出进行反规范化，以转换以千美元为单位的价格

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig5_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig5_HTML.jpg)

图 1-5

根据数据集绘制线性模型

```
def predict_prices(*x):
    x = normalize(x, dataset_stats['size'])
    return denormalize(model.predict(x), dataset_stats['price'])

```

让我们用这个函数来预测一下房价:

```
predict_prices(90)
array([[202.53752]], dtype=float32)

```

“predict”函数将返回与您的输入相关联的输出列表，其中每一项都是包含预测值的向量(在本例中，是单位大小的向量，因为我们的模型只有一个输出单元)。如果你回头看图 [1-2](#Fig2) ，你会发现*尺码* = 90 的预测价格 202.53752 实际上看起来与数据的分布并没有那么远——这很好。为了直观显示我们的线性模型相对于数据集的外观，让我们再次绘制数据集，并计算模型上的两个点以绘制直线:

```
# Draw the linear regression model as a line between the first and
# the last element of the numeric series. x will contain the lowest
# and highest size (assumption: the series is ordered) and y will
# contain the price predictions for those inputs.
x = [size[0], size.iat[-1]]
y = [p[0] for p in predict_prices(*x)]

# Create a new figure and plot both the input data points and the
# linear model approximation.
fig = plt.figure()

data_points = fig.add_subplot()
data_points.scatter(size, price)
data_points.set_xlabel(columns[0])
data_points.set_ylabel(columns[1])

model_line = fig.add_subplot()
model_line.plot(x, y, 'r')

```

前面代码的输出有望如图 [1-5](#Fig5) 所示。这告诉我们，模型计算出的线实际上与我们的数据并没有那么远。如果您碰巧看到您的线远离模型，这可能意味着您没有在足够的数据上训练您的模型，或者学习率太高/太低，或者您的数据集中的指标之间没有很强的线性相关性，可能您需要更高的多项式模型，或者可能您的模型中有许多“异常值”-即，主分布之外的许多数据点将线“拖”出来。

### 1.4.7 保存和加载您的模型

您的模型现在已加载到笔记本的内存中，但是一旦 Jupyter 笔记本停止运行，您将会丢失它。您可能希望将它保存到文件系统中，这样以后就可以恢复它，而不必再次经历训练阶段。或者您可以将它包含在另一个脚本中进行预测。幸运的是，在文件系统上保存和加载 Keras 模型非常容易——然而，下面的例子将假设您使用的是 TensorFlow `\geq 2.0`的一个版本:

```
def model_save(model_dir, overwrite=True):
    import json
    import os
    os.makedirs(model_dir, exist_ok=True)

    # The TensorFlow model save won't keep track of the labels of
    # your model. It's usually a good practice to store them in a
    # separate JSON file.
    labels_file = os.path.join(model_dir, 'labels.json')
    with open(labels_file, 'w') as f:
        f.write(json.dumps(list(columns)))

    # Also, you may want to keep track of the x and y mean and
    # standard deviation to correctly normalize/denormalize your
    # data before/after feeding it to the model.
    stats = [
        dict(dataset_stats['size']),
        dict(dataset_stats['price']),
    ]

    stats_file = os.path.join(model_dir, 'stats.json')
    with open(stats_file, 'w') as f:
        f.write(json.dumps(stats))

        # Then, save the TensorFlow model using the save primitive
        model.save(model_dir, overwrite=overwrite)

```

然后，您可以将该模型加载到另一个笔记本、脚本、应用程序等中(TensorFlow 的绑定可用于当今使用的大多数编程语言),并将其用于您的预测:

```
def model_load(model_dir):
    import json
    import os
    from tensorflow.keras.models import load_model

    labels = []
    labels_file = os.path.join(model_dir, 'labels.json')

    if os.path.isfile(labels_file):
        with open(labels_file) as f:
            labels = json.load(f)

    stats = []
    stats_file = os.path.join(model_dir, 'stats.json')

    if os.path.isfile(stats_file):
        with open(stats_file) as f:
            stats = json.load(f)

    m = load_model(model_dir)
    return m, stats, labels

model, stats, labels = model_load(model_dir)
price = predict_prices(90)

```

## 1.5 多元线性回归

到目前为止，我们已经探索了具有单个输入和输出变量的线性回归模型。现实世界的回归问题通常更复杂，输出特征通常表示为多个变量的函数。例如，一栋房子的价格不仅取决于它的大小，还取决于它的建造年份、卧室数量、是否有花园或露台等额外设施、离市中心的距离等等。在这种一般情况下，我们将每个输入数据点表示为一个向量![$$ \overline{x}=\left({x}_1,{x}_2,\dots, {x}_n\right)\in {\mathfrak{R}}^n $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq30.png)，等式 [1.2](#Equ2) 中看到的回归表达式被重新表示为

![$$ {h}_{\theta}\left(\overline{x}\right)={\theta}_0+{\theta}_1{x}_1+{\theta}_2{x}_2+\dots +{\theta}_n{x}_n={\theta}_0+\sum \limits_{i=1}^n{\theta}_i{x}_i $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ17.png)

(1.17)

按照惯例，多元回归情况下的输入向量重写为![$$ \overline{x}=\left({x}_0,{x}_1,{x}_2,\dots, {x}_n\right)\in {\mathfrak{R}}^{n+1} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq31.png)，其中*x*T3】0= 1，因此前面的表达式可以更简洁地写成

![$$ {h}_{\theta}\left(\overline{x}\right)=\sum \limits_{i=0}^n{\theta}_i{x}_i $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ18.png)

(1.18)

或者，通过使用向量符号，假设函数可以写成参数向量![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq32.png)和特征向量![$$ \overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq33.png) :

![$$ {h}_{\theta}\left(\overline{x}\right)=\left[{\theta}_0,\dots, {\theta}_n\right]\left[\begin{array}{c}{x}_0\\ {}\vdots \\ {}{x}_n\end{array}\right]=\overline{\theta^T}\overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ19.png)

(1.19)之间的*标量积*

请记住，按照惯例，向量被表示为值的*列*。 <sup>*T*</sup> 符号表示*转置的*向量，因此向量表示为一行。将一个行向量乘以一个列向量，就得到两个向量的*标量积*，所以![$$ \overline{\theta^T}\overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq34.png)是将![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq35.png)的标量积表示成![$$ \overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq36.png)的一种简洁方式。

因此，在多变量情况下，等式 [1.8](#Equ8) 中的均方误差成本函数可以重写为

![$$ J\left(\overline{\theta}\right)=\frac{1}{2m}\sum \limits_{i=1}^m{\left(\sum \limits_{j=0}^n{\theta}_j{x}_j^{(i)}-{y}^{(i)}\right)}^2 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ20.png)

(1.20)

或者，用矢量符号:

![$$ J\left(\overline{\theta}\right)=\frac{1}{2m}\sum \limits_{i=1}^m{\left(\overline{\theta^T}{\overline{x}}^{(i)}-{y}^{(i)}\right)}^2 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ21.png)

(1.21)

由于输入不再是单一的，我们不再谈论平面上的线，而是一个 *n* 维空间中的*超曲面*——它们是由一个变量定义的 2D 空间中的 1D 线，由两个变量定义的 3D 空间中的 2D 平面，由三个变量定义的 4D 空间中的 3D 空间，等等。成本函数本身将具有![$$ \overline{\theta}=\left({\theta}_0,{\theta}_1,\dots, {\theta}_n\right)\in {\mathfrak{R}}^{n+1} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq37.png)参数-虽然在单变量情况下它是 3D 空间中的抛物面，但在 *n* 输入要素的情况下，它将是 *n* + 1 维表面。这使得多变量情况比单变量情况更难可视化，但我们仍然可以依靠我们的性能指标来评估模型的表现，或者按特征分解线性 *n* 维表面，以分析每个变量相对于输出特征的表现。

通过应用方程 [1.10](#Equ10) 中所示的梯度下降的一般矢量方程，我们也可以以如下方式重写方程 [1.11](#Equ11) 和 [1.12](#Equ12) 中的参数更新公式(记住*x*T8】0= 1 的约定):

![$$ {\theta}_j^{\left(k+1\right)}={\theta}_j^{(k)}-\frac{\alpha }{m}\sum \limits_{i=1}^m\left(\overline{\theta^T}{\overline{x}}^{(i)}-{y}^{(i)}\right){x}_j^{(i)} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ22.png)

(1.22)

我们现在应该有了所有的工具来编写一个仅用`numpy`的多元回归算法:

```
import numpy as np

def gradient_descent(x, y, theta, alpha):
    m = len(x)
    n = len(theta)
    new_theta = np.zeros(n)

    # Perform the gradient descent on theta
    for i in range(m):
        # s = theta[0] + (theta[1]*x[1] + .. + theta[n]*x[n]) - y[i]
        s = theta[0]
        for j in range(1, n):
            s += theta[j]*x[i][j-1]
        s -= y[i]

        new_theta[0] += s
        for j in range(1, n):
            new_theta[j] += s * x[i][j-1]

    return theta - (alpha/m) * new_theta

def train(x, y, steps, alpha=0.001):
    # Initialize theta randomly
    theta = np.random.randint(low=0, high=10, size=len(x[0])+1)

    # Perform the gradient descent <steps> times
    for i in range(steps):
        theta = gradient_descent(x, y, theta, alpha)

    # Return the linear function associated to theta

    def model(x):
        y = theta[0]
        for i in range(len(theta)-1):
            y += theta[i+1] * x[i]
        return y

    return model

```

这些变化将使针对单变量情况分析的线性回归算法也适用于一般的多变量情况。

在前面的代码中，为了清楚起见，我将所有的矢量运算扩展到了`for`语句中，但是请记住，大多数真正的回归算法都使用本地矢量运算来执行矢量和以及标量积(如果有的话)。你现在应该已经注意到梯度下降是一个计算量很大的过程。该算法在一个 *m* 大小的数据集上遍历*个时期*次，每次执行一些矢量和与标量积，每个数据集项由 *n* 个特征组成。一旦你在多个节点上执行梯度下降，事情变得更加计算昂贵，就像在神经网络中一样。以矢量形式表达前面的步骤允许利用硬件或软件中可用的一些优化和并行化。我将把它作为一个带回家的练习，使用矢量原语编写前面的 *n* 维梯度下降算法。

在进入一个实际的例子之前，让我花一些时间来讨论多特征问题中两个相当重要的话题:*特征选择*和*训练/测试集分割*。

### 1.5.1 冗余功能

向模型中添加更多的输入要素通常会使模型在真实示例中更加准确。在我们解决的第一个回归问题中，我们的模型可以仅根据房子的大小来预测房子的价格。我们凭直觉知道，从现实世界的例子中添加更多的特征通常会使价格预测更加准确。我们凭直觉知道，如果我们还输入一些特征，如房屋的建造年份、该道路上房屋的平均价格、是否有阳台或花园，或者离市中心的距离，我们可能会得到更准确的预测。然而，这是有限度的。非常重要的一点是，你提供给你的模型的特征彼此之间是*线性独立的*。给定一个向量列表![$$ \left[{\overline{x}}_1,{\overline{x}}_2,\dots, {\overline{x}}_m\right] $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq38.png)，如果这个列表中的一个向量![$$ {\overline{x}}_i $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq39.png)可以写成

![$$ \overline{x_i}=\sum \limits_{j=1}^m{k}_j\overline{x_j} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ23.png)

(1.23)，那么这个向量就定义为*线性相关*

用![$$ k=\left[{k}_1,{k}_2,\dots, {k}_m\right]\in {\mathfrak{R}}^m $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq40.png)。换句话说，如果一个特征可以表示为任何其他数量的特征的线性组合，那么这个特征就是多余的 T2。冗余特征例子包括

*   用欧元和美元表示的价格相同

*   用米和英尺或米和公里表示的相同距离

*   包含产品净重、皮重和毛重的数据集

*   包含产品的基础价格、增值税率和最终价格的数据集

上述场景都是其他要素线性组合的示例。在将数据集输入模型之前，应移除数据集中所有冗余的要素；否则，模型的预测将会向构成派生特征的特征倾斜，或*偏向*。有两种方法可以做到这一点:

1.  *手动*:查看您的数据，并尝试了解是否有任何属性是冗余的——即，是其他特征的线性组合的特征。

2.  *解析*:你的数据集中有 *m* 个输入，每个输入由 *n* 个特征表示。你可以把它们排列成一个 *m* × *n* 矩阵![$$ X\in {\mathfrak{R}}^{m\times n} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq41.png)。这个矩阵的**秩**、 *ρ* ( *X* )定义为其线性无关向量(行或列)的个数。如果其关联矩阵 *X* 是这样的，我们可以说我们的数据集没有冗余特征

![$$ \rho (X)=\min \left(m,n\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ24.png)

(1.24)

如果*ρ*(*X*)= min(*m*，*n*)–1，那么数据集有一个线性相关向量。如果*ρ*(*X*)= min(*m*，*n*)–2，那么它有两个线性相关向量，以此类推。`numpy`和`scipy`都有内置的方法来计算矩阵的秩，所以如果你想仔细检查数据集中是否有多余的特征，这可能是一个好方法。

尝试删除数据集中的重复行也同样重要，因为它们可能会在某些特定范围内“拉动”您的模型。然而，如果*m*≫*n*-也就是说，如果数据样本的数量远大于要素的数量，则数据集中重复行的影响不会像重复列那样糟糕。

### 主成分分析

从训练集中删除冗余特征的一种分析(并且易于自动化)方法是执行所谓的*主成分分析*，或 *PCA* 。当您有大量的输入要素，并且手动执行函数依赖关系分析可能很麻烦时，PCA 特别有用。PCA 是一种用于*特征提取*的算法，即通过将 *A* 中的点映射到一个新的 *k* 维空间*A*<sup>’</sup>中来减少一个 *n* 维输入空间 *A* 中的维数，使得 *A* 中的特征

PCA 背后的数学初看起来可能有点难，但它依赖于一个非常直观的几何概念，所以请相信我接下来的几个公式。

PCA 的第一步是*特征归一化*，正如我们在方程 [1.13](#Equ13) :

![$$ {\hat{x}}_i=\frac{x_i-{\mu}_x}{\sigma_x}\ \mathrm{for}\ i=1,\dots, n $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ25.png)

(1.25)中看到的

用*μ*<sub>T3】xT5】表示![$ \overline{x} $](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq42.png)的算术平均值， *σ* <sub>* x *</sub> 表示其标准差。</sub>

然后，给定具有输入特征![$$ \overline{x}=\left[{x}_1,{x}_2,\dots, {x}_n\right] $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq43.png)的归一化训练集，我们计算![$$ \overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq44.png)的*协方差矩阵*为

![$$ \mathit{\operatorname{cov}}\left(\overline{x},\overline{x}\right)=\left(\overline{x}-{\mu}_x\right){\left(\overline{x}-{\mu}_x\right)}^T=\left[\begin{array}{ccc}\left({x}_1-{\mu}_x\right)\left({x}_1-{\mu}_x\right)&amp; \dots &amp; \left({x}_1-{\mu}_x\right)\left({x}_n-{\mu}_x\right)\\ {}\left({x}_2-{\mu}_x\right)\left({x}_1-{\mu}_x\right)&amp; \dots &amp; \left({x}_2-{\mu}_x\right)\left({x}_n-{\mu}_x\right)\\ {}\vdots &amp; &amp; \vdots \\ {}\left({x}_n-{\mu}_x\right)\left({x}_1-{\mu}_x\right)&amp; \dots &amp; \left({x}_n-{\mu}_x\right)\left({x}_n-{\mu}_x\right)\end{array}\right] $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ26.png)

(1.26)

您可能还记得线性代数中的两个向量的乘积![$$ {\overline{x}}^T\overline{y} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq45.png)，通常被称为*标量积*或*点积*，它返回一个实数，而两个向量的乘积![$$ \overline{x}{\overline{y}}^T $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq46.png)，返回一个大小为*n*×*n*的矩阵，这就是我们用前面的协方差矩阵得到的结果。向量协方差矩阵背后的直觉是拥有输入分布的几何表示，这就像一维情况下方差概念的多维扩展。

然后我们继续计算![$$ \mathit{\operatorname{cov}}\left(\overline{x},\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq47.png)的*特征向量*。矩阵的*特征向量*定义为当我们对其应用矩阵所描述的几何变换时，保持不变的非零向量(或者至多被标量 *λ* 缩放，称为*特征值*)。例如，考虑我们的星球围绕它的轴的旋转运动。旋转运动可以映射到一个矩阵中，给定地球上的任何一点，该矩阵可以告诉该点在应用旋转后将位于何处。应用旋转时，位置不会改变的点只有那些沿着旋转轴的点。然后，我们可以说旋转轴是与我们行星的旋转相关的矩阵的*特征向量*，并且至少在这种特定情况下，该向量的*特征值*是*λ*= 1——沿着轴的点在旋转期间根本不变；它们甚至没有被缩放。球体旋转的情况具有单个特征向量(旋转轴)，但是其他几何变换可能具有多个特征向量，每个特征向量具有不同的相关特征值。为了使这种直觉形式化，我们说，给定一个描述 n 维空间中某个几何变换的矩阵![$$ A\in {\mathfrak{R}}^{n\times n} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq48.png)，当我们对它应用 *A* 时，它的特征向量![$$ \overline{v} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq49.png)必定是一个至多按因子 *λ* 缩放的向量:

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig6_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig6_HTML.jpg)

图 1-6

归一化数据集的主成分分析。绿色向量表示对数据分布影响更大的组件。您可能希望将输入数据映射到这个新的坐标系。你也可以只选择最大的向量，而不会丢失很多信息

![$$ A\overline{v}=\lambda \overline{v} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ27.png)

(1.27)

通过将前面等式中的![$$ \overline{v} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq50.png)分组，我们得到

![$$ \overline{v}\left(A-\lambda {I}_n\right)=0 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ28.png)

(1.28)

其中 *I* <sub>*n*</sub> 是大小为 *n* × *n* 的*单位矩阵*(主对角线上为 1，其他地方为 0 的矩阵)。前面的向量符号可以展开成一组 *n* 方程，通过求解它们，我们可以得到 *A* 的特征值 *λ* 。通过替换前面方程中的特征值，我们可以得到与矩阵相关的特征向量。

在计算自协方差矩阵的特征向量的背后有一种几何直觉。这些特征向量表明在 n 维输入空间的哪个方向上数据的“扩散”最大。这些方向是您的输入空间的*主成分*，也就是说，这些成分与您的数据分布更相关，因此您可以将原始空间映射到较低维度的新空间，而实际上不会丢失太多信息。如果一个输入特征可以表示为一些其他输入特征的线性组合，那么输入空间的协方差矩阵将具有两个或多个具有相同特征值的特征向量，因此，维数可以被压缩。您还可以通过选择具有 *k* 个最高关联特征值的 *k* 个特征向量，来决定删减一些对数据分布影响很小的特征，即使它们不是严格的线性相关，直观上，这些特征向量是数据分布中最重要的组成部分，图 [1-6](#Fig6) 中显示了一个示例。

一旦我们有了输入空间的主分量，我们需要通过沿着特征向量重新定向输入空间的轴来转换输入空间——注意，这些特征向量是相互正交的，就像笛卡尔平面的轴一样。我们把从选择的主成分(自协方差矩阵的特征向量)构造的矩阵称为 *W* 。给定一个用矩阵 *X* 表示的归一化数据集，它的点将通过

![$$ \hat{X}= XW $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ29.png)

(1.29)映射到新的空间

然后，我们将在新矩阵![$$ \hat{X} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq51.png)上训练我们的算法，新矩阵的维数将等于或低于初始特征数，而没有显著的信息损失。

许多用于机器学习和数据科学的 Python 库已经提供了一些用于主成分分析的功能。使用“scikit-learn”的示例:

```
import numpy as np
from sklearn.decomposition import PCA

# Input vector
x = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])

# Define a PCA model that brings the components down to 2
pca = PCA(n_components=2)

# Fit the input data through the model
pca.fit(x)

```

PCA 有一些明显的优势，其中之一是，它通过分析减少了具有大量特征的输入空间的维数，降低了过度拟合的风险，并提高了算法的性能。然而，它将原始输入空间映射到由主成分构建的新空间，这些新的合成特征可能无法像真实世界的特征(如“大小”、“距离”或“时间”)那样直观地掌握此外，如果您选择的组件数量少于影响数据的组件的实际数量，您可能会丢失信息，因此通常比较应用 PCA 前后模型的性能以检查您是否删除了一些实际需要的功能是一种很好的做法。

### 1.5.3 训练集和测试集

我们看到的第一个线性回归例子是在一个非常小的数据集上训练的；因此，我们决定在同一个数据集上训练和测试模型。然而，在所有现实世界的场景中，您通常会在非常大的数据集上训练您的模型，并且在您的模型尚未被训练的数据点上评估您的模型是很重要的。为此，输入数据集通常分为两部分:

1.  **训练集**包含你的模型将被训练的数据点。

2.  **测试集**包含你的模型将被评估的数据点。

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig8_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig8_HTML.jpg)

图 1-8

看一下自动 MPG 数据集

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig7_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig7_HTML.jpg)

图 1-7

70/30 训练集/测试集拆分示例

这通常是通过*根据预定义的分数分割*你的数据集来完成的——透视表左边的项目将构成训练集，右边的项目将构成测试集，如图 [1-7](#Fig7) 所示。对数据集分割的一些观察:

1.  您想要选择的分割分数在很大程度上取决于您的数据集。如果您有一个非常大的数据集(假设数百万个数据点或更多)，那么您可以选择一个大的训练集分割(例如，90%的训练集和 10%的测试集)，因为即使测试集的部分很小，它仍将包括数万或数十万个项目，并且这仍将足以评估您的模型。在具有较小数据集的场景中，您可能希望试验不同的部分，以在用于训练目的的可用数据的*利用*和为评估您的模型而选择的测试集的*统计显著性*之间找到最佳平衡。换句话说，您可能希望在用于训练目的的可用数据的*利用*和基于统计显著数据集的模型的*评估*之间找到平衡。

2.  如果数据集是根据某个要素排序的，请确保在执行分割之前对其进行洗牌。模型训练和评估的数据尽可能一致是非常重要的。

### 1.5.4 加载和可视化数据集

在本例中，我们将加载汽车 MPG 数据集[15]，该数据集包括几个关于 20 世纪 70-80 年代汽车的参数(气缸、重量、加速度、年份、马力、燃油效率等)。).我们希望建立一个模型，在给定相应输入特征的情况下，预测那些年汽车的燃油效率。

首先，让我们下载数据集，将其加载到我们的笔记本中，并查看它:

```
import pandas as pd
import matplotlib.pyplot as plt

dataset_url = 'http://archive.ics.uci.edu/ml/' +
              'machine-learning-databases/' +
              'auto-mpg/auto-mpg.data'

# These are the dataset columns we are interested in
columns = ['MPG','Cylinders','Displacement','Horsepower',
           'Weight', 'Acceleration', 'Model Year']

# Load the CSV file
dataset = pd.read_csv(dataset_url, names=columns,
    na_values = "?", comment="\t",
    sep=" ", skipinitialspace=True)

# The dataset contains some empty cells - remove them
dataset = dataset.dropna()

# Take a look at the last few records of the dataset
dataset.tail()

```

你可能会看到如图 [1-8](#Fig8) 所示的表格。让我们看看这些特性是如何相互关联的。我们将使用`seaborn`库，特别是`pairplot`方法来可视化每个指标的数据点如何与每个其他指标进行对比:

```
sns.pairplot(dataset[["MPG", "Cylinders", "Displacement", "Weight"]], diag_kind="kde")

```

您应该会看到类似于图 [1-9](#Fig9) 所示的图。每个图表都绘制了由一对指标分解的数据点:这是发现相关性的一个有用的方法。你会注意到，气缸数量、排量和重量与 MPG 有很大关系，而其他指标之间的关系更松散。

我们现在将数据集分成两部分，如第 1.5.3 节所示。训练集将包含 80%的数据，测试集包含剩余的 20%:

```
# Random state initializes the random seed for randomizing the
# seed. If None then it will be calculated automatically
train_dataset = dataset.sample(frac=0.8, random_state=1)

# The test dataset contains all the records after the split
test_dataset = dataset.drop(train_dataset.index)

# Fuel efficiency (MPG) will be our output label, so drop
# it from the training and test datasets
train_labels = train_dataset.pop('MPG')
test_labels = test_dataset.pop('MPG')

```

并将数据标准化:

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig9_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig9_HTML.jpg)

图 1-9

通过 seaborn 了解每个功能与其他功能之间的关系

```
def normalize(x, stats):
    return (x - stats['mean']) / stats['std']

def denormalize(x, stats):
    return x * stats['std'] + stats['mean']

data_stats = train_dataset.describe().transpose()
label_stats = train_labels.describe().transpose()

norm_train_data = normalize(train_dataset, data_stats)
norm_train_labels = normalize(train_labels, label_stats)

```

然后，就像前面的例子一样，我们将定义一个线性模型，并尝试通过我们的数据来拟合它:

```
from tensorflow.keras.experimental import LinearModel

model = LinearModel(len(train_dataset.keys()),
    activation='linear', dtype="float32")

model.compile(optimizer='rmsprop', loss="mse", metrics=['mae', 'mse'])
history = model.fit(norm_train_data, norm_train_labels, epochs=200,
    verbose=0)

```

这次的不同之处在于

*   上例中的模型只有一个输入单元；这一个的输入单元与我们的训练集的列一样多(不包括输出特征)。

*   这次我们使用两个评估指标，都是`mae`和`mse`。在大多数情况下，保留一个主要的评估指标而不是损失函数是一个很好的实践。

让我们再次绘制训练迭代的损失函数:

```
epochs = history.epoch
loss = history.history['loss']

fig = plt.figure()
plot = fig.add_subplot()
plot.set_xlabel('epoch')
plot.set_ylabel('loss')
plot.plot(epochs, loss)

```

您应该会看到如图 [1-10](#Fig10) 所示的图形。

此外，我们现在可以在测试集上评估模型，并查看它在未经训练的数据上的表现:

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig10_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig10_HTML.jpg)

图 1-10

多元回归训练中损失函数的研究进展

```
norm_test_data = normalize(test_dataset, data_stats)
norm_test_labels = normalize(test_labels, label_stats)
model.evaluate(norm_test_data, norm_test_labels)

```

请记住，到目前为止，我们一直在使用一个单一的回归单元:当我们将它们打包到一个神经网络中时，事情会变得更好。我们可以从测试集中挑选一些值，看看模型的预测与预期的标签有多远:

```
sampled_data = norm_test_data.iloc[:10]
sampled_labels = denormalize(norm_test_labels.iloc[:10], label_stats)
predictions = [p[0] for p in
    denormalize(model.predict(sampled_data), label_stats)]

for i in range(10):
    print(f'predicted: {predictions[i]} ' +
        f'actual: {sampled_labels.iloc[i]}')

```

在条形图上绘制这些值应该会产生如图 [1-11](#Fig11) 所示的图形。

然后，您可以对我们在第 1.4.7 节中遇到的`model_save`和`model_load`进行最小的更改，以从另一个笔记本或应用程序中保存和加载您的模型。

## 1.6 多项式回归

我们现在知道如何创建一个或多个输入变量的线性回归模型。我们已经看到，这些模型可以在几何上由 *n* 维线性*超曲面*表示，在 *n* + 1 维空间中，该空间由 *n* 维输入特征加上 *y* 维输出特征组成—该模型将是一条穿过 2D 空间中的点的线。在一元线性回归的情况下，如果有两个输入特征和一个输出变量，则该模型将是一个穿过 3D 空间中的点的 2D 曲面，依此类推。

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig11_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig11_HTML.jpg)

图 1-11

预测值和期望值之间的比较

然而，并不是现实世界中的所有问题都可以用线性模型来近似。考虑`<REPO_DIR>/datasets/house-size-price-2.csv`下的数据集。这是我们之前遇到的房屋大小与价格数据集的变体，但这一次，我们在 100-130 平方米的范围内达到了一个平台期(例如，这种大小或具有特定房间配置的房屋在该位置卖得不多)，然后价格在 130 平方米后再次保持上涨。其表示如图 [1-12](#Fig12) 所示。你不能仅仅用一条直线来捕捉这样的增长-停止-增长序列。更糟糕的是，如果您试图通过我们之前分析的线性过程拟合穿过该数据集的直线，该直线可能会被平台周围的点“拉”下来，以最小化总均方误差，最终也会损失数据集中剩余点的精度。

在这种情况下，您可能想利用一个*多项式*模型。请记住，线性回归可以以最简单的形式建模为*h*<sub>*θ*</sub>(*x*)=*θ*<sub>0</sub>+*θ*<sub>1</sub>*x*，但是没有什么可以阻止我们定义一个假设函数*h*<sub>*θ*</sub>(*x 例如，这种房屋大小-价格非线性模型可能不能很好地用二次函数来表示——记住抛物线首先上升，然后下降，当大小增加时，你不会真的期望房价显著下降。然而，三次模型可以很好地工作——记住，平面上的三次函数看起来像两个“半抛物线”粘在一起，围绕着一个*拐点*，这也是函数的对称点。因此，这种情况下的假设函数可能是这样的:*

*![$$ {h}_{\theta }(x)={\theta}_0+{\theta}_1x+{\theta}_2{x}^2+{\theta}_3{x}^3 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ30.png)*

*(1.30)*

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig12_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig12_HTML.jpg)

图 1-12

无法用线性模型精确表示的房屋大小与价格数据集的示例

找到使前面的函数最小化的![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq52.png)的值的一个聪明的方法是将 *x* 的额外幂视为额外变量，执行变量替换，然后将其视为一般的多元线性回归问题。换句话说，我们希望将多项式函数的梯度下降问题转化为多元线性回归问题。例如，我们可以将前面的*h*<sub>*θ*</sub>(*x*)表达式改写为![$$ \overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq53.png)的函数，其中

![$$ \overline{x}=\left[1,x,{x}^2,{x}^3\right] $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ31.png)

(1.31)

所以*h*<sub>*θ*</sub>(*x*)可以改写为

![$$ {h}_{\theta}\left(\overline{x}\right)={\theta}_0{x}_0+{\theta}_1{x}_1+{\theta}_2{x}_2+{\theta}_3{x}_3 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ32.png)

(1.32)

以这种形式写出的假设与我们在方程 [1.17](#Equ17) 中看到的假设相同。因此，我们可以通过我们分析过的线性多元梯度下降程序继续计算 *θ* 的值，只要您记住以下几点:

1.  当你进行预测时，你从算法中得到的![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq54.png)的值必须插入到方程 [1.30](#Equ30) 中的三次假设函数中，而不是我们在那一节中看到的线性函数中。

2.  特征缩放/输入归一化在回归模型中总是很重要，但是当涉及到多项式回归问题时，它甚至更加重要。如果房子的平方米大小在[0，…，10 <sup>3</sup> 的范围内，那么它的平方值将在[0，…，10 <sup>6</sup> 的范围内，它的立方值将在[0，…，10 <sup>9</sup> 的范围内。如果在将输入输入到模型之前不对其进行归一化，最终会得到一个最高多项式项的权重远远大于其余项的模型，这样的模型可能根本不会收敛。

3.  在前面的示例中，我们选择了三次函数，因为它看起来很适合数据，有些增长，拐点，然后再次增长。然而，这并不适用于所有的模型。一些模型可能在更高的多项式幂(例如， *x* 的 4 次或 5 次幂)或 *x* 的分数次幂(例如，平方根或立方根)下表现更好。或者，在多个输入特征的情况下，一些关系可以通过一些特征的乘积或比率来很好地表达。这里重要的一点是*总是*在推理什么是最适合它的分析函数之前查看你的数据集。有时，您可能还想尝试绘制几个样本假设函数，看看哪一个函数的形状最适合您的数据。

总的来说，将多项式回归问题转化为多元回归问题是一个好主意，因为正如我们之前看到的，线性模型的成本函数通常由简单的 *n* 维二次模型表示，该模型保证只有一个点具有零梯度，并且该点也是全局最小值。在这种配置中，一个设计良好的梯度下降算法应该能够通过简单地跟随梯度向量的方向而收敛到最优解，而不会在对高次多项式函数进行微分时可能会遇到的凸起和凹陷处“卡住”。

## 1.7 正规方程

梯度下降无疑是解决回归问题最流行的方法之一。然而，这不是唯一的方法。正如我们已经看到的，梯度下降通过沿着梯度的方向迭代“行走”直到我们达到最小值，找到优化成本函数的参数![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq55.png)。**正规方程**提供了一种代数方法来一次性计算![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq56.png)。我们很快就会看到，这种方法与梯度下降法相比，既有一些优点，也有一些缺点。在这一节中，我们将简要介绍什么是正规方程以及它是如何推导出来的，而不会过多地深入到形式证明中。我假设你对线性代数和向量/矩阵运算(逆、转置和乘积)有所了解。然而，如果不是这样，请随意跳过这一节，或者只记下最后一个等式。法线方程为最小化问题提供了梯度下降的分析替代方案，但它并不严格要求建立模型。

我们已经看到，回归模型的一般成本函数可以写成

![$$ J\left(\overline{\theta}\right)=\frac{1}{2m}\sum \limits_{i=1}^m{\left(\overline{\theta^T}\overline{x_i}-{y}_i\right)}^2 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ33.png)

(1.33)

而寻找最优模型的问题就是寻找![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq57.png)的值使得![$$ J\left(\overline{\theta}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq58.png)的梯度向量为零:

![$$ \nabla J\left(\overline{\theta}\right)=\left[\begin{array}{c}\frac{\partial }{\partial {\theta}_0}J\left(\overline{\theta}\right)\\ {}\vdots \\ {}\frac{\partial }{\partial {\theta}_n}J\left(\overline{\theta}\right)\end{array}\right]=0 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ34.png)

(1.34)

或者，换句话说:

![$$ \frac{\partial }{\partial {\theta}_j}J\left(\overline{\theta}\right)=0\ \mathrm{for}\ j=0,\dots, n $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ35.png)

(1.35)

如果对一个![$$ X\in {\mathfrak{R}}^{m\times n} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq59.png)数据集展开方程 [1.33](#Equ33) 中的标量积和求和，其中 *n* 是特征的数量， *m* 是输入样本的数量，![$$ \overline{y}\in {\mathfrak{R}}^m $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq60.png)表示输出特征的向量，并求解偏导数，则得到 *n* + 1 个线性方程，其中![$$ \overline{\theta}\in {\mathfrak{R}}^{n+1} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq61.png)表示变量。像所有的线性方程组一样，这个方程组也可以表示为一个矩阵×矢量积的解，我们可以求解相关的方程来计算![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq62.png)。原来![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq63.png)可以通过解以下方程来推断:

![$$ \overline{\theta}={\left({X}^TX\right)}^{-1}{X}^T\overline{y} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ36.png)

(1.36)

其中 *X* 是与数据集的输入要素相关联的 *m* × *n* 矩阵(在每个向量的开头添加了一个*X*T8】0= 1 项，如我们之前所见)，而 *X* <sup>*T*</sup> 表示*转置的*矩阵(即通过交换行和列得到的矩阵)，而<sup>-1</sup>

法线方程比梯度下降算法有几个优点:

1.  你不需要执行多次迭代，也没有陷入困境或发散的风险:![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq65.png)的值可以通过求解 *n* + 1 相关的梯度线性方程组直接计算出来。

2.  因此，你不需要选择一个学习率参数 *α* ，因为不涉及增量学习过程。

然而，它也有一些缺点:

1.  即使输入特征 *n* 的数量非常大，梯度下降仍将表现良好。在你的*θ*-更新步骤中，更多的特征转化为更大的标量积，并且当 *n* 增长时，标量积的复杂度线性增加。另一方面， *n* 的值越大，意味着等式 [1.36](#Equ36) 中的*X*<sup>*T*</sup>*X*矩阵越大，计算一个非常大的矩阵的逆是一个计算量非常大的过程(它有一个*O*(*n*<sup>3</sup>)复杂度)。这意味着，对于输入要素数量较少的回归问题，法线方程可能是一个很好的解决方案，而梯度下降法在包含更多列的数据集上表现更好。

2.  The normal equation works only if the *X*<sup>*T*</sup>*X* matrix is *invertible*. A matrix is invertible only if it is a *full rank* square matrix, that is, its rank equals its number of rows/columns, and therefore, it has a non-zero *determinant*. If *X*<sup>*T*</sup>*X* is not full rank, it means that either you have some linearly dependent rows or columns (therefore, you have to remove some redundant features or rows) or that you have more features than dataset items (therefore, you have to either remove some features or add some training examples). These normalization steps are also important in gradient descent, but while a non-invertible dataset matrix could result in either a biased or non-optimal model if you apply gradient descent, it will fail on a division by zero if you apply the normal equation. However, most of the modern frameworks for machine learning also work in the case of non-invertible matrices, as they use mathematical tools for the calculation of the *pseudo-inverse* such as the *Moore-Penrose inverse*. Anyway, even if the math will still work, keep in mind that a non-invertible characteristic matrix is usually a flag for linearly dependent metrics that may affect the performance of your model, so it’s usually a good idea to prune them before calculating the normal equation.

    ![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig13_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig13_HTML.jpg)

    图 1-13

    逻辑函数图

## 1.8 逻辑回归

线性回归解决了为数值预测创建模型的问题。然而，并不是所有的问题都需要在数值连续域中进行预测。我们之前提到过，分类问题构成了机器学习中的另一大类问题，也就是说，你希望你的模型对所提供的输入的*类*或类型(例如，它是垃圾邮件吗？是异常吗？里面有猫吗？).幸运的是，我们不需要为了使回归过程适应分类问题而对它进行很多修改。我们已经学习了定义一个将![$$ \overline{x}\in {\mathfrak{R}}^n $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq67.png)映射到真实值的![$$ {h}_{\theta}\left(\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq66.png)假设函数。我们只需要找到一个假设函数，它输出这样的值![$$ 0\le {h}_{\theta}\left(\overline{x}\right)\le 1 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq68.png)，并定义一个阈值函数，将输出值映射到一个数值类(例如，0 表示*假*，1 表示*真*)。换句话说，给定一个线性模型![$$ \overline{\theta^T}\overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq69.png)，我们需要找到一个函数 *g* 使得

![$$ {h}_{\theta}\left(\overline{x}\right)=g\left(\overline{\theta^T}\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ37.png)

(1.37)

![$$ 0\le {h}_{\theta}\left(\overline{x}\right)\le 1 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ38.png)

(1.38)

对于 *g* 的一个常见选择是 *sigmoid 函数*，或者**逻辑函数**，这也给这种类型的回归起了个名字**逻辑回归**。变量 *z* 的逻辑函数有如下公式:

![$$ g(z)=\frac{1}{1+{e}^{-z}} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ39.png)

(1.39)

这种功能的形状如图 [1-13](#Fig13) 所示。函数值会随着函数的减小而接近零，随着函数的增大而接近一，函数在 *z* = 0 附近有一个拐点，其值为 0.5。因此，将线性回归的输出映射到[0…1]范围是一个很好的选择，利用原点周围的强非线性来映射“跳跃”。通过将我们的线性模型代入方程 [1.39](#Equ39) ，我们得到逻辑回归的公式:

![$$ {h}_{\theta}\left(\overline{x}\right)=g\left(\overline{\theta^T}\overline{x}\right)=\frac{1}{1+{e}^{-\overline{\theta^T}\overline{x}}} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ40.png)

(1.40)

现在让我们坚持使用单个输出类的逻辑回归(假/真)。我们将很快看到如何将它扩展到多类问题。如果我们坚持这个定义，那么逻辑曲线更早地表达了输入为“真”的概率您可以将逻辑函数的输出值解释为一个*贝叶斯*概率——在给定输入特征的情况下，一个项目属于/不属于输出类的概率:

![$$ {h}_{\theta}\left(\overline{x}\right)=P\left({}^{``}{\mathrm{true}}^{"}|\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ41.png)

(1.41)

给定 sigmoid 函数的形状，我们可以将分类问题形式化为:

![$$ \mathrm{prediction}=\left\{\begin{array}{l}\mathrm{true}\ \mathrm{if}\ g\left(\overline{\theta^T}\overline{x}\right)\ge 0.5\\ {}\mathrm{false}\ \mathrm{if}\ g\left(\overline{\theta^T}\overline{x}\right)&lt;0.5\end{array}\right. $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ42.png)

(1.42)

由于当 *z* ≥ 0 时 *g* ( *z* ) ≥ 0.5，我们可以将前面的表达式重新表述为

![$$ \mathrm{prediction}=\left\{\begin{array}{l}\mathrm{true}\ \mathrm{if}\ \overline{\theta^T}\overline{x}\ge 0\\ {}\mathrm{false}\ \mathrm{if}\ \overline{\theta^T}\overline{x}&lt;0\end{array}\right. $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ43.png)

(1.43)

逻辑回归背后的思想是画一个**决策边界**。如果基础回归模型是线性模型，那么想象在你的数据上画一条线(或者一个*超曲面*)。左边的点代表负值，右边的点代表正值。如果基础模型是更复杂的多项式模型，则决策边界在模拟数据点之间的非线性关系时会更复杂。

我们举个例子:考虑一下`<REPO_DIR>/datasets/spam-email-1.csv`下的数据集。它包含用于垃圾邮件检测的数据集，每一行都包含与电子邮件相关联的元数据。

1.  第一行`blacklist_word_count`，报告电子邮件中有多少单词与垃圾邮件黑名单中的单词相匹配。

2.  第二行`sender_spam_score`是由垃圾邮件过滤器分配的介于 0 和 1 之间的分数，表示基于发件人的电子邮件地址、域或内部域策略，该电子邮件是垃圾邮件的概率。

3.  第三行`is_spam`，如果电子邮件不是垃圾邮件，则为 0，如果电子邮件是垃圾邮件，则为 1。

我们可以绘制数据集，看看指标之间是否有任何关联。我们将把`blacklist_word_count`标在 *x* 轴上，把`sender_spam_score`标在 *y* 轴上，如果是垃圾邮件，用红色表示关联的点，如果不是垃圾邮件，用蓝色表示关联的点:

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig14_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig14_HTML.jpg)

图 1-14

垃圾邮件数据集图

```
import pandas as pd
import matplotlib.pyplot as plt

csv_url = 'https://raw.githubusercontent.com/BlackLight' +
          '/mlbook-code/master/datasets/spam-email-1.csv')

data = pd.read_csv(csv_url)

# Split spam and non-spam rows
spam = data[data['is_spam'] == 1]
non_spam = data[data['is_spam'] == 0]

columns = data.keys()
fig = plt.figure()

# Plot the non-spam data points in blue
non_spam_plot = fig.add_subplot()
non_spam_plot.set_xlabel(columns[0])
non_spam_plot.set_ylabel(columns[1])
non_spam_plot.scatter(non_spam[columns[0]], non_spam[columns[1]], c="b")

# Plot the spam data points in red
spam_plot = fig.add_subplot()
spam_plot.scatter(spam[columns[0]], spam[columns[1]], c="r")

```

您应该会看到如图 [1-14](#Fig14) 所示的图表。我们可以直观地看到，我们可以在平面上大致画一条线来区分垃圾邮件和非垃圾邮件。根据我们选择的线的斜率，我们可能会让一些情况漏过，但一条好的分隔线应该足够精确，以便在大多数情况下做出好的预测。逻辑回归的任务是找到我们可以在方程 [1.40](#Equ40) 中绘制的 *θ* 的参数，以获得良好的预测模型。

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig15_HTML.png](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig15_HTML.png)

图 1-15

具有一个输入变量的逻辑回归成本函数![$$ J\left(\overline{\theta}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq70.png)的示例图

### 1.8.1 成本函数

我们在线性回归中见过的大多数原理(线性或多项式模型的定义、成本/损失函数、梯度下降、特征归一化等。)也适用于逻辑回归。主要区别在于我们如何编写成本函数![$$ J\left(\overline{\theta}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq71.png)。虽然当您想要计算价格预测和实际价格之间的平均误差时，均方误差是一个有意义的指标，但当您想要确定在给定离散数量的类的情况下，您是否预测了数据点的正确类时，它就没有多大意义了。

我们希望构建一个成本函数来表达*分类误差*—即预测的类别是正确还是错误—以及分类误差有多大，即模型对其分类的“确定/不确定”程度。让我们通过调用![$$ C\left({h}_{\theta}\left(\overline{x}\right),y\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq73.png)的新参数:

![$$ J\left(\overline{\theta}\right)=\frac{1}{m}\sum \limits_{i=1}^mC\left({h}_{\theta}\left(\overline{x_i}\right),{y}_i\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ44.png)

(1.44)，重写我们之前定义的成本函数![$$ J\left(\overline{\theta}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq72.png)

当涉及到逻辑回归时，函数 *C* 经常以这种形式表示(我们现在将坚持一个二元分类问题，即只有两个输出类的问题，像*真/假*；我们后面会扩展到多个类):

![$$ C\left({h}_{\theta}\left(\overline{x}\right),y\right)=\left\{\begin{array}{l}-\log \left({h}_{\theta}\left(\overline{x}\right)\right)\ \mathrm{if}\ y=1\\ {}-\log \left(1-{h}_{\theta}\left(\overline{x}\right)\right)\ \mathrm{if}\ y=0\end{array}\right. $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ45.png)

(1.45)

直觉如下:

1.  如果 *y* <sub>*i*</sub> = 1(第 *i* 个数据点的类为正)并且预测值![$$ {h}_{\theta}\left(\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq74.png)也等于 1，那么代价将为零(–log(1)= 0，即没有预测误差)。随着预测值远离 1，成本将逐渐增加。如果*y*<sub>*I*</sub>= 1 并且![$$ {h}_{\theta}\left(\overline{x}\right)=0 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq75.png)，那么代价函数将假设一个无穷大的值( *log* (0) = ∞)。在实际应用中，我们当然不会用无穷大，但我们可能会用一个非常大的数来代替。实际值为 1 而预测值为 0 的情况类似于模型以 100%的置信度预测外面正在下雨，而实际上并没有下雨-如果发生这种情况，您希望通过应用大的成本函数使模型回到正轨。

2.  同样，如果 *y* <sub>*i*</sub> = 0，预测值也等于 0，那么代价函数将为 null(–log(1–0)= 0)。如果取而代之的是 *y* <sub>*i*</sub> = 0 和![$$ {h}_{\theta}\left(\overline{x}\right)=1 $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq76.png)，那么成本将会趋于无穷大。

在一个输入和一个输出变量的情况下，成本函数的曲线将如图 [1-15](#Fig15) 所示。如果我们将等式 [1.45](#Equ45) 中的表达式组合在一起，我们可以将等式 [1.44](#Equ44) 中的逻辑回归成本函数重写为

![$$ J\left(\overline{\theta}\right)=-\frac{1}{m}\left[\sum \limits_{i=1}^m{y}^{(i)}\log {h}_{\theta}\left({\overline{x}}^{(i)}\right)+\left(1-{y}^{(i)}\right)\log \left(1-{h}_{\theta}\left({\overline{x}}^{(i)}\right)\right)\right] $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ46.png)

(1.46)

我们已经将方程 [1.45](#Equ45) 的两个表达式压缩在一起。如果*y*<sub>T5】I</sub>= 1，那么方括号内的和的第一项适用，如果*y*<sub>*I*</sub>= 0，那么第二项适用。

就像线性回归中，寻找![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq77.png)最优值的问题是一个最小化代价函数的问题——也就是执行梯度下降。因此，我们仍然可以应用等式 [1.9](#Equ9) 中所示的梯度更新步骤，以获得到成本函数表面底部的方向。此外，就像线性回归一样，我们利用了一个*凸*成本函数——也就是说，一个具有单个零梯度点的成本函数恰好也是全局最小值。

通过用方程 [1.40](#Equ40) 中定义的逻辑函数替换前面公式中的![$$ {h}_{\theta}\left(\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq78.png)，并求解方程 [1.9](#Equ9) 中的偏导数，我们可以推导出![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq79.png)在*k*+1-逻辑回归步骤:

![$$ {\theta}_j^{\left(k+1\right)}={\theta}_j^{(k)}-\frac{\alpha }{m}\sum \limits_{i=1}^m\left({h}_{\theta}\left({\overline{x}}^{(i)}\right)-{y}^{(i)}\right){x}_j^{(i)}\ \mathrm{for}\ j=0\dots n $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ47.png)

(1.47)

你会注意到，*θ*-更新步骤的公式与我们在方程 [1.22](#Equ22) 中看到的线性回归基本相同，尽管我们是通过不同的途径得到它的。我们可能已经预料到了，因为我们的问题仍然是找到一条在某种程度上符合我们数据的线。唯一的区别是，线性情况下的假设函数 *h* <sub>*θ*</sub> 是![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq80.png)参数和输入向量(![$$ \overline{\theta^T}\overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq81.png))的线性组合，而在逻辑回归情况下，它是我们在方程 [1.40](#Equ40) 中引入的 sigmoid 函数。

### 1.8.2 从头构建回归模型

现在，我们可以将之前看到的线性情况下的梯度下降算法扩展到逻辑回归。让我们实际上把我们到目前为止分析的所有部分(假设函数、成本函数和梯度下降)放在一起，为回归问题建立一个小框架。在大多数情况下，您不需要从头开始构建回归算法，但是这是一个很好的方式来查看我们到目前为止所涉及的概念在实践中是如何工作的。首先，让我们定义逻辑回归的假设函数:

```
import math
import numpy as np

def h(theta):
    """
    Return the hypothesis function associated to
    the parameters theta
"""

def _model(x):
    """
    Return the hypothesis for an input vector x
    given the parameters theta. Note that we use
    numpy.dot here as a more compact way to
    represent the scalar product theta*x
    """
    ret = 1./(1 + math.exp(-np.dot(theta, x)))

    # Return True if the hypothesis is >= 0.5,
    # otherwise return False
    return ret >= 0.5

return _model

```

注意，如果我们用标量积![$$ \overline{\theta^T}\overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq82.png)替换前面的假设函数，我们将逻辑回归问题转化为线性回归问题:

```
import numpy as np

def h(theta):
    def _model(x):
        return np.dot(theta, x)
    return _model

```

然后让我们编写梯度下降算法:

```
def gradient_descent(x, y, theta, alpha):
    """
    Perform the gradient descent.

    :param x: Array of input vectors
    :param y: Output vector
    :param theta: Values for theta
    :param alpha: Learning rate
    """

    # Number of samples
    m = len(x)
    # Number of features+1
    n = len(theta)
    new_theta = np.zeros(n)

    # Perform the gradient descent on theta
    for j in range(n):
        for i in range(m):
            new_theta[j] += (h(theta)(x[i]) - y[i]) * x[i][j]
            new_theta[j] = theta[j] - (alpha/m) * new_theta[j]

        return new_theta

```

然后是由*个时期*个梯度下降迭代组成的`train`方法:

```
def train(x, y, epochs, alpha=0.001):
    """
    Train a model on the specified dataset

    :param x: Array of normalized input vectors
    :param y: Normalized output vector
    :param epochs: Number of training iterations
    :param alpha: Learning rate
    """
    # Set x0=1
    new_x = np.ones((x.shape[0], x.shape[1]+1))
    new_x[:, 1:] = x
    x = new_x

    # Initialize theta randomly
    theta = np.random.randint(low=0, high=10, size=len(x[0]))

    # Perform the gradient descent <epochs> times
    for i in range(epochs):
        theta = gradient_descent(x, y, theta, alpha)

    # Return the hypothesis function associated to
    # the parameters theta
    return h(theta)

```

最后，一个预测函数，给定一个输入向量、数据集的统计数据和模型

1.  归一化输入向量

2.  设置*x*T2 0= 1

3.  根据给定的假设返回预测值*h*<sub>T3】θT5】</sub>

```
def normalize(x, stats):
    return (x - stats['mean']) / stats['std']

def denormalize(x, stats):
    return stats['std'] * x + stats['mean']

def predict(x, stats, model):
    """
    Make a prediction given a model and an input vector
    """

# Normalize the values
x = normalize(x, stats).values
# Set x0=1
x = np.insert(x, 0, 1.)

# Get the prediction
return model(x)

```

最后，一个给出输入值和预期输出列表的`evaluate`函数评估给定假设函数的准确度(正确分类的输入数除以输入总数):

```
def evaluate(x, y, stats, model):
    """
    Evaluate the accuracy of a model.

    :param x: Array of input vectors
    :param y: Vector of expected outputs
    :param stats: Input normalization stats
    :param model: Hypothesis function
    """
    n_samples = len(x)
    ok_samples = 0

    for i, row in x.iterrows():
        expected = y[i]
        predicted = predict(row, stats, model)
        if expected == predicted:
            ok_samples += 1

    return ok_samples/n_samples

```

现在，让我们通过在我们之前加载的数据集上训练和评估垃圾邮件检测模型来尝试这个框架:

```
columns = dataset_stats.keys()
# x contains the input features (first two columns)
inputs = data.loc[:, columns[:2]]
# y contains the output features (last column)
outputs = data.loc[:, columns[2]]

# Get the statistics for inputs and outputs
x_stats = inputs.describe().transpose()
y_stats = outputs.describe().transpose()

# Normalize the features
norm_x = normalize(inputs, x_stats)
norm_y = normalize(outputs, y_stats)

# Train a classifier on the normalized data
spam_classifier = train(norm_x, norm_y, epochs=100)

# Evaluate the accuracy of the classifier
accuracy = evaluate(inputs, outputs, x_stats, spam_classifier)
print(accuracy)

```

如果我们回头看看原始数据是如何分布的，以及我们定义了一个线性决策边界的事实，希望您应该测量出超过 85%的准确性，这并不是那么糟糕。

### 1.8.3 TensorFlow 方式

现在，我们已经了解了回归模型的所有细节，让我们构建一个逻辑回归模型，用 TensorFlow 解决我们的垃圾邮件分类问题。我们之前看到的线性回归的例子只需要做一些调整:

```
from tensorflow.keras.experimental import LinearModel

columns = dataset_stats.keys()

# Input features are on the first two columns
inputs = data.loc[:, columns[:2]]

# Output feature is on the last column
outputs = data.loc[:, columns[2:]]

# Normalize the inputs
x_stats = inputs.describe().transpose()
norm_x = normalize(inputs, x_stats)

# Define and compile the model
model = LinearModel(2, activation="sigmoid", dtype="float32")
model.compile(optimizer='sgd', loss="sparse_categorical_crossentropy",
    metrics=['accuracy', 'sparse_categorical_crossentropy'])

# Train the model
history = model.fit(norm_x, outputs, epochs=700, verbose=0)

```

与线性模型相比，前面的代码有一些变化:

*   我们使用方程 [1.39](#Equ39) 中定义的`sigmoid`激活函数，而不是`linear`。

*   我们使用`sgd`优化器— *随机梯度下降*。

*   我们使用分类交叉熵损失函数，类似于等式 [1.46](#Equ46) 中定义的函数。

*   我们使用`accuracy`(即正确分类的样本数除以样本总数)作为性能指标。

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig16_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig16_HTML.jpg)

图 1-16

logistic 模型在训练时期的损失函数

让我们看看损失函数在训练迭代中是如何发展的:

```
epochs = history.epoch
loss = history.history['loss']

fig = plt.figure()
plot = fig.add_subplot()
plot.set_xlabel('epoch')
plot.set_ylabel('loss')
plot.plot(epochs, loss)

```

您应该会看到如图 [1-16](#Fig16) 所示的图表。

我们在线性回归案例中看到的所有其他建模方法——`evaluate`、`predict`、`save`和`load`——也适用于逻辑回归案例。

### 多类回归

到目前为止，我们已经讨论了多个输入要素的逻辑回归，但只有一个输出类——对或错。扩展逻辑回归以处理多个输出类是一个非常直观的过程，称为 *one vs. all* 。

假设在我们之前看到的情况下，不是二进制垃圾邮件/非垃圾邮件分类，而是有三种可能的分类:*正常*、*重要*和*垃圾邮件*。其思想是将原来的分类问题分解成三个二元逻辑回归假设函数 *h* <sub>*θ*</sub> ，每个类一个:

1.  对决策边界*正常/不正常*建模的函数![$$ {h}_{\theta}^{(1)}\left(\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq83.png)

2.  对决策边界*重要/不重要*建模的函数![$$ {h}_{\theta}^{(2)}\left(\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq84.png)

3.  对决策边界*垃圾邮件/非垃圾邮件*建模的函数![$$ {h}_{\theta}^{(3)}\left(\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq85.png)

![../images/497180_1_En_1_Chapter/497180_1_En_1_Fig17_HTML.jpg](../images/497180_1_En_1_Chapter/497180_1_En_1_Fig17_HTML.jpg)

图 1-17

具有非线性决策边界的数据集的绘图

每个假设函数表示输入向量属于特定类别的概率。因此，具有最高值的假设函数是输入所属的函数。换句话说，给定一个输入![$$ \overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq86.png)和 *c* 类，我们想要为![$$ \overline{x} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq87.png)选择具有最高相关假设函数值的类:

![$$ \underset{0\le i\le c}{\max }{h}_{\theta}^{(i)}\left(\overline{x}\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ48.png)

(1.48)

直觉告诉我们，具有最高值的假设函数代表了具有最高概率的类，这就是我们想要选择的预测。

### 非线性边界

到目前为止，我们已经探索了逻辑回归模型，其中 sigmoid 的自变量是线性函数。然而，就像线性回归一样，并不是所有的分类问题都可以通过绘制线性决策边界来建模。图 [1-17](#Fig17) 中数据集的分布绝对不适合线性决策边界，但椭圆决策边界可以很好地完成这项工作。就像我们在线性回归中看到的那样，通过变量替换，非线性模型函数可以有效地转化为线性多元函数，然后我们可以最小化该函数，以获得![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq88.png)的值。在前面的例子中，一个好的假设函数可能是这样的:

![$$ {h}_{\theta}\left(\overline{x}\right)=g\left({\theta}_0{x}_0+{\theta}_1{x}_1+{\theta}_2{x}_2+{\theta}_3{x}_1{x}_2+{\theta}_4{x}_1^2+{\theta}_5{x}_2^2\right) $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_Equ49.png)

(1.49)

然后，我们可以应用我们已经看到的变量替换程序，用新变量替换高次多项式项，并继续用线性模型解决相关的多元回归问题，以获得![$$ \overline{\theta} $$](../images/497180_1_En_1_Chapter/497180_1_En_1_Chapter_TeX_IEq89.png)的值。就像线性回归一样，为了更好地表达复杂的决策边界，可以添加到模型中的额外多项式项的数量没有限制-无论如何要考虑到，添加太多多项式项可能最终会对决策边界进行建模，这可能会使数据过度拟合。正如我们将在后面看到的，检测非线性边界的另一种常见方法是将多个逻辑回归单元连接在一起——也就是说，建立一个*神经网络*。