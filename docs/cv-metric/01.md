# 一、图像捕获和表示

身体转化为光，光转化为身体，这非常符合自然的进程，自然似乎很喜欢这种转化

-艾萨克牛顿

计算机视觉从图像开始。本章概述了一系列涉及图像捕捉、处理和表示的主题，包括计算成像、2D 成像和 3D 深度成像方法、传感器处理、立体和单目多视图立体的深度场处理以及表面重建。提供了所选主题的高层次概述，并为感兴趣的读者提供了深入挖掘的参考。在 2D 和 3D 成像领域有深厚背景的读者可以从本章的轻松阅读中受益。

图像传感器技术

本节提供图像传感器技术的基本概述，作为理解图像如何形成以及开发图像传感器处理的有效策略以优化计算机视觉图像质量的基础。

典型的图像传感器由 CCD 单元 (电荷耦合器件 )或标准 CMOS 单元(互补金属氧化物半导体)制成。CCD 和 CMOS 传感器具有相似的特性，都广泛用于商业相机。然而，今天的大多数传感器使用 CMOS 单元，主要是出于制造考虑。传感器和光学系统通常被集成在一起，以创建*晶圆级相机* 用于生物学或显微镜等应用，如图[图 1-1](#Fig1) 所示。

![9781430259299_Fig01-01.jpg](img/images/9781430259299_Fig01-01.jpg)

[图 1-1](#_Fig1) 。具有光学器件和滤色器的通用集成图像传感器装置

图像传感器旨在实现不同应用的特定设计目标，提供不同水平的灵敏度和质量。查阅厂家资料，熟悉每个传感器。例如，针对给定的半导体制造工艺优化每个光电二极管传感器单元元件的尺寸和材料成分，以便实现硅管芯面积和光强度和颜色检测的动态响应之间的最佳折衷。

对于计算机视觉，采样理论的影响是相关的——例如，应用于目标场景像素覆盖的奈奎斯特频率。传感器分辨率和光学器件必须一起为每个像素提供足够的分辨率，以对感兴趣的特征进行成像，因此，感兴趣的特征应该以对该特征重要的最小像素的最小尺寸的两倍进行成像或采样。当然，2 倍过采样只是精度的最低目标；实际上，单像素宽的特征不容易分辨。

为了获得最佳结果，应该针对给定的应用对相机系统进行校准，以确定不同照明和距离情况下的传感器噪声和像素位深度的动态范围。应该开发适当的传感器处理方法，以处理任何颜色通道的传感器的噪声和非线性响应，检测和校正坏像素，并处理几何失真的建模。如果您设计了一种简单的校准方法，使用具有精细和粗糙灰度、颜色和特征像素大小的测试图案，您可以查看结果。在第 2 章中，我们调查了一系列适用于传感器处理的图像处理方法。但是，让我们从调查传感器材料开始。

传感器材料

硅基图像传感器是最常见的，尽管其他材料如镓(Ga)也用于工业和军事应用，以覆盖比硅能达到的更长的红外波长。图像传感器的分辨率范围取决于所用的相机，从单像素光电晶体管相机，到工业应用的 1D 线扫描阵列，再到普通相机的 2D 矩形阵列，一直到高分辨率成像的球形阵列。(传感器配置和摄像机配置将在本章后面介绍。)

常见的成像传感器使用硅作为 CCD、CMOS、BSI 和 Foveon 方法 s 制成，本章稍后将对此进行讨论。硅图像传感器具有非线性光谱响应曲线；光谱的近红外部分被很好地感应到，而蓝色、紫色和近紫外被不太好地感应到，如图 1-2 中的[所示。请注意，读取原始传感器数据并将数据量化为数字像素时，必须考虑硅光谱响应。传感器制造商在这方面进行设计补偿；然而，在校准您的相机系统和为您的应用设计传感器处理方法时，也应该考虑传感器颜色响应。](#Fig2)

![9781430259299_Fig01-02.jpg](img/images/9781430259299_Fig01-02.jpg)

[图 1-2](#_Fig2) 。几种硅光电二极管的典型光谱响应。请注意，在 900 纳米左右的近红外范围内灵敏度最高，在 400-700 纳米的可见光谱范围内具有非线性灵敏度。由于正常的硅响应，从相机中移除 IR 滤光器增加了近红外灵敏度。(光谱数据图像 OSI 光电公司，经许可使用)

传感器光电二极管电池

图像传感中的一个关键考虑因素是光电二极管尺寸或单元尺寸。使用小型光电二极管的传感器单元将无法捕获与大型光电二极管一样多的光子。如果单元尺寸低于要捕获的可见光的波长，例如 400nm 的蓝光，则在传感器设计中必须克服额外的问题以校正图像颜色。传感器制造商非常注意设计最佳尺寸的单元，以同样好地成像所有颜色([图 1-3](#Fig3) )。在极端情况下，由于缺乏积累的光子和传感器读数噪声，小型传感器可能对噪声更敏感。如果光电二极管传感器单元太大，也没有任何好处，芯片尺寸和硅成本增加，没有任何优势。普通的商用传感器设备可能具有大约 1 平方微米或更大的传感器单元尺寸；然而，每个制造商都是不同的，为了达到特定的要求，需要进行权衡。

![9781430259299_Fig01-03.jpg](img/images/9781430259299_Fig01-03.jpg)

[图 1-3](#_Fig3) 。波长的原色分配。请注意，原色区域重叠，绿色是所有颜色的良好单色代理

传感器配置:Mosaic、Foveon、BSI

多光谱传感器设计有多种片上配置，包括镶嵌和堆叠方式，如图[图 1-4](#Fig4) 所示。在*镶嵌法*中，滤色片以镶嵌图案排列在每个单元上方。*FOV eon*[<sup>1</sup>](#Fn2)*传感器堆叠方法* 依赖于颜色波长穿透到半导体材料中的深度物理学，其中每种颜色穿透到硅的不同深度，从而对单独的颜色进行成像。总单元尺寸容纳所有颜色，因此不需要为每种颜色设置单独的单元。

![9781430259299_Fig01-04.jpg](img/images/9781430259299_Fig01-04.jpg)

[图 1-4](#_Fig4) 。(左)堆叠 RGB 单元以吸收不同深度的不同波长的 Foveon 方法，每个单元位置都有所有 RGB 颜色。(右)标准马赛克单元布局，每个光电二极管上方有 RGB 滤光片，滤光片仅允许特定波长的光进入每个光电二极管

*背面照明*(BSI)传感器配置重新排列芯片上的传感器布线，以允许更大的单元面积和在每个单元中积累更多的光子。有关正面和背面芯片电路布置的比较，请参见 Aptina [410]白皮书。

传感器单元的排列也影响颜色响应。例如，[图 1-5](#Fig5) 显示了原色( *R、G、B* )传感器以及白色( *W* )传感器的各种排列，其中 *W* 传感器有一个透明或中性滤色器。传感器单元排列允许一系列像素处理选项，例如，在传感器处理期间，在相邻单元的各种配置中组合所选像素，以形成优化颜色响应或空间颜色分辨率的像素。事实上，一些应用只是使用原始传感器数据并执行自定义处理，以提高分辨率或开发替代的颜色混合。

![9781430259299_Fig01-05.jpg](img/images/9781430259299_Fig01-05.jpg)

[图 1-5](#_Fig5) 。几种不同的细胞颜色镶嵌配置，包括白色、原色 RGB 和次生 CYM 细胞。每种配置都为传感器处理提供了不同的选项，以优化颜色或空间分辨率。(图片由英特尔出版社授权使用，来自构建智能系统)

传感器的整体尺寸和格式也决定了镜头的尺寸。一般来说，更大的镜头可以让更多的光进入，所以更大的传感器通常更适合摄影应用的数码相机。此外，芯片上的单元布局纵横比决定了像素的几何形状，例如，数码相机通常采用 4:3 的纵横比，而 35mm 胶片则采用 3:2 的纵横比。传感器配置细节值得了解，以便您可以设计最佳的传感器处理和图像预处理流水线。

动态范围和噪声

当前最先进的传感器为每个颜色单元提供至少 8 位，通常为 12 至 14 位。传感器电池需要面积和时间来积累光子，因此必须仔细设计更小的电池以避免问题。噪声可能来自光学器件、滤色片、传感器单元、增益和模数转换器、后处理或压缩方法(如果使用的话)。传感器读出噪声也会影响有效分辨率，因为每个像素单元都是从传感器中读出，发送到 A/D 转换器，并形成数字行和列以转换为像素。更好的传感器将提供更少的噪声和更高的有效位分辨率。Ibenthal 的工作[409]对去噪进行了很好的调查。

此外，每种颜色的传感器光子吸收是不同的，对于蓝色来说可能是有问题的，蓝色对于较小的传感器来说可能是最难成像的颜色。在某些情况下，制造商可能试图为每种颜色提供一种简单的内置于传感器中的伽马曲线校正方法，但这并不推荐。对于要求苛刻的色彩应用，考虑比色设备模型和色彩管理(将在第 2 章中讨论)，或者甚至通过表征传感器每个色彩通道的非线性并开发一组简单的校正 LUT 变换。(适用于深度传感的噪声过滤方法也在[第二章](02.html)中有所涉及。)

传感器处理

需要传感器处理来对来自传感器阵列的像素进行去马赛克和组合，并校正感测缺陷。本节我们将讨论传感器处理的基础知识。

通常，在每个成像系统中提供专用传感器处理器，包括快速硬件传感器接口、优化的 VLIW 和 SIMD 指令，以及专用的固定功能硬件模块，以处理传感器处理的大规模并行像素处理工作负载。通常，传感器处理是透明的、自动的，并且由成像系统的制造商设置，并且来自传感器的所有图像都以相同的方式处理。可以存在旁路来提供原始数据，该原始数据可以允许用于诸如数字摄影的应用的定制传感器处理。

去马赛克

根据传感器单元的配置，如图 1-5 中的[所示，采用各种去马赛克算法从原始传感器数据中创建最终的 *RGB* 像素。Losson 和 Yang [406]以及 Li 等人[407]所做的一项出色调查为所涉及的挑战和所采用的各种方法提供了一些背景资料。](#Fig5)

去马赛克的主要挑战之一是像素插值，以将来自附近单元的颜色通道组合成单个像素。给定传感器单元布局的几何形状和单元布局的纵横比，这不是一个小问题。一个相关的问题是颜色单元权重 *—* 例如，每种颜色有多少应该被整合到每个 *RGB* 像素中。由于镶嵌传感器中的空间像元分辨率大于最终组合的 *RGB* 像素分辨率，因此一些应用需要原始传感器数据来利用所有可能的精度和分辨率，或者执行特殊处理以提高有效像素分辨率，或者更好地完成空间精确色彩处理和去镶嵌。

坏像素校正

像 LCD 显示器这样的传感器可能会有坏像素。厂商可以在工厂校准传感器，并提供已知缺陷的传感器缺陷图，提供这些坏像素的坐标，用于在相机模块或驱动软件中进行校正。在一些情况下，在传感器上使用自适应缺陷校正方法[408]来监控相邻像素，以主动寻找缺陷，然后校正一系列缺陷类型，例如单个像素缺陷、列或行缺陷以及例如 2×2 或 3×3 簇的缺陷。相机驱动程序还可以提供自适应缺陷分析，以实时查找缺陷，并可能在相机设置菜单中提供特殊的补偿控制。

颜色和照明校正

需要色彩校正来平衡整体色彩准确度和白平衡。如图 1-2 中的[所示，硅传感器对红色和绿色的颜色敏感度通常非常好，但对蓝色的敏感度较差，因此提供最准确颜色的机会始于理解和校准传感器。](#Fig2)

大多数图像传感器处理器都包含一个用于渐晕校正的几何处理器，，它表现为图像边缘较暗的照明，如[第 7 章](07.html) ( [图 7-6](07.html#Fig6) )所示。校正基于几何扭曲函数，该函数在工厂校准以匹配光学渐晕图案，允许可编程照明函数增加朝向边缘的照明。关于适用于渐晕的图像扭曲方法的讨论，参见参考文献[490]。

几何校正

镜头可能有几何像差或可能向边缘弯曲，产生径向失真的图像，这个问题与上面讨论的渐晕有关，并在第 7 章 ( [图 7-6](07.html#Fig6) )中显示。为了处理镜头失真，大多数成像系统都有一个专用的传感器处理器，带有硬件加速的数字扭曲单元，类似于 GPU 中的纹理采样器。几何校正在工厂中为光学系统进行了校准和编程。关于图像扭曲方法的讨论，参见参考文献[490]。

相机和计算成像

许多新颖的相机配置正在进入商业应用，使用*计算成像*方法从原始传感器数据合成新图像——例如，深度相机和高动态范围相机。如图 1-6 中的[所示，传统的相机系统](#Fig6) 使用单个传感器、镜头和照明器来创建 2D 图像。然而，计算成像相机可以提供多个光学器件、多个可编程照明模式和多个传感器，从而实现新颖的应用，例如 3D 深度感测和图像重新照明，利用深度信息，将图像作为纹理映射到深度图上，以及引入新的光源，然后在图形流水线中重新渲染图像。由于计算相机开始出现在消费设备中，并将成为计算机视觉流水线的前端，我们调查了一些使用的方法。

![9781430259299_Fig01-06.jpg](img/images/9781430259299_Fig01-06.jpg)

[图 1-6](#_Fig6) 。计算成像系统与传统照相机的比较。(上图)简单的相机模型，带有闪光灯、镜头和成像设备，之后是图像增强，如锐化和色彩校正。(下图)使用可编程闪光灯、光学阵列和传感器阵列的计算成像，然后是计算成像应用

计算成像概述

计算成像[447，414]提供了从原始图像数据合成新图像的选项。计算相机可以控制可编程闪光图案投影仪、透镜阵列和多个图像传感器，以及从原始数据合成新图像，如图[图 1-6](#Fig6) 所示。要更深入地挖掘计算成像并探索当前的研究，请参见哥伦比亚大学的 CAVE 计算机视觉实验室和罗切斯特技术成像研究所。下面是一些正在使用的方法和应用。

单像素计算相机

单像素计算相机可以从同一场景的一系列单个光电探测器像素图像中重建图像。单像素相机领域[103，104]属于压缩传感研究领域，该领域还具有图像处理之外的应用，延伸到诸如模数转换的领域。

如图[图 1-7](#Fig7) 所示，*单像素相机*可以使用类似于衍射光栅的*微镜阵列*或*数字镜器件*(DMD)。光栅排列成矩形微镜栅格阵列，允许栅格区域被打开或关闭以产生二进制栅格图案。二进制模式被设计为伪随机二进制基集。通过组合相邻区域的图案来调整网格图案的分辨率，例如，2x2 或 3x3 微镜区域的网格。

![9781430259299_Fig01-07.jpg](img/images/9781430259299_Fig01-07.jpg)

[图 1-7](#_Fig7) 。一种单像素成像系统，其中入射光通过微镜的 DMD 阵列反射到单个光电二极管上。微镜阵列中的网格位置可以对光打开或关闭，如图所示，以创建二元图案，其中白色网格正方形是反射性的和开放的，而黑色网格正方形是封闭的。(图片经许可使用，R. G. Baraniuk，压缩传感讲义)

通过一组伪随机微透镜阵列图案拍摄一系列单像素图像，然后从该组图像中重建图像。事实上，重建图像所需的模式样本数量低于奈奎斯特频率，因为使用了稀疏随机采样方法，并且随机采样方法在研究中已被证明在数学上是足够的[103，104]。网格基集采样方法直接适用于图像压缩，因为只采集相对稀疏的一组模式和样本。因为微镜阵列使用矩形形状，所以图案类似于一组 HAAR 基函数。(更多信息参见[图 2-20](02.html#Fig20) 和[图 6-22](06.html#Fig22) 。)

DMD 方法是值得注意的，因为可以从单个光电探测器拍摄的相当小的一组图像中重建图像，而不是像 CMOS 或 CCD 图像传感器那样从光电探测器的 2D 阵列中重建图像。由于仅使用单个传感器，该方法有望应用于由 CMOS 和 CCD 传感器成像的近红外和可见光谱之外的波长。例如，使用对不可见波长敏感的非硅传感器，DMD 方法可用于检测隐藏的武器或物质在不可见波长的发射。

2D 计算相机

可编程 2D 传感器阵列、透镜和照明器的新配置正被开发成相机系统，如*计算相机*【424，425，426】，其应用范围从数字摄影到军事和工业用途，采用计算成像方法来增强事后图像。计算相机借用了共焦成像[419]和共焦显微镜[421，420]的许多计算成像方法，例如，使用多个照明模式和多个焦平面图像。他们还利用第二次世界大战后开发的合成孔径雷达系统[422]的研究，使用单个移动相机平台的宽基线数据创建高分辨率图像和 3D 深度图。使用多个图像传感器的合成孔径 和使用晶圆级集成的重叠视野光学系统也是研究的主题【419】。我们在这里调查一些计算 2D 传感器方法，包括*高分辨率* (HR) *、高动态范围* (HDR)和*高帧率* (HF)相机。

目前商用数字百万像素相机的范围从大约 1000 万像素以上，提供的分辨率相当于或超过 35mm 相机中使用的高端胶片[412]，因此图像传感器的像素大小相当于最佳分辨率胶片上的一粒银。从表面上看，似乎没有什么动机去追求更高的分辨率用于商业用途，因为目前的数字方法已经取代了大多数电影应用，电影打印机已经超过了人眼的分辨率。

然而，非常高分辨率的千兆像素成像设备被设计和构造为图像传感器和透镜的阵列，为拍摄图像后的计算成像提供了优势。一种配置是由图像传感器的正交 2D 阵列和相应的光学器件组成的 *2D 阵列相机*、；另一种配置是*球形摄像机* 如图[图 1-8](#Fig8)【411，415】，是作为 DARPA 研究项目在哥伦比亚大学 CAVE 开发的。

![9781430259299_Fig01-08a.jpg](img/images/9781430259299_Fig01-08a.jpg)

![9781430259299_Fig01-08b.jpg](img/images/9781430259299_Fig01-08b.jpg)

[图 1-8](#_Fig8) 。(上图)超高分辨率千兆像素相机的组件，采用新颖的球形镜头和传感器排列。(下图)以 82，000 x 22，000 = 1.7 千兆像素显示的高分辨率图像。(所有数字和图像均由哥伦比亚大学洞穴研究项目使用)

*高动态范围* (HDR)相机【416，417，418】通过拍摄不同曝光设置的场景的多幅图像，然后将这些图像组合起来，可以产生更深的像素，具有更高的位分辨率和更好的颜色通道分辨率。这种组合使用合适的加权方案来产生具有更高位深度的更深像素的新图像，例如每个颜色通道 32 个像素，从而提供超出普通商业 CMOS 和 CCD 传感器能力的图像。HDR 方法允许弱光和强光同样良好地成像，并且可以使用自适应局部方法来组合弱光和强光，以消除眩光并创建更均匀和令人满意的图像对比度。

*高帧率* (HF)相机【425】能够将场景的快速连续图像捕捉成一组，并使用包围技术组合这组图像，以改变曝光、闪光、聚焦、白平衡和景深。

3D 深度相机系统

将 3D 深度场用于计算机视觉为许多应用提供了不言而喻的优势，因为计算机视觉在很大程度上关注于从 2D 图像中提取 3D 信息，这导致了各种各样的精度和不变性问题。正在为 3D 深度场计算机视觉设计新的 3D 描述符，并在第 6 章中讨论。

使用深度图，可以很容易地将场景分割成前景和背景，以识别和跟踪简单的对象。数字摄影应用在三维空间中结合了各种计算机视觉方法，从而变得更加丰富。使用 3D 深度图的所选区域作为遮罩实现了局部图像增强，例如基于深度的对比度、锐化或其他预处理方法。

如[表 1-1](#Tab1) 所示，从图像中提取深度的方法有很多。在某些情况下，只需要一个摄像头和传感器，其余的由软件完成。请注意，*照明方法* 是许多深度传感方法的关键组成部分，如结构光方法。传感器、镜头和照明的组合用于深度成像和计算成像，如图[图 1-9](#Fig9) 所示。本节我们将探讨几种精选的深度检测方法。

[表 1-1](#_Tab1) 。获取深度信息的选定方法

![image](img/images/Table1-1a.jpg) ![image](img/images/Table1-1b.jpg)

*Source: Courtesy of Ken Salsmann Aptina [427], with a few other methods added by the author.*

![9781430259299_Fig01-09.jpg](img/images/9781430259299_Fig01-09.jpg)

[图 1-9](#_Fig9) 。常见相机的多种镜头和传感器配置:a .常规，b .飞行时间，c .立体，d .阵列，e .全光，f .球透镜球面

深度感测并不是一个新的领域，在几个具有巨大工业应用和财政资源的相关学科中都有很好的覆盖，例如卫星成像、遥感、摄影测量和医学成像。然而，随着 Kinect 等商用深度传感相机的出现，涉及深度传感的主题在计算机视觉中越来越受关注，这使得预算有限的研究生可以使用手机或 PC 来实验 3D 深度图和点云。

*多视角立体* (MVS) 深度感测已经被用于计算数字高程图或 DEM，以及数字地形图或 DTM，从使用雷达和激光雷达成像的卫星图像，以及从使用特别装备的具有高分辨率相机和稳定相机平台的飞机的区域航空勘测，包括与相邻区域的照片叠加在一起的数字地形图。*照片拼接* 是计算机视觉中的一个相关话题，正在受到关注。关于*数字地形制图* 的文献中有大量关于合适的几何模型和视差计算方法的信息。此外，*通过 CAT 和 MRI 模态的 3D 医学成像* 得到了丰富的研究社区的支持，使用出色的深度传感方法，并提供基于深度的渲染和可视化。然而，观察一个领域(如计算机视觉)对其他领域使用的众所周知的方法的“再发明”总是很有趣的。正如所罗门所说:“日光之下，并无新事。”在本节中，我们引用相关研究，在计算机视觉的背景下研究深度感测，并将其他相关学科的有趣旅程留给感兴趣的读者。

双目立体

Stereo [432，433，437]可能是捕捉 3D 深度图的最基本和最熟悉的方法，因为许多方法和算法都在使用，所以我们在这里提供了一个高层次的概述，并选择了一些标准参考。立体算法的第一步是通过确定摄像机系统的*立体校准*参数来参数化从世界坐标点到其相应图像坐标的投影变换。开源软件可用于立体声校准。 [<sup>2</sup>](#Fn3) 注意，L/R 图像对在搜索用于视差计算的特征之前被校正。计算立体深度 *r* ，如图[图 1-10](#Fig10) 所示。

![9781430259299_Fig01-10.jpg](img/images/9781430259299_Fig01-10.jpg)

[图 1-10](#_Fig10) 。基本双目立体原理的简化示意图

Scharstein 和 Szeliski [440]以及 Lazaros [441]的工作对立体算法和方法进行了出色的调查。立体几何是由*射影*和*欧几里得*【437】组合而成；我们将在本节的后面讨论一些影响其精度的几何问题。用于比较立体算法的标准在线资源由米德尔伯里学院提供， [<sup>3</sup>](#Fn4) 许多新算法在此进行了基准测试并提供了比较结果，包括[附录 B](10.html) 中讨论的大量真实情况数据集。

立体深度所需的基本几何校准信息包括以下基本内容。

*   **摄像机校准参数。**相机校准超出了本工作的范围，但是参数被定义为 11 个自由参数[435，432]—3 个用于旋转，3 个用于平移，5 个固有参数—加上一个或多个镜头失真参数，以从 2D 相机空间中的像素重建世界坐标中的 3D 点。可以使用几种方法来执行相机校准，包括已知的校准图像模式或许多自校准方法之一[436]。*外在*参数定义摄像机在世界坐标中的位置，*内在*参数定义摄像机图像坐标中像素坐标之间的关系。关键变量包括光学系统下图像的主点或中心点处的两个相机之间的校准基线距离；光学系统的焦距；它们的像素大小和长宽比，这是通过传感器大小除以每个轴上的像素分辨率来计算的；以及摄像机的位置和方向。
*   **基本矩阵或本质矩阵。**这两个矩阵是相关的，定义了用于投影重建的立体摄像机系统的流行几何形状【438，436，437】。它们的推导超出了本文的范围。根据所采用的算法，可以使用任一矩阵。*本质矩阵*仅使用外部摄像机参数和摄像机坐标，而*基础矩阵*取决于外部和内部参数，并揭示核线上立体图像对之间的像素关系。

在任一情况下，我们都以投影变换结束，以从立体图像对中的 2D 相机点重建 3D 点。

立体声处理步骤通常如下:

1.  **拍摄:** 同时拍摄左/右图像对。
2.  **矫正:** 将左右图像对矫正到同一平面上，使像素行 x 坐标与直线对齐。几种投影扭曲方法可用于纠正[437]。通过沿着 *x* 轴对齐图像，校正将图像间的图案匹配问题简化为沿着 *x* 轴的 1D 搜索。校正还可以包括作为单独步骤的光学器件的径向失真校正；然而，许多相机包括内置的工厂校准的径向失真校正。
3.  **特征描述:** 对于图像对中的每个像素，隔离出每个像素周围的一个小区域作为*目标特征描述符。*各种方法被用于立体特征描述【215，120】。
4.  **对应:** 在对面图像对中搜索每个目标特征。搜索操作通常进行两次，首先在右图像中搜索左对目标特征，然后在左图像中搜索右对目标特征。为了提高景深精度，对应需要亚像素精度。
5.  **三角测量:** 使用三角测量计算匹配点之间的差异或距离[439]。使用多种方法中的一种对所有 L/R 目标特征匹配进行排序，以找到最佳质量的匹配[440]。
6.  **空洞填充:** 对于没有对应的良好匹配的像素和相关联的目标特征，在深度图中的该位置有一个空洞。孔洞可能是由 L/R 图像对中的特征遮挡引起的，或者仅仅是由开始时的不良特征引起的。使用局部区域最近邻像素插值方法来填充孔洞。

立体深度-距离分辨率是视点距离的指数函数:一般来说，基线越宽，长距离深度分辨率越好。较短的基线对于近距离深度更好(参见[图 1-10](#Fig10) 和 [1-20](#Fig20) )。据测量，人眼基线或瞳孔间距在 50 至 75 毫米之间，男性平均为 70 毫米，女性为 65 毫米。

*多视图立体* (MVS) 是一种使用同一主题的不同基线从几个视图计算深度的相关方法，例如从单个或单目相机，或相机阵列。本节稍后将介绍单目、MVS 和阵列相机深度感测。

结构光和编码光

结构光或编码光使用特定的图案投射到场景中并反射回来，然后测量以确定深度；参见图 1-11 。在本讨论中，我们定义了以下使用结构光的方法[445]:

*   **空间单一模式方法** **，**在单一图像中只需要单一照明模式。
*   **定时多路复用多模式方法** ，需要一系列模式照明和图像，通常使用二进制或 *n* 阵列代码，有时涉及相移或抖动后续帧中的模式以提高分辨率。常见的模式序列包括格雷码、二进制码、正弦码和其他独特的码。

![9781430259299_Fig01-11.jpg](img/images/9781430259299_Fig01-11.jpg)

[图 1-11](#_Fig11) 。所选的结构光模式和方法:a .格雷码，b .二进制码，c .规则点网格，d .随机点网格(用于原始 Kinect)，e .正弦相移模式，f .压缩结构光的随机模式[446]

例如，在最初的微软 Kinect 3D 深度相机中，由几个略有不同的微网格图案或红外光的伪随机点组成的结构光被投影到场景中，然后拍摄单个图像来捕捉场景中出现的点。基于对实际系统和专利申请的分析，最初的 Kinect 使用几种方法来计算深度，包括(1)红外点的大小——较大的点和较低的模糊度意味着位置更近，而较小的点和较高的模糊度意味着位置更远；(2)光斑的形状——圆形表示平行面，椭圆形表示斜面；以及(3)通过一起使用小区域或斑点的微图案，使得分辨率不是非常好，但是噪声灵敏度是好的。使用这种方法从*单个图像*计算深度，而不需要几个连续的图案和图像。

多图像方法用于结构光，包括投射多组时序结构和编码图案，如图[图 1-11](#Fig11) 所示。在多图像方法中，每个图案被顺序发送到场景中并被成像，然后来自所有图案的深度测量的组合被用于创建最终的深度图。

来自结构光的深度测量的工业、科学和医学应用可以达到高精度，以扩展到微米范围的精度成像几米大小的物体。使用图案投影方法，以及使用多个照明光束产生波长干涉的激光条纹图案方法;干涉被测量以计算距离。例如，普通的牙科设备使用插入口中的小型手持式激光测距仪来创建具有缺失部分的牙齿区域的高度精确的深度图像，然后使用 CAD/CAM 微型铣床使用该图像来创建新的、实际上完美配合的牙冠或填充物。

当然，红外光模式在日光下的户外效果不好；它们被自然光洗掉了。此外，可使用的红外发射器的强度受到实用性和安全性的限制。在室内有效使用结构光的距离受到可用于 IR 发射器的功率量的限制；也许 5 米是室内红外光的现实极限。Kinect 声称目前使用均匀恒定红外照明的 TOF(飞行时间)方法的范围约为 4 米，而第一代 Kinect 传感器使用结构光具有类似的深度范围。

除了创建深度图，结构光或编码光还用于采用光学编码器的测量，如在机器人和过程控制系统中。编码器测量径向或线性位置。它们提供红外照明模式，并在标尺或十字线上测量响应，这对于线性电机和旋转丝杠等单轴定位设备非常有用。例如，像二进制位置码和反射二进制格雷码[444]这样的模式可以很容易地转换成二进制数(见[图 1-11](#Fig11) )。每个格雷码集元素在连续元素之间的汉明距离为 1。

结构光方法在处理高镜面反射和阴影时会遇到问题；然而，这些问题可以通过在图案投影仪和场景之间使用光学漫射器来缓解，使用漫射结构光方法[443]来保持照明编码。此外，多图案结构光方法不能处理快速移动的场景；然而，单一模式方法可以很好地处理帧运动，因为只需要一帧。

光学编码:衍射光栅

*衍射光栅*是光学编码[447]的多种方法之一，用于创建一组用于深度场成像的图案，其中光结构元素，如镜子、光栅、光导或特殊透镜，放置在探测器或透镜附近。据报道，最初的 Kinect 系统使用衍射光栅方法来创建随机红外点照明图案。传感器上方的衍射光栅【430，431】，如图[图 1-12](#Fig12) 所示，可以提供角度敏感的像素传感。在这种情况下，光以各种角度折射到周围的细胞中，这由衍射光栅或其他光束形成元件(如光导)的位置决定。这允许相对于给定视角以不同方式处理相同的传感器数据，从而产生不同的图像。

![9781430259299_Fig01-12.jpg](img/images/9781430259299_Fig01-12.jpg)

[图 1-12](#_Fig12) 。硅上的衍射光栅用来产生用于深度成像的塔尔博特效应(1836 年左右首次观察到)。(更多信息见参考文献[430])。)衍射光栅是一种光结构元件

该方法允许减小检测器尺寸，同时使用从窄孔径衍射光栅并行捕获的低分辨率图像的组合系列来提供更高分辨率的图像。衍射光栅使得从相同的传感器数据中产生大范围的信息成为可能，包括深度信息、增加的像素分辨率、透视位移以及在图像被拍摄后在多个焦平面上的聚焦。衍射光栅是一种照明编码装置。

如[图 1-13](#Fig13) 所示，光结构或编码元件可置于多种配置中，包括【447】:

*   客体侧编码:靠近主体
*   光瞳平面编码:靠近物侧镜头
*   焦平面编码:靠近探测器
*   照明编码:靠近照明器

![9781430259299_Fig01-13.jpg](img/images/9781430259299_Fig01-13.jpg)

[图 1-13](#_Fig13) 。图案的光学结构化和编码的各种方法[447]:(从左至右):物方编码、光瞳平面编码、焦平面编码、照明编码或结构光。照明图案在光学编码器中确定

请注意，照明编码在[图 1-11](#Fig11) 中显示为结构光模式，而照明编码的变体在[图 1-7](#Fig7) 中显示，使用一组打开或关闭的镜子来创建模式。

飞行时间传感器

通过测量红外光传播和反射所花费的时间量，创建了*飞行时间* (TOF)传感器【450】。TOF 传感器是一种测距仪或激光雷达[449]。有几种单芯片 TOF 传感器阵列和深度相机解决方案可用，例如第二版的 Kinect 深度相机。基本概念包括在已知时间向场景中传播红外光，例如通过脉冲 IR 激光器，然后测量光在每个像素返回所用的时间。据报道，根据 TOF 传感器的使用条件、设计中采用的具体方法以及红外激光器的功率大小，高端系统在数百米范围内的精度达到亚毫米级[449]。

TOF 传感器中的每个像素都有几个有源组件，如[图 1-14](#Fig14) 所示，包括红外传感器井、测量从照明到检测红外光的往返时间的定时逻辑，以及用于同步电子快门和脉冲红外激光器的光闸。TOF 传感器提供激光测距能力。例如，通过门控电子快门以消除短的往返响应，可以减少诸如雾或烟反射的环境条件。此外，可以通过以期望的时间间隔打开和关闭快门来测量特定的深度范围，例如长距离。

![9781430259299_Fig01-14.jpg](img/images/9781430259299_Fig01-14.jpg)

[图 1-14](#_Fig14) 。一种假设的 TOF 传感器配置。注意，光脉冲长度和传感器可以一起选通，以瞄准特定的距离范围

用于 TOF 传感器的照明方法可以对第一图像使用非常短的 IR 激光脉冲，获取没有激光脉冲的第二图像，然后获取图像之间的差异以消除环境 IR 光的影响。通过使用光子混合器设备(PMD) 用射频载波信号调制红外光束，可以测量返回的红外信号的相移，以提高精度——这在许多激光测距方法中很常见【450】。结合增强型 CCD 传感器的快速光学门控可用于在有限条件下将精度提高到亚毫米范围，即使在 100 米以上的范围内也是如此。然而，多次红外反射会给距离图像带来误差，因为单个红外脉冲是在整个场景上发出的，并且在成像之前可能会从几个表面反射。

由于 TOF 传感器的深度感测方法与传感器电子设备集成在一起，因此与立体声和其他方法相比，所需的处理开销非常低。然而，红外光在室外环境下的局限性仍然存在[448]，这可能会影响深度精度。

阵列摄像机

如前面的[图 1-9](#Fig9) 所示，*阵列相机*包含几个相机，通常排列成 2D 阵列，如 3x3 阵列，为计算成像提供几个关键选项。用于便携式设备的商用阵列相机开始出现。如前所述，他们可以利用阵列中传感器的组合，使用多视图立体方法来计算视差。阵列相机的一些关键优势包括用于计算 3D 深度图的宽基线图像集，该 3D 深度图可以看穿遮挡及其周围，从每个传感器的较低分辨率图像插值的较高分辨率图像，全聚焦图像，以及在一个或多个位置重新聚焦的特定图像。阵列相机的最大孔径等于传感器之间的最宽基线。

径向摄像机

围绕透镜的圆锥形或径向反射镜和 2D 图像传感器创建了径向相机[413]，其结合了 2D 和 3D 成像。如[图 1-15](#Fig15) 所示，径向反射镜允许 2D 图像形成在传感器的中心，而包含反射的 3D 信息的径向环形图像形成在传感器周边。通过基于锥形镜的几何形状将环形信息处理成点云，提取深度，并且图像中心的 2D 信息可以作为纹理图覆盖，用于完全 3D 重建。

![9781430259299_Fig01-15.jpg](img/images/9781430259299_Fig01-15.jpg)

[图 1-15](#_Fig15) 。(左)带有锥形镜的径向相机系统，用于捕捉 3D 反射。(中心)捕捉边缘周围的 3D 反射和中心面的 2D 信息。(右)从径向图像 3D 信息和作为纹理图的 2D 面重建的 3D 图像。(图片由哥伦比亚大学 CAVE 许可使用)

全光学:光场相机

全光方法 创建一个定义为*光场的 3D 空间，*由多个光学元件创建。全光系统使用一组微光学器件和主光学器件对 4D 光场进行成像，并在后处理过程中从光场中提取图像[451，452，423]。全光照相机只需要一个图像传感器，如图[图 1-16](#Fig16) 所示。4D 光场包含空间中每个点的信息，并且可以表示为体积数据集，将每个点视为具有 3D 定向表面的*体素、*或 3D 像素，具有颜色和不透明度。可以处理体积数据以产生不同的视图和透视位移，允许在拍摄图像后在多个焦平面聚焦。可以获取体积的切片来隔离透视并呈现 2D 图像。渲染光场可以通过使用光线追踪和体绘制方法来完成[453，454]。

![9781430259299_Fig01-16.jpg](img/images/9781430259299_Fig01-16.jpg)

[图 1-16](#_Fig16) 。全光照相机插图。可以从相同的传感器图像处理场景中的多个独立对象。拍摄图像后，可以为每个对象独立计算景深和焦距，从而在 3D 光场内调整透视和焦平面

除了光场的体积和表面渲染，来自 3D 场或体积的 2D 切片可以通过傅立叶投影切片定理[455]在频域进行处理，如图 1-17 所示。这是处理 3D MRI 和 CAT 扫描数据的医学成像方法的基础。Levoy [455，452]和 Krig [137]描述了傅立叶投影切片法对体积和 3D 范围数据的应用。基本算法描述如下:

1.  使用 3D FFT 将体积数据正向变换成幅度和相位数据。
2.  为了形象化，通过*八分区移位*每个立方体来重新排列频率体积中产生的 3D FFT 结果，以围绕体积中心的 3D 笛卡尔坐标系的中心对齐频率 0 数据，类似于 2D 频谱围绕 2D 笛卡尔坐标系的中心进行*象限移位*以显示频谱的方式。
3.  从平行于 FOV 平面的体积中提取平面 2D 切片，其中切片穿过体积的原点(中心)。从频域体数据获取的切片的角度决定了期望的 2D 视图的角度和景深。
4.  来自频域的 2D 切片通过逆 2D FFT，以产生对应于所选角度和景深的 2D 空间图像。

![9781430259299_Fig01-17.jpg](img/images/9781430259299_Fig01-17.jpg)

[图 1-17](#_Fig17) 。傅里叶投影切片定理算法的图形表示，它是光场处理的一种方法。3D 傅立叶空间用于过滤数据以创建 2D 视图和渲染图[455，452，137]。(图片由英特尔出版社授权使用，来自构建智能系统)

3D 深度处理

由于历史原因，在深度感测和相关方法的讨论中使用了几个术语及其缩写，因此我们在本节中涵盖了一些重叠的主题。[表 1-1](#Tab1) 之前提供了深度感测的基本物理方法的概要。不管深度感测方法如何，都有许多相似之处和共同的问题。考虑到相机系统的校准精度、深度场的几何模型、深度数据的测量精度、深度数据中存在的任何噪声以及预期应用，深度信息的后处理是至关重要的。

我们在此调查几个相互关联的深度感测主题，包括:

*   稀疏深度传感方法
*   密集深度感应方法
*   光流
*   同步定位和绘图(SLAM)
*   结构来自运动(SFM)
*   三维表面重建，三维表面融合
*   单眼深度感应
*   立体和多视角立体(MVS)
*   深度感测中的常见问题

人类的深度感知依赖于一组先天和后天的视觉线索，这些线索超出了本工作的范围，并重叠到几个领域，包括光学、眼科和心理学[464]；然而，我们在深度处理的上下文中提供了上述所选主题的概述。

方法概述

对于深度处理方法的讨论，根据表 1-1 所示的方法，深度感测分为两大类:

*   **稀疏深度方法** ，利用计算机视觉方法提取局部兴趣点和特征。只有选定的点被组合成稀疏的深度图或点云。随着相机或场景的移动，逐帧跟踪特征，并更新稀疏点云。通常只需要一台摄像机。
*   **密集深度法** ，计算每个像素的深度。这将使用立体声、TOF 或 MVS 等方法创建密集的深度图。它可能涉及一个或多个摄像机。

许多稀疏深度方法使用标准的单目相机和计算机视觉特征跟踪，如光流和 SLAM(这将在本节稍后讨论)，并且逐帧跟踪特征描述符以计算视差和稀疏深度。密集深度方法通常更多地基于特定的深度相机技术，如立体或结构光。也有例外，下面会讲到。

深度感测和处理中的问题

深度感测方法各有特定的问题；然而，这里我们可以解决一些常见的问题。首先，一个常见的问题是深度场的*几何建模* ，它很复杂，包括透视和投影。大多数深度感测方法将整个区域视为笛卡尔坐标系，这给深度解决方案带来了一些小问题。相机传感器为 2D 欧氏模型，离散体素在 3D 欧氏空间成像；然而，使用简单的笛卡尔模型在相机和真实世界之间进行映射会引入几何失真。其他问题包括*对应、*或未能匹配独立帧中的特征，以及*噪声*和*遮挡*。我们将在下一节中探讨这些问题。

几何场和变形

场几何是影响深度感测和 2D 成像的复杂领域。对于商业应用，几何场问题可能并不重要，因为定位面部、跟踪简单物体和增强现实在 3D 精度方面并不苛刻。然而，军事和工业应用通常要求高精度和准确度，因此需要仔细的几何处理。为了理解深度感测方法常见的几何场问题，让我们来分解一下主要领域:

*   射影几何问题，处理透视
*   极坐标和球面几何问题，处理视角随着与观察者的距离而扩展
*   由于透镜像差造成的径向失真
*   坐标空间问题，由于传感器和体素的笛卡尔坐标，以及将光线从场景投射到传感器的极坐标性质

本讨论的目标是列举深度感测中的问题，而不是解决它们，并在适用的情况下提供参考。由于几何的主题是广泛的，我们只能在这里提供一些更好的方法来模拟深度场的例子。希望通过确定深度感测中涉及的几何问题，对这一重要课题给予更多的关注。对于任何深度系统，包括校正在内的完整几何模型都非常复杂。通常，在流行的商业应用中会忽略高级几何的话题；然而，我们可以肯定的是，鉴于所需的精度，粒子束武器和导弹系统等先进的军事应用不会忽视这些复杂性。

一些研究人员已经研究了处理射影几何问题的更鲁棒的非线性方法[465，466]，特别是通过将核几何相关的失真建模为 3D *圆柱失真*、 而不是平面失真，并通过提供合理的计算方法进行校正。此外，Lovegrove 和 Davison [484]的工作使用*球形拼接*方法处理几何场，以对齐整个图像进行深度融合，由于球形建模而提高了精度。

双眼单视区、帕努姆区和深度融合

如[图 1-18](#Fig18) 所示，*双眼单视界*区域，最早由托勒密等人在天文学背景下研究，是一个包含 3D 点的曲面，这些点与观察者的距离相同，并且位于同一焦平面上。*帕努姆区* 是人类视觉系统将视网膜中的点融合成相同距离和焦平面的单个物体的双眼单视界周围的区域。人类视觉系统能够调和 3D 点之间的距离并合成一个共同的深度场，这是一个小小的奇迹！双眼单视区域和帕努姆区域面临的挑战在于，任何深度算法都必须有一个后处理步骤，才能像人类视觉系统那样正确地融合这些点。误差幅度取决于通常的变量，包括基线和像素分辨率，误差在深度场的边界最明显，在中心不太明显。有些球面畸变是由于透镜朝向边缘的像差造成的，可以部分地进行校正，如本章前面讨论的早期传感器处理过程中的几何校正。

![9781430259299_Fig01-18.jpg](img/images/9781430259299_Fig01-18.jpg)

[图 1-18](#_Fig18) 。立体和多视图立体方法的问题，显示双眼单视区域和 Panum 区域，以及空间中的三个点，这三个点从左眼的角度看起来是同一点，但从右眼的角度看却不同。这三个点围绕在帕努姆区域的双眼单视界周围，并被人类融合以合成视在深度

笛卡尔坐标与极坐标:球面射影几何

如图[图 1-19](#Fig19) 所示，TOF 或单目深度传感方法中使用的 2D 传感器也有特定的几何问题；问题朝着视野的边缘增加。请注意，从空间中的一点到传感器中的一个像素的深度实际上是在一个使用极坐标的球坐标系中测量的，但传感器的几何图形是纯笛卡尔的，因此几何误差被*烤成了蛋糕*。

![9781430259299_Fig01-19.jpg](img/images/9781430259299_Fig01-19.jpg)

[图 1-19](#_Fig19) 。2D 深度传感器和透镜具有处理距离的夸大的成像几何问题，其中深度根据透镜和传感器上的入射角而不同。注意，P1 和 P2 离焦平面的距离相等；然而，每个点通过光学系统到传感器的距离是不相等的，因此根据所使用的几何模型，计算的深度将是不准确的

因为立体和 MVS 方法也使用单个 2D 传感器，所以影响单个传感器深度感测方法的相同问题也影响多相机方法，增加了开发精确且计算合理的几何模型的困难。

深度粒度

如[图 1-20](#Fig20) 所示，简单的笛卡尔深度计算无法将深度场分解成线性均匀的粒度；事实上，深度场粒度随着与传感器的距离成指数增加，而在长距离下解析深度的能力则不太准确。

![9781430259299_Fig01-20.jpg](img/images/9781430259299_Fig01-20.jpg)

[图 1-20](#_Fig20) 。典型立体相机系统的 z 深度粒度非线性问题。注意，使用立体和 MVS 方法的实际深度感测在深度场中具有局限性，主要受像素分辨率、基线和焦距的影响。在 10 米处，深度粒度几乎是米，因此对象必须移动至少+或-米，以便计算测量的立体深度的变化

例如，在一个使用 480p 视频分辨率的基线为 70mm 的假设立体视觉系统中，如图[图 1-20](#Fig20) 所示，10 米处的深度分辨率下降到大约 1 米；换句话说，在 10 米远的地方，物体可能看起来不会在 *Z* 方向移动，除非它们在 *Z* 方向移动了至少正负一米。深度分辨率可以简单地通过加倍传感器分辨率来加倍。随着距离的增加，人类越来越多地使用单眼深度线索来确定深度，例如物体的大小、物体的运动速度、颜色强度和表面纹理细节。

一致

对应或特征匹配对于大多数深度感测方法来说是常见的。对于立体特征匹配算法的分类，参见 Scharstein 和 Szeliski [440]。在这里，我们沿着应用于立体、多视图立体和结构光的特征描述符方法和三角测量的路线讨论对应性。

亚像素精度 是大多数深度传感方法的目标，因此存在几种算法【468】。通过拟合表面来找到最高匹配来关联两个补丁或强度模板是很流行的；然而，傅立叶方法也用于关联相位【467，469】，类似于强度关联方法。

对于立体系统，在特征匹配之前对图像对进行校正，以便在大约相同的比例下沿着相同的线找到特征，如图[图 1-11](#Fig11) 所示；具有很少或没有旋转不变性的描述符是合适的[215，120]。诸如相关性模板的特征描述符是好的，而诸如 SIFT 特征描述方法 [161]的强大方法是过度的。特征描述符区域可以是有利于在 *x* 轴上的差异并且期望在 *y* 轴上有很小变化的矩形，例如矩形 3×9 描述符形状。差异预计出现在 *x* 轴，而不是 *y* 轴。使用描述符形状的几种窗口大小调整方法，包括固定大小和自适应大小[440]。

多视图立体系统类似于立体；然而，校正阶段可能不那么精确，因为帧之间的运动可能包括缩放、平移和旋转。由于缩放和旋转在帧之间可能有显著的对应问题，其他特征描述的方法已经被应用于 MVS，具有更好的结果。应用于多视图和宽基线立体的一些值得注意的特征描述符方法包括 MSER [194]方法(也在第 6 章的[中讨论)，其使用斑点状补片，以及苏珊[164，165]方法](06.html)(也在第 6 章的[中讨论)，其基于具有特征存在的已知质心或核心的对象区域或分割来定义特征。](06.html)

对于结构光系统，光模式的类型将决定特征，相位的相关性是一种流行的方法[469]。例如，依赖于使用相位相关[467]模板匹配的相移模式的结构光方法声称精确到 1/100 个像素的<sup>分之一</sup>。其他方法也用于结构光对应，以实现亚像素精度[467]。

孔洞和遮挡

当一个图案不能在帧之间匹配时，深度图中存在一个*洞*。孔也可能是由咬合引起的。无论哪种情况，深度图都必须修复，有几种方法可以做到这一点。应提供一张*球洞图*，显示问题所在。一种简单的方法是在局部深度图补片中使用双线性插值来填补空洞。另一种简单的方法是使用来自当前扫描线的深度图中的最后一个已知良好的深度值。

存在用于处理遮挡的更鲁棒的方法[472，471],其使用计算更昂贵但稍微更精确的方法，例如自适应局部窗口来优化插值区域。处理孔洞的另一种方法是将*表面融合*到深度体【473】(接下来讨论)，由此将多个连续的深度图整合到深度体中作为累积表面，然后可以从深度体中提取深度图。

表面重建和融合

从深度图信息创建表面的一般方法是*表面重建。*计算机图形方法可用于渲染和显示表面。基本思想是组合几个深度图来构建一个更好的表面模型，包括表面的 *RGB* 2D 图像渲染为*纹理图*。通过创建集成来自不同视点的若干深度图的 3D 表面的迭代模型，可以增加深度精度，可以减少或消除遮挡，并且创建更宽的 3D 场景视点。

Curless 和 Levoy [473]的工作提出了一种将多个距离图像或深度图融合到 3D 体积结构中的方法。该算法通过整合多个距离图像将所有距离图像作为*等值面*渲染到体积中。使用存储在现有表面的体数据结构中的带符号的距离函数和加权因子，新的表面被集成到体中，用于实际表面存在的位置的累积最佳猜测。当然，生成的表面有几个令人满意的特性，包括减少噪音、减少孔洞、减少遮挡、多视点和更好的精确度(见[图 1-21](#Fig21) )。

![9781430259299_Fig01-21.jpg](img/images/9781430259299_Fig01-21.jpg)

[图 1-21](#_Fig21) 。(右)Curless 和 Levoy [473]从距离图像或深度图构建表面的方法。这里显示的是使用光线投射投射到体积中的三个不同的加权表面测量。(左)Kinect 融合方法的处理流程

应用于 SLAM 的 Curless 和 Levoy 方法的衍生方法是 Kinect 融合方法【474】，如图[图 1-22](#Fig22) 所示，使用计算密集型 SIMD 并行实时方法，不仅提供表面重建，还提供摄像机跟踪和 6 自由度或 *6 自由度*摄像机姿态。光线追踪和纹理贴图用于表面渲染。还有其他从多幅图像进行表面重建的方法[480，551]。

![9781430259299_Fig01-22.jpg](img/images/9781430259299_Fig01-22.jpg)

[图 1-22](#_Fig22) 。使用 ESM [485]获得六自由度相机姿态的密集全图像对准解决方案的图示

噪音

噪声是深度传感器的另一个问题[409]，各种原因包括低照度和在某些情况下的运动噪声，以及劣质的深度感测算法或系统。此外，深度图通常非常模糊，因此可能需要图像预处理，如第 2 章中的[所述，以减少明显的噪声。许多人更喜欢用于深度图处理的双边滤波器[302]，因为它尊重局部结构并保留边缘过渡。此外，已经开发了其他噪声滤波器来弥补双边滤波器的弱点，这些滤波器非常适合于去除深度噪声，包括导向滤波器[486]，其可以像双边滤波器、边缘避免小波方法[488]和域变换滤波器[489]一样执行边缘保持噪声滤波。](02.html)

单目深度处理

单目或单传感器深度感测使用逐帧运动来创建立体视差，从而从成对的图像帧创建深度图。在立体对之间具有校准的固定几何形状的立体处理的假设不适用于单目方法，因为每次摄像机移动时，必须重新计算*摄像机姿态*。摄像机姿态是一个 *6 自由度* (6DOF)方程，包括 *x、y、z* 沿各轴的直线运动和绕各轴的滚动、俯仰、偏航旋转运动。在单目深度感测方法中，必须为每一帧计算相机姿态，作为比较两帧和计算视差的基础。

注意，6DOF 矩阵的计算可以使用*惯性传感器、*如加速度计和 MEMS 陀螺仪【483】作为粗对准步骤来增强，随后是稍后讨论的关于光流*的基于视觉特征的表面对准方法。*由于商用惯性传感器是手机和平板电脑的标准配置，随着传感器的成熟，惯性姿态估计将变得更加有效和常见。虽然商用加速度计的精度不是很好，但单目深度传感系统可以利用惯性传感器进行姿态估计，从而节省计算时间。

多视角立体

大多数单目多视图立体(MVS)深度算法的几何模型是基于射影几何和核几何；Hartley 和 Zisserman [437]在经典文本中对两者进行了很好的概述。Seitz 等人【478】对六种 MVS 算法进行了分类和准确性比较。在这一节中，我们来看几个有代表性的方法。

稀疏法:PTAM

稀疏 MVS 方法创建稀疏的 3D 点云，而不是完整的深度图。稀疏深度的基本目标很简单:逐帧跟踪特征，计算特征差异以创建深度，并执行 6DOF 对齐以定位新帧并获得相机姿态。根据应用的不同，稀疏深度可能非常适合用作特征描述符的一部分，以增加透视视点的不变性，或者为基于场景中几个关键地标的导航提供足够的信息。机器人社区已经开发了几种稀疏深度感测方法，分别称为 *SLAM* 、 *SFM* 和*光流*(下面讨论)。

然而，我们首先通过讨论一种特定的方法来更详细地说明稀疏深度感测:*并行跟踪和映射*(PTAM)【456，457】，它既可以跟踪 6DOF 相机姿态，又可以生成适用于轻型增强现实应用的稀疏深度图，从而允许将化身逐帧放置在场景中的已知位置和方向。基本算法由两部分组成，它们在并行线程中运行:用于更新姿态的跟踪线程和用于更新稀疏 3D 点云的映射线程。接下来，我们将对每一项进行快速概述。

*映射线程*处理最后 N 个关键帧的历史缓冲区和历史缓冲区中每一帧的 N 级图像金字塔，通过束调整过程(简单地说就是通过选择的最小化方法，例如 leven Berg-Marquardt【437】)使用最新的输入深度特征，从该历史缓冲区中不断细化稀疏的 3D 点云。当时间允许时，在低场景运动期间，束调整过程可以在有限的一组最近帧上执行局部调整，或者在所有帧上执行全局调整。

*跟踪线程*基于已知良好特征最后出现的位置的投影，扫描输入图像帧的预期特征，以引导特征搜索，使用 6DOF 相机姿态作为投影的基础。FAST9 [138]角点检测器用于定位角点，随后是 Shi-Tomasi [157]非最大抑制步骤，以移除弱角点候选(在[第 6 章](06.html)中详细讨论)。特征匹配阶段遵循图像金字塔上由粗到细的渐进过程来计算 6DOF 姿态。

*目标特征*在新的帧中使用围绕每个所选角的 8×8 补丁进行计算。*参考特征*也被计算为来自原始补丁的 8×8 补丁，原始补丁取自它们被发现的第一幅已知图像。为了在特征匹配之前对准参考和目标补片，每个参考补片的表面法线用于相对于最后已知的 6DOF 相机姿态预扭曲补片，并且使用零均值 SSD 距离来执行对准的特征匹配。

当*定位*失败时，单目深度感测的一个弱点就显现出来了；也就是说，如果运动太多，或者光照变化太大，系统可能无法定位并且跟踪停止。另一个缺点是，该算法必须完全针对特定的局部场景或工作空间(如桌面)进行初始化。对于初始化，PTAM 遵循五点立体声校准方法，需要几秒钟的时间与用户合作执行。另一个缺点是，包含点云的 3D 体积的大小旨在用于小的局部场景或工作空间。不过从积极的一面来说，3D 点云的精度非常好，接近像素大小；姿态对于 AR 或游戏应用来说足够精确；并且可以通过在场景中走动来创建 360 度的透视点云。PTAM 已经在移动电话[456]上实现，使用适度的计算和存储器资源，具有准确度和帧速率的折衷。

密法:DTAM

与稀疏方法相比，密集单目深度感测是相当计算密集型的，因此研究和开发更加有限。目标与稀疏单目深度大致相同——即，计算用于图像对齐的 6DOF 相机姿态，但创建密集的每像素深度图，而不是稀疏的点云。为了便于说明，我们强调了由纽科姆、洛夫格罗夫和戴维森[482]开发的密集跟踪和绘图方法(DTAM)中的关键概念。

虽然 DTAM 的目标是计算每个像素的密集深度，而不是稀疏深度，但 DTAM 与 PTAM [457]有一些相同的要求，因为两者都是单目方法。DTAM 和 PTAM 都需要计算每个新帧的 6DOF 姿态，以便对准新帧来计算视差。DTAM 还需要一个用户辅助的场景单目校准方法，它使用 PTAM 校准方法。DTAM 也适用于小型的本地化场景或工作空间。DTAM 分享了取自 Lovegrove 和 Davison [484]的球面拼接方法的几个背景概念，包括基于有效的二阶最小化(ESM)方法[485]的*整个图像对齐的概念，*，据报道，该方法使用比 LK 方法[458]更少的迭代来找到稳定的表面对齐，作为生成 6DOF 姿态的过程的一部分。

显然，DTAM 和球面镶嵌都使用球面坐标几何模型来将新帧镶嵌到密集的 3D 表面中，在图像金字塔上从粗略到精细对准，以迭代 6DOF 相机姿态的解决方案。全像面对齐的思路如图[图 1-22](#Fig22) 所示。使用局部导向滤波方法【486】将新的和现有的深度表面整合到成本体积中。也就是说，引导滤波器使用引导图像来将进入的深度信息合并到成本体中。

DTAM 还充分利用了 SIMD 指令和高度线程并行的 SIMT GPGPU 编程，以获得在商用 GPU 硬件上实时操作所需的性能。

光流、SLAM 和 SFM

*光流*以位移*矢量*的形式测量特征和图案在帧与帧之间的运动。光流类似于稀疏单目深度传感方法，它可以应用于宽基线立体匹配问题[463]。由于光流研究及其应用领域广阔[459，460，461]，我们在此仅提供介绍，着眼于描述所使用的方法和获得的特征。

光流可以被认为是稀疏特征跟踪问题，其中特征可以被认为是一个*粒子*【462】，因此光流和粒子流分析是相似的。粒子流分析应用于各种粒子场流分析问题，包括天气预报，模拟燃烧和爆炸，流体动力学和机器人导航。存在用于 2D 和 3D 光流的方法。各种光流算法关注的是离散标量值的连续场中的跟踪特征描述符或矩阵，而不是单个标量或像素。对于计算机视觉，光流算法的输入是一组连续的 2D 图像和像素，或 3D 体积和体素，输出是一组显示被跟踪特征的运动方向的矢量。

早期 Lucas Kanade (LK)方法【458，459，460，461】的许多衍生和替代方法被用于光流；然而，这仍然是最受欢迎的参考点，因为它以相关性模板的形式使用本地特征(如第 6 章中的[所述)。Baker 和 Matthews 的 *Lucas Kanade 20 years on，*【480】对基于 LK 的最先进方法进行了很好的报道。高效的二阶最小化(ESM)方法](06.html)【485】与 LK 方法相关。据报道，ESM 是一种稳定的解决方案，使用的迭代次数比 LK 少。相反，它依赖于像素邻域，如 3×3 矩阵或模板区域，并试图猜测特征移动的方向，迭代搜索局部区域，并使用最小二乘解对搜索结果进行平均，以找到最佳猜测。

虽然 LK 方法有许多变体[459，460，461]，但大多数 LK 导出的光流方法的关键假设包括帧与帧之间的小的特征位移、刚性特征和以局部梯度形式的足够的纹理信息，以便识别特征。使用各种方法来寻找局部梯度，例如 Sobel 和 Laplacian(在第 2 章中讨论)。具有帧与帧之间的大特征位移和很少纹理信息的场不太适合 LK 方法。这是因为 LK 算法通过检查每个局部矩阵的特征值来优化迭代解，从而忽略了几乎没有梯度信息的区域。然而，最近的和稳健的研究方法正在超越 LK [459，460]的限制，并包括 Deepflow [344]，它是为可变形特征和大位移光流[394]设计的，使用类似于卷积网络[339]的多层特征尺度层次[404]。

表面重建在定位和测绘中的应用被用于*同时定位和测绘* (SLAM) 和*从运动中构造* (SFM)的方法中——例如，在机器人导航中。SLAM 的一个目标是定位或找到当前位置和 6DOF 相机姿态。另一个目标是创建包含深度的局部区域图。要更深入地了解 SLAM 和 SFM 方法，请参阅贝利和休·达兰特-威特的历史调查。

3D 表示:体素、深度图、网格和点云

深度信息以各种可转换的格式表示和存储，这取决于预期的用途。我们在这里总结了一些常见的格式；参见图 1-23 。

![9781430259299_Fig01-23.jpg](img/images/9781430259299_Fig01-23.jpg)

[图 1-23](#_Fig23) 。各种 3D 深度格式。泽尼克多项式的渲染。(从左到右):深度图，使用 3D 四边形的多边形网格渲染，相当于体素的点云渲染

对于不同的算法来说，在深度格式之间进行转换的能力是可取的，并且很容易做到。常见的 3D 深度格式包括:

*   **2D 像素阵列，3D 深度图:**2D 像素阵列是内存中 2D 图像的默认格式，它是许多处理操作的自然存储格式，如卷积和邻域过滤。对于深度图图像，像素值是 *Z* ，或深度值。数组中的每个点可能包含*{颜色，深度}。*
*   **3D 体素体积:**由 3D 体素阵列组成的 3D 体数据结构对于几种算法是理想的，包括用于 3D 表面重建的深度图集成和用于图形渲染的表面光线跟踪。体素是体积元素，就像像素是图片元素一样。每个体素可能包含*{颜色，正常}*；深度坐标隐含在体积结构中。
*   **3D 多边形网格:**在标准 3D 多边形网格中存储 3D 点提供了一组连接的点或顶点，每个点或顶点具有表面法线、3D 坐标、颜色和纹理。网格格式非常适合在 GPU 流水线中渲染表面，如 OpenGL 或 DirectX。网格中的每个点可以包含 *{x，y，z，颜色，法线}，*，并且与标准模式中的相邻点相关联，例如描述表面的四边形或三角形。
*   **3D 点云:**这是一种稀疏结构，可以直接转换成标准的 3D 多边形网格。点云格式是稀疏单目深度传感方法的理想选择。云中的每个点都可能包含 *{x，y，z，颜色，法线}。*

摘要

在这一章中，作为视觉流水线的第一步，我们概述了图像传感方法和传感器图像处理方法。我们讨论了可用的图像传感器技术，着眼于可能有助于充分利用图像数据的图像预处理，因为图像传感方法通常决定了所需的图像预处理。(关于图像预处理的更多讨论在第 2 章中提供。)讨论了用于 2D 和 3D 成像的传感器配置，以及用于计算成像以在捕获数据后创建新图像的各种相机配置，例如 HDR 图像和图像重聚焦。深度成像方法也在这里讨论，包括立体和飞行时间，因为移动设备越来越多地为消费应用提供 3D 深度相机技术。深度图可用于计算机视觉中解决许多问题，如前景和背景物体的三维特征描述和三维图像分割。3D 深度处理和 3D 特征的主题贯穿本书始终；[第 6 章](06.html)涵盖 3D 特征描述符，[第 7 章](07.html)和[附录 B](10.html) 涵盖 3D 真实情况数据。

__________________

[<sup>1</sup>](#_Fn2) Foveon 是 Foveon Inc .的注册商标。

[<sup>2</sup>](#_Fn3) `http://opencv.org`，摄像机标定和三维重建

[<sup>3</sup>](#_Fn4)T0】