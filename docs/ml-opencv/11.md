# 使用超参数调整选择正确的模型

既然我们已经探索了各种各样的机器学习算法，我相信你已经意识到它们中的大多数都有大量的设置可供选择。这些设置或调谐旋钮，即所谓的**超参数**，帮助我们在试图最大化性能时控制算法的行为。

例如，我们可能想要选择决策树中的深度或分裂标准，或者调整神经网络中的神经元数量。寻找模型的重要参数值是一项棘手的任务，但对于几乎所有模型和数据集都是必要的。

在本章中，我们将深入探讨**模型评估**和**超参数调整**。假设我们有两个不同的...

# 技术要求

可以从以下链接查阅本章代码:[https://github . com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/chapter 11](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter11)。

以下是软件和硬件要求的总结:

*   您将需要 OpenCV 版本 4.1.x (4.1.0 或 4.1.1 都可以)。
*   您将需要 Python 3.6 版本(任何 Python 3 . x 版本都可以)。
*   您将需要 Anaconda Python 3 来安装 Python 和所需的模块。
*   除了这本书，你可以使用任何操作系统——苹果操作系统、视窗操作系统和基于 Linux 的操作系统。我们建议您的系统中至少有 4 GB 内存。
*   你不需要一个图形处理器来运行本书提供的代码。

# 评估模型

模型评估策略有许多不同的形式和形状。因此，在接下来的章节中，我们将重点介绍三种最常用的相互比较模型的技术:

*   k 倍交叉验证
*   拔靴带
*   麦克内尔的测试

原则上，模型评估很简单:在一些数据上训练模型后，我们可以通过将模型预测与一些地面真实值进行比较来估计其有效性。我们很早就知道，我们应该将数据分成训练集和测试集，并且我们尽可能地遵循这个指导。但是我们到底为什么要再做一次呢？

# 以错误的方式评估模型

我们从不在训练集上评估模型的原因是，原则上，如果我们向任何数据集扔一个足够强的模型，它就可以被学习。

借助 Iris 数据集可以快速演示这一点，我们在[第 3 章](03.html)、*监督学习的第一步*中详细讨论了这个数据集。在那里，目标是根据鸢尾花的物理尺寸对它们进行分类。我们可以使用 scikit-learn 加载 Iris 数据集:

```py
In [1]: from sklearn.datasets import load_iris
...     iris = load_iris()
```

解决这个问题的一个简单方法是将所有数据点存储在矩阵`X`中，并将所有类标签存储在向量`y`中:

```py
In [2]: import numpy as np
...     X = iris.data.astype(np.float32)
...     y = iris.target
```

接下来，我们选择一个模型及其超参数。例如，让我们使用来自[第 3 章](03.html)、*监督学习的第一步*的算法，它只提供一个超参数:邻居的数量， *k* 。借助 *k=1* ，我们得到一个非常简单的模型，将未知点的标签分类为与其最近邻居属于同一类。

在以下步骤中，您将学习如何构建 **k 最近邻** ( **k-NN** )并计算其精度:

1.  在 OpenCV 中，kNN 实例化如下:

```py
In [3]: import cv2
...     knn = cv2.ml.KNearest_create()
...     knn.setDefaultK(1)
```

2.  然后，我们训练模型，并使用它来预测我们已经知道的数据的标签:

```py
In [4]: knn.train(X, cv2.ml.ROW_SAMPLE, y)
...     _, y_hat = knn.predict(X)
```

3.  最后，我们计算正确标记点的分数:

```py
In [5]: from sklearn.metrics import accuracy_score
...     accuracy_score(y, y_hat)
Out[5]: 1.0
```

我们可以看到，准确率得分为`1.0`，说明我们的模型 100%正确标注了点。

If a model gets 100% accuracy on the training set, we say the model memorized the data.

但是预期的准确性真的被测量了吗？我们是否已经提出了一个我们期望 100%正确的模型？

正如你可能已经收集到的，答案是否定的。这个例子表明，即使是一个简单的算法也能够记忆真实世界的数据集。想象一下，对于一个深度神经网络来说，这个任务是多么容易！通常，模型的参数越多，它就越强大。我们将很快回到这个问题。

# 以正确的方式评估模型

使用所谓的测试集可以更好地了解模型的性能，但是你已经知道这一点了。当展示了从训练过程中获得的数据时，我们可以检查一个模型是否已经学习了数据中的一些相关性，或者它是否已经记住了训练集。

我们可以使用 scikit-learn 的`model_selection`模块中熟悉的`train_test_split`将数据分成训练集和测试集:

```py
In [6]: from sklearn.model_selection import train_test_split
```

但是我们如何选择正确的列车测试比率呢？甚至有正确比率这种东西吗？还是认为这是模型的另一个超参数？

这里有两个相互矛盾的问题:

*   如果我们的...

# 选择最佳型号

当一个模型表现不佳时，通常不清楚如何让它变得更好。在这本书里，我宣布了一个拇指规则，例如，如何选择神经网络的层数。更糟糕的是，答案往往是反直觉的！例如，向网络中添加另一层可能会使结果更糟，而添加更多的训练数据可能根本不会改变性能。

你可以看到为什么这些问题是机器学习最重要的方面。归根结底，确定哪些步骤将改进或不改进我们的模型的能力是成功的机器学习实践者与众不同的地方。

我们来看一个具体的例子。还记得[第 5 章](05.html)、*使用决策树进行医疗诊断*吗，我们在回归任务中使用决策树？我们将两个不同的树拟合到一个 sin 函数中——一个深度为 2，另一个深度为 5。提醒一下，回归结果如下所示:

![](Images/e4b66301-dbb4-4153-b592-ef5624085515.png)

应该清楚的是，这两种配合都不是特别好。然而，这两个决策树以两种不同的方式失败！

深度为 2 的决策树(上一张截图中的粗线)试图拟合数据中的四条直线。因为数据本质上比几条直线更复杂，这个模型失败了。我们可以尽可能多地训练它，尽可能多地生成训练样本——它永远无法很好地描述这个数据集。这种模型被称为数据不足。换句话说，模型没有足够的复杂性来考虑数据中的所有特征。因此，该模型具有很高的偏差。

另一个决策树(细线，深度 5)犯了不同的错误。该模型具有足够的灵活性，几乎可以完美地解释数据中的精细结构。然而，在某些点上，模型似乎遵循噪声的特定模式；我们增加了原罪函数而不是原罪函数本身。你可以在图的右边看到，蓝色曲线(细线)会抖动很多。据说这样的模型会使数据过度膨胀。换句话说，这个模型太复杂了，以至于最终要考虑数据中的随机误差。因此，该模型具有较高的方差。

长话短说——秘密就在这里:从根本上说，选择正确的模型归结为在偏见和差异之间找到一个平衡点。

The amount of flexibility a model has (also known as the **model complexity**) is mostly dictated by its hyperparameters. That is why it is so important to tune them!

让我们回到 kNN 算法和 Iris 数据集。如果我们针对所有可能的值 *k* 重复将模型拟合到 Iris 数据的过程，并计算训练和测试分数，我们将期望结果如下所示:

![](Images/67d69189-ef9f-41e5-baca-de06a01932b6.png)

上图显示了作为模型复杂性函数的模型得分。如果这一章有什么我想让你记住的，那就是这个图表。让我们打开它。

该图将模型得分(训练或测试得分)描述为模型复杂性的函数。如前图所述，神经网络的模型复杂性大致随着网络中神经元的数量而增长。在 kNN 的情况下，适用相反的逻辑——值 *k* 越大，决策边界越平滑，因此复杂度越低。换句话说， *k=1* 的 kNN 将在前面的图中一直向右，此时训练分数是完美的。难怪我们在训练集上获得了 100%的准确率！

从上图中，我们可以看出在模型复杂性领域有三种状态:

*   训练数据下的模型复杂度非常低(高偏差模型)。在这种情况下，无论我们训练了多长时间，模型在训练集和测试集上都只能获得很低的分数。
*   一个具有很高复杂度(或高方差)的模型会过度训练数据，这表明该模型可以很好地预测训练数据，但不能预测未知数据。在这种情况下，模型开始学习仅出现在训练数据中的复杂性或特殊性。因为这些特性不适用于看不见的数据，所以训练分数越来越低。
*   对于某些中间值，测试分数最大。我们试图找到的正是这种中间状态，即考试分数最高的状态。这是偏见和差异之间权衡的最佳点！

这意味着我们可以通过绘制模型复杂性图来为手头的任务找到最佳算法。具体来说，我们可以使用以下指标来了解我们目前所处的状态:

*   如果训练和测试分数都低于我们的预期，我们可能处于上图中最左边的状态，此时模型对数据的拟合不足。在这种情况下，一个好主意可能是增加模型的复杂性，然后再试一次。
*   如果训练分数比测试分数高得多，那么我们可能处于上图中最右边的状态，此时模型正在过度拟合数据。在这种情况下，一个好主意可能是降低模型的复杂性，然后再试一次。

虽然这个过程在总体上是可行的，但是有更复杂的模型评估策略，比简单的训练-测试分割更彻底，这一点我们将在后面的章节中讨论。

# 理解交叉验证

交叉验证是一种评估模型泛化性能的方法，通常比将数据集拆分为训练集和测试集更稳定和彻底。

交叉验证最常用的版本是 **k 倍交叉验证**，其中 *k* 是用户指定的数字(通常是 5 或 10)。这里，数据集被分割成大小大致相等的 *k* 部分，称为**折叠**。对于包含 *N* 数据点的数据集，每个折叠应该具有大约 *N / k* 个样本。然后，在数据上训练一系列模型，使用 *k - 1* 折叠进行训练，剩余一个折叠进行测试。对 *k* 迭代重复该过程，每次选择不同的折叠...

# 在 OpenCV 中手动实现交叉验证

在 OpenCV 中执行交叉验证最简单的方法是手工进行数据拆分。

例如，为了实现双重交叉验证，我们将执行以下过程:

1.  加载数据集:

```py
      In [1]: from sklearn.datasets import load_iris
      ...     import numpy as np
      ...     iris = load_iris()
      ...     X = iris.data.astype(np.float32)
      ...     y = iris.target
```

2.  将数据分成大小相等的两部分:

```py
      In [2]: from sklearn.model_selection import model_selection
      ...     X_fold1, X_fold2, y_fold1, y_fold2 = train_test_split(
      ...         X, y, random_state=37, train_size=0.5
      ...     )
```

3.  实例化分类器:

```py
      In [3]: import cv2
      ...     knn = cv2.ml.KNearest_create()
      ...     knn.setDefaultK(1)
```

4.  在第一个折叠上训练分类器，然后预测第二个折叠的标签:

```py
      In [4]: knn.train(X_fold1, cv2.ml.ROW_SAMPLE, y_fold1)
      ...     _, y_hat_fold2 = knn.predict(X_fold2)
```

5.  在第二个折叠上训练分类器，然后预测第一个折叠的标签:

```py
      In [5]: knn.train(X_fold2, cv2.ml.ROW_SAMPLE, y_fold2)
      ...     _, y_hat_fold1 = knn.predict(X_fold1)
```

6.  计算两次折叠的准确度分数:

```py
      In [6]: from sklearn.metrics import accuracy_score
      ...     accuracy_score(y_fold1, y_hat_fold1)
      Out[6]: 0.92000000000000004
      In [7]: accuracy_score(y_fold2, y_hat_fold2)
      Out[7]: 0.88
```

该程序将产生两个准确度分数，一个用于第一次折叠(准确度 92%)，一个用于第二次折叠(准确度 88%)。平均来说，我们的分类器在看不见的数据上达到了 90%的准确率。

# 使用 scikit-learn 进行 k 倍交叉验证

在 scikit-learn 中，交叉验证可以分三步进行:

1.  加载数据集。既然我们之前已经这么做了，就不用再做了。
2.  实例化分类器:

```py
      In [8]: from sklearn.neighbors import KNeighborsClassifier      ...     model = KNeighborsClassifier(n_neighbors=1)
```

3.  使用`cross_val_score`功能进行交叉验证。该函数将模型、完整数据集(`X`)、目标标签(`y`)和折叠数的整数值(`cv`)作为输入。不需要手动拆分数据，该功能会根据折叠次数自动拆分数据。交叉验证完成后，该函数返回测试分数:

```py
 In [9]: from sklearn.model_selection ...
```

# 实施遗漏交叉验证

实现交叉验证的另一种流行方法是选择与数据集中数据点数量相等的折叠数量。换句话说，如果有 *N* 个数据点，我们设置 *k=N* 。这意味着我们最终将不得不进行交叉验证的 *N* 次迭代，但是在每次迭代中，训练集将仅由单个数据点组成。这个过程的优点是我们可以使用除一个以外的所有数据点进行训练。因此，该程序也被称为**省去**交叉验证。

在 scikit-learn 中，该功能由`model_selection`模块中的`LeaveOneOut`方法提供:

```py
In [11]: from sklearn.model_selection import LeaveOneOut
```

该对象可以通过以下方式直接传递给`cross_val_score`功能:

```py
In [12]: scores = cross_val_score(model, X, y, cv=LeaveOneOut())
```

因为每个测试集现在都包含一个数据点，所以我们期望计分器返回 150 个值——数据集中每个数据点一个值。我们得到的每一点可能是对的，也可能是错的。因此，我们期望`scores`是 1(`1`)和 0(`0`)的列表，它们分别对应于正确和不正确的分类:

```py
In [13]: scores
Out[13]: array([ 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,
                 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                 1., 1., 1., 1., 1., 1., 1.])
```

如果我们想知道分类器的平均性能，我们仍然会计算分数的平均值和标准偏差:

```py
In [14]: scores.mean(), scores.std()
Out[14]: (0.95999999999999996, 0.19595917942265423)
```

我们可以看到这个评分方案返回了非常类似于五重交叉验证的结果。

You can learn more about other useful cross-validation procedures at [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html).

# 使用自举估计鲁棒性

k 倍交叉验证的另一个程序是**引导**。

引导不是将数据拆分成折叠，而是通过从数据集中随机抽取样本来构建训练集。通常，通过抽取替换样本来形成引导。想象一下，把所有的数据点放进一个袋子里，然后从袋子里随机抽取。抽取样本后，我们会把它放回袋子里。这允许一些样本在训练集中出现多次，这是交叉验证所不允许的。

然后在不属于引导程序的所有样本上测试分类器(所谓的**袋外**例子)，并且该过程被重复大量次...

# 在 OpenCV 中手动实现引导

引导可以通过以下过程实现:

1.  加载数据集。既然我们之前已经这么做了，就不用再做了。
2.  实例化分类器:

```py
      In [15]: knn = cv2.ml.KNearest_create()
      ...      knn.setDefaultK(1)
```

3.  从我们的带有 *N* 样本的数据集中，随机选择带有替换的 *N* 样本，形成一个自举。这可以通过 NumPy 的`random`模块中的`choice`功能最容易地完成。我们告诉函数用替换(`replace=True`)在`[0, len(X)-1]`范围内抽取`len(X)`样本。然后，该函数返回一个索引列表，我们从该列表中构建引导数据库:

```py
      In [16]: idx_boot = np.random.choice(len(X), size=len(X),
      ...                                  replace=True)
      ...      X_boot = X[idx_boot, :]
      ...      y_boot = y[idx_boot]
```

4.  将自举中未显示的所有样本放入袋外样品组:

```py
      In [17]: idx_oob = np.array([x not in idx_boot
      ...      for x in np.arange(len(X))],dtype=np.bool)
      ...      X_oob = X[idx_oob, :]
      ...      y_oob = y[idx_oob]
```

5.  在引导样本上训练分类器:

```py
      In [18]: knn.train(X_train, cv2.ml.ROW_SAMPLE, y_boot)
      Out[18]: True
```

6.  在袋外样品上测试分类器:

```py
      In [19]: _, y_hat = knn.predict(X_oob)
      ...      accuracy_score(y_oob, y_hat)
      Out[19]: 0.9285714285714286
```

7.  重复*步骤 3-6* 特定的迭代次数。
8.  自举的迭代。重复这些步骤多达 10，000 次，以获得 10，000 个准确度分数，然后对分数求平均值，以了解分类器的平均性能。

为了方便起见，我们可以从*第 3 步* - *第 6 步*中构建一个函数，这样就可以很容易地将程序运行一些`n_iter`次。我们还传递一个模型(我们的 kNN 分类器，`model`)、特征矩阵(`X`)和带有所有类标签的向量(`y`):

```py
In [20]: def yield_bootstrap(model, X, y, n_iter=10000):
...          for _ in range(n_iter):
```

`for`循环中的步骤本质上是前面提到的代码中的*步骤 3* - *6* 。这包括在引导程序上训练分类器，并在现成的例子中测试它:

```py
...              # train the classifier on bootstrap
...              idx_boot = np.random.choice(len(X), size=len(X),
...                                          replace=True)
...              X_boot = X[idx_boot, :]
...              y_boot = y[idx_boot]
...              knn.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)
... 
...              # test classifier on out-of-bag examples
...              idx_oob = np.array([x not in idx_boot
...                                  for x in np.arange(len(X))],
...                                 dtype=np.bool)
...              X_oob = X[idx_oob, :]
...              y_oob = y[idx_oob]
...              _, y_hat = knn.predict(X_oob)
```

然后，我们需要返回准确度分数。你可能会在这里期待一个`return`声明。但是，更优雅的方法是使用`yield`语句，它会自动将功能转换为生成器。这意味着我们不必初始化一个空列表(`acc = []`)然后在每次迭代时追加新的准确度分数(`acc.append(accuracy_score(...))`)。簿记是自动完成的:

```py
...              yield accuracy_score(y_oob, y_hat)
```

为了确保我们都得到相同的结果，让我们修复随机数生成器的种子:

```py
In [21]: np.random.seed(42)
```

现在，让我们通过将函数输出转换为列表来运行`n_iter=10`次的过程:

```py
In [22]: list(yield_bootstrap(knn, X, y, n_iter=10))
Out[22]: [0.98333333333333328,
          0.93650793650793651,
          0.92452830188679247,
          0.92307692307692313,
          0.94545454545454544,
          0.94736842105263153,
          0.98148148148148151,
          0.96078431372549022,
          0.93220338983050843,
          0.96610169491525422]
```

如您所见，对于这个小样本，我们得到的准确率在 92%到 98%之间。为了更可靠地估计模型的性能，我们重复该过程 1000 次，并计算所得分数的平均值和标准偏差:

```py
In [23]: acc = list(yield_bootstrap(knn, X, y, n_iter=1000))
...      np.mean(acc), np.std(acc)
Out[23]: (0.95524155136419198, 0.022040380995646654)
```

随时欢迎你增加重复次数。但是一旦`n_iter`足够大，该过程应该对采样过程的随机性具有鲁棒性。在这种情况下，当我们不断增加`n_iter`到例如 10，000 次迭代时，我们不期望看到分值分布的任何更多变化:

```py
In [24]: acc = list(yield_bootstrap(knn, X, y, n_iter=10000))
...      np.mean(acc), np.std(acc)
Out[24]: (0.95501528733009422, 0.021778543317079499)
```

典型地，通过自举获得的分数将用于*统计测试*以评估我们结果的*显著性*。让我们看看这是如何做到的。

# 评估我们结果的重要性

假设我们为两个版本的 kNN 分类器实现了交叉验证过程。最终的测试分数是——A 型 92.34%，b 型 92.73%，我们怎么知道哪个模型更好？

按照我们这里介绍的逻辑，我们可能会支持模型 B，因为它有更好的测试分数。但是如果这两种模式没有明显的不同呢？这可能有两个潜在的原因，都是我们测试程序随机性的结果:

*   据我们所知，B 型车只是运气好。也许我们为交叉验证程序选择了一个非常低的 k 值。也许模型 B 最终得到了一个有益的火车测试分割，这样模型在分类时就没有问题了...

# 实施学生测验

最著名的统计测试之一是**学生 t-test** 。你可能以前听说过它:它允许我们确定两组数据是否有明显的不同。这对威廉·希利·戈塞来说是一个非常重要的测试，他是这项测试的发明者，在吉尼斯啤酒厂工作，想知道两批黑啤的质量是否不同。

Note that "Student" here is capitalized. Although Gosset wasn't allowed to publish his test due to company policy, he did so anyway under his pen name, Student.

在实践中，t 检验允许我们确定两个数据样本是否来自具有相同均值或*期望值*的基础分布。

出于我们的目的，这意味着我们可以使用 t 检验来确定两个独立分类器的测试分数是否具有相同的平均值。我们从假设两组考试成绩相同开始。我们称之为*零假设*，因为这是我们想要废掉的假设，也就是说，我们正在寻找证据*拒绝*假设，因为我们想要确保一个分类器明显优于另一个。

我们接受或拒绝基于 t 检验返回的参数 **p 值**的无效假设。p 值取`0`和`1`之间的值。`0.05`的 p 值意味着零假设 100 次中只有 5 次是正确的。因此，一个小的 p 值表明有力的证据表明该假设可以被安全地拒绝。习惯上使用 *p=0.05* 作为截止值，低于该值我们拒绝零假设。

如果这太令人困惑，可以这样想:当我们运行 t-test 来比较分类器测试分数时，我们希望获得一个小的 p 值，因为这意味着两个分类器给出的结果明显不同。

我们可以从`stats`模块用 SciPy 的`ttest_ind`功能实现学生的 t-test:

```py
In [25]: from scipy.stats import ttest_ind
```

让我们从一个简单的例子开始。假设我们对两个分类器进行了五次交叉验证，并获得了以下分数:

```py
In [26]: scores_a = [1, 1, 1, 1, 1]
...      scores_b = [0, 0, 0, 0, 0]
```

这意味着模型 A 在所有五次折叠中实现了 100%的精度，而模型 B 获得了 0%的精度。在这种情况下，很明显这两个结果是显著不同的。如果我们对这些数据进行 t 检验，我们会发现一个非常小的 p 值:

```py
In [27]: ttest_ind(scores_a, scores_b)
Out[27]: Ttest_indResult(statistic=inf, pvalue=0.0)
```

我们有！我们实际上得到最小可能的 p 值， *p=0.0* 。

另一方面，如果两个分类器得到完全相同的数字，除了在不同的折叠过程中？在这种情况下，我们希望这两个分类器是等价的，这可以通过一个非常大的 p 值来表示:

```py
In [28]: scores_a = [0.9, 0.9, 0.9, 0.8, 0.8]
...      scores_b = [0.8, 0.8, 0.9, 0.9, 0.9]
...      ttest_ind(scores_a, scores_b)
Out[28]: Ttest_indResult(statistic=0.0, pvalue=1.0)
```

与前述类似，我们得到最大可能的 p 值， *p=1.0* 。

为了看看在一个更现实的例子中会发生什么，让我们回到前面例子中的 kNN 分类器。使用从十倍交叉验证过程中获得的测试分数，我们可以用以下过程比较两个不同的 kNN 分类器:

1.  获取模型 A 的一组测试分数，我们选择模型 A 作为之前的 kNN 分类器( *k=1* ):

```py
      In [29]: k1 = KNeighborsClassifier(n_neighbors=1)
      ...      scores_k1 = cross_val_score(k1, X, y, cv=10)
      ...      np.mean(scores_k1), np.std(scores_k1)
      Out[29]: (0.95999999999999996, 0.053333333333333323)
```

2.  获取模型 B 的一组测试分数，让我们选择模型 B 作为 *k=3* 的 kNN 分类器:

```py
      In [30]: k3 = KNeighborsClassifier(n_neighbors=3)
      ...      scores_k3 = cross_val_score(k3, X, y, cv=10)
      ...      np.mean(scores_k3), np.std(scores_k3)
      Out[30]: (0.96666666666666656, 0.044721359549995787)
```

3.  将 t 检验应用于两组分数:

```py
      In [31]: ttest_ind(scores_k1, scores_k3)
      Out[31]: Ttest_indResult(statistic=-0.2873478855663425,
               pvalue=0.77712784875052965)
```

如您所见，这是一个很好的例子，两个分类器给出了不同的交叉验证分数(96.0%和 96.7%)，结果没有显著差异！因为我们得到了一个很大的 p 值( *p=0.777* ，所以我们期望这两个分类器 100 次中有 77 次是等价的。

# 实施麦克内尔的测试

更先进的统计技术是**麦克内尔的测试**。该测试可用于配对数据，以确定两个样本之间是否有任何差异。与 t 检验的情况一样，我们可以使用 McNemar 的检验来确定两个模型是否给出了显著不同的分类结果。

McNemar 的测试对成对的数据点进行操作。这意味着我们需要知道，对于两个分类器，它们是如何对每个数据点进行分类的。根据第一个分类器正确但第二个分类器错误的数据点数量，以及反之，我们可以确定两个分类器是否等价。

让我们假设前面的模型 A 和模型 B 应用于相同的五个数据点。而模型...

# 用网格搜索调整超参数

超参数调整最常用的工具是*网格搜索*，这基本上是一个花哨的术语，表示我们将使用`for`循环尝试所有可能的参数组合。

让我们看看这是如何在实践中做到的。

# 实现简单的网格搜索

回到我们的 kNN 分类器，我们发现我们只有一个超参数可以调整: *k* 。通常，您会有大量的开放参数需要处理，但是 kNN 算法对于我们来说足够简单，可以手动实现网格搜索。

在开始之前，我们需要像以前一样将数据集分割成训练集和测试集:

1.  这里我们选择 75-25 的分割:

```py
In [1]: from sklearn.datasets import load_iris...     import numpy as np...     iris = load_iris()...     X = iris.data.astype(np.float32)...     y = iris.targetIn [2]: X_train, X_test, y_train, y_test = train_test_split(...          X, y, random_state=37...      )
```

2.  然后，目标是循环所有可能的 *k* 值。当我们这样做的时候，我们希望保持...

# 理解验证集的价值

按照我们将数据分成训练集和测试集的最佳实践，我们可能会告诉人们，我们已经找到了一个在数据集上以 97.4%的准确率运行的模型。然而，我们的结果不一定能推广到新的数据。这个论点与本书前面我们保证训练-测试分离时的论点相同，即我们需要一个独立的数据集进行评估。

然而，当我们在最后一节实现网格搜索时，我们使用测试集来评估网格搜索的结果，并更新超参数 *k* 。这意味着我们不能再使用测试集来评估最终数据了！基于测试集准确性做出的任何模型选择都会将信息从测试集泄露到模型中。

解决该数据的一种方法是再次分割数据，并引入所谓的**验证集**。验证集不同于训练集和测试集，专门用于选择模型的最佳参数。对这个验证集进行所有的探索性分析和模型选择，并保留一个单独的测试集，只用于最终评估，这是一个很好的做法。

换句话说，我们最终应该将数据分成三个不同的集合:

*   一个**训练集**，用来建立模型
*   一个**验证集**，用于选择模型的参数
*   一个**测试集**，用于评估最终模型的性能

下图说明了这种三向分割:

![](Images/55d53de3-6073-4095-a210-4d7bf70204d1.png)

上图显示了如何将数据集拆分为训练集、验证集和测试集的示例。实际上，三向分割分两步实现:

1.  将数据分成两部分:一部分包含训练和验证集，另一部分包含测试集:

```py
      In [6]: X_trainval, X_test, y_trainval, y_test =
      ...        train_test_split(X, y, random_state=37)
      In [7]: X_trainval.shape
      Out[7]: (112, 4)
```

2.  再次将`X_trainval`分成适当的训练和验证集:

```py
      In [8]: X_train, X_valid, y_train, y_valid = train_test_split(
      ...         X_trainval, y_trainval, random_state=37
      ...     )
      In [9]: X_train.shape
      Out[9]: (84, 4)
```

然后，我们重复前面代码中的手动网格搜索，但这一次，我们将使用验证集来找到最佳的 *k* (参见代码亮点):

```py
In [10]: best_acc = 0.0
...      best_k = 0
...      for k in range(1, 20):
...          knn = cv2.ml.KNearest_create()
...          knn.setDefaultK(k)
...          knn.train(X_train, cv2.ml.ROW_SAMPLE, y_train)
...          _, y_valid_hat = knn.predict(X_valid)
...          acc = accuracy_score(y_valid, y_valid_hat)
...          if acc >= best_acc:
...              best_acc = acc
...              best_k = k
...      best_acc, best_k
Out[10]: (1.0, 7)
```

我们现在发现用 *k=7* ( `best_k`)就可以达到 100%的验证分数(`best_acc`)！然而，回想一下，这个分数可能过于乐观。为了找出模型的实际表现，我们需要在测试集中的数据上测试它。

为了得到最终的模型，我们可以使用在网格搜索中找到的 *k* 的值，并在训练和验证数据上重新训练模型。这样，我们使用了尽可能多的数据来构建模型，同时仍然遵守列车测试分离原则。

这意味着我们应该在`X_trainval`上重新训练模型，该模型包含训练集和验证集，并在测试集上对其进行评分:

```py
In [25]: knn = cv2.ml.KNearest_create()
...      knn.setDefaultK(best_k)
...      knn.train(X_trainval, cv2.ml.ROW_SAMPLE, y_trainval)
...      _, y_test_hat = knn.predict(X_test)
...      accuracy_score(y_test, y_test_hat), best_k
Out[25]: (0.94736842105263153, 7)
```

通过这个过程，我们在测试集上找到了 94.7%的令人生畏的准确率。因为我们遵守了训练-测试分离原则，所以我们现在可以确定，当应用于新数据时，这是我们可以从分类器中预期的性能。虽然没有验证时报告的 100%准确率高，但还是很不错的分数！

# 将网格搜索与交叉验证相结合

我们刚刚实现的网格搜索的一个潜在危险是，结果可能对我们如何精确地分割数据相对敏感。毕竟，我们可能无意中选择了一个分割，将大部分易于分类的数据点放入测试集中，导致得分过于乐观。虽然一开始我们会很高兴，但当我们在一些新的数据上尝试该模型时，我们会发现分类器的实际性能远低于预期。

相反，我们可以将网格搜索和交叉验证结合起来。这样，数据被多次分割成训练集和验证集，并在网格搜索的每一步执行交叉验证以进行评估...

# 将网格搜索与嵌套交叉验证相结合

尽管带有交叉验证的网格搜索使得模型选择过程更加健壮，但是您可能已经注意到，我们仍然只执行了一次分割成训练集和验证集的操作。因此，我们的结果可能仍然过于依赖于数据的精确训练-验证分割。

我们可以更进一步，使用多个分割进行交叉验证，而不是将数据分割成训练集和验证集。这将导致所谓的**嵌套交叉验证**，过程如下图所示:

![](Images/5e469a29-e08e-478e-8c5e-fea9a6a789f7.png)

在嵌套交叉验证中，网格搜索框上有一个外部循环，重复地将数据分割成训练集和验证集。对于这些拆分中的每一个，将运行网格搜索，这将报告一组最佳参数值。然后，对于每个外部分割，我们使用最佳设置获得测试分数。

Running a grid search over many parameters and on large datasets can be computationally intensive. A particular parameter setting on a particular cross-validation split can be done completely independently from the other parameter settings and models. Hence, parallelization over multiple CPU cores or a cluster is very important for grid search and cross-validation.

现在我们知道了如何找到模型的最佳参数，让我们更仔细地看看我们可以用来给模型评分的不同评估指标。

# 使用不同评估指标的评分模型

到目前为止，我们已经使用准确性(正确分类样本的比例)评估了分类性能，使用 R <sup>2</sup> 评估了回归性能。然而，这只是总结监督模型在给定数据集上表现如何的众多可能方法中的两种。实际上，这些评估指标可能不适合我们的应用，在模型之间进行选择和调整参数时，选择正确的指标非常重要。

选择指标时，我们应该始终牢记机器学习应用程序的最终目标。在实践中，我们通常不仅对做出准确的预测感兴趣，而且对将这些预测作为更大的...

# 选择正确的分类标准

我们在[第 3 章](03.html)、*监督学习的第一步*中讨论了几个基本的评分函数。最基本的分类指标如下:

*   **准确性**:这将计算测试集中已被正确预测的数据点的数量，并将该数量作为测试集大小的一部分(`sklearn.metrics.accuracy_score`)返回。这是分类器最基本的评分函数，我们在整本书中都广泛使用了它。
*   **精度**:这描述了分类器不将阳性样本标记为阴性(`sklearn.metrics.precision_score`)的能力。
*   **回忆** **(或灵敏度)**:这描述了分类器检索所有阳性样本的能力(`sklearn.metrics.recall_score`)。

虽然精度和召回率是重要的衡量标准，但只看其中一项并不能让我们对全局有一个很好的了解。总结这两种度量的一种方法是 **f-score** 或 **f-measure** ( `sklearn.metrics.f1_score`)，它将精度和召回率的调和平均值计算为 *2(精度 x 召回率)/(精度+召回率)。*

有时我们需要做的不仅仅是最大限度地提高准确性。例如，如果我们在商业应用中使用机器学习，那么决策应该由业务目标驱动。这些目标之一可能是保证至少 90%的召回率。接下来的挑战是开发一个在满足所有次要需求的同时仍然具有合理精度的模型。像这样设定目标，通常叫做**设定操作点**。

然而，在开发新系统时，通常不清楚操作点应该是什么。为了更好地理解这个问题，重要的是调查所有可能的精度权衡，并立即召回它们。这可以通过一种叫做**精确-回忆曲线** ( `sklearn.metrics.precision_recall_curve`)的工具来实现。

Another commonly used tool to analyze the behavior of classifiers is the **Receiver Operating Characteristic** (**ROC**) curve. The ROC curve considers all possible thresholds for a given classifier similar to the precision-recall curve, but it shows the *false positive rate* against the *true positive rate* instead of reporting precision and recall.

# 选择正确的回归度量

回归的评估可以像分类一样详细地完成。在[第 3 章](03.html)、*监督学习的第一步*中，我们也谈到了回归的一些基本度量:

*   **均方误差**:回归问题最常用的误差度量是测量训练集中每个数据点的预测值和真实目标值之间的平方误差，在所有数据点上取平均值(`sklearn.metrics.mean_squared_error`)。
*   **解释方差**:一个更复杂的度量是衡量一个模型可以在多大程度上解释测试数据的变化或分散(`sklearn.metrics.explained_variance_score`)。通常，解释的差异量是通过使用...

# 将算法链接在一起以形成流水线

到目前为止，我们讨论的大多数机器学习问题至少包括一个预处理步骤和一个分类步骤。问题越复杂，这个*加工链*可能会越长。将多个处理步骤粘合在一起，甚至在网格搜索中使用它们的一种方便方法是使用 scikit-learn 中的`Pipeline`类。

# 在 scikit-learn 中实现管道

`Pipeline`类本身有一个`fit`、`predict`和一个`score`方法，它们的行为就像 scikit-learn 中的任何其他估计器一样。`Pipeline`类最常见的用例是将不同的预处理步骤与一个监督模型(如分类器)链接在一起。

让我们从[第 5 章](05.html)*回到乳腺癌数据集，使用决策树进行医学诊断*。使用 scikit-learn，我们导入数据集，并将其分成训练集和测试集:

```py
In [1]: from sklearn.datasets import load_breast_cancer...     import numpy as np...     cancer = load_breast_cancer()...     X = cancer.data.astype(np.float32)...     y = cancer.targetIn [2]: X_train, X_test, y_train, y_test = train_test_split(... X, y, random_state=37 ...
```

# 在网格搜索中使用管道

在网格搜索中使用管道的工作方式与使用任何其他估计器相同。

我们定义一个参数网格，从管道和参数网格中搜索并构建`GridSearchCV`。但是，在指定参数网格时，会有细微的变化。我们需要为每个参数指定它属于管道的哪个步骤。我们要调整的两个参数`C`和`gamma`，都是`SVC`的参数。在前一节中，我们给这个步骤命名为`"svm"`。为管道定义参数网格的语法是为每个参数指定步骤名称，后跟`__`(双下划线)，后跟参数名称。

因此，我们将如下构建参数网格:

```py
In [8]: param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],
...                   'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}
```

有了这个参数网格，我们可以像往常一样使用`GridSearchCV`:

```py
In [9]: grid = GridSearchCV(pipe, param_grid=param_grid, cv=10)
...     grid.fit(X_train, y_train);
```

网格中的最佳分数存储在`best_score_`中:

```py
In [10]: grid.best_score_
Out[10]: 0.97652582159624413
```

同样，最佳参数存储在`best_params_`中:

```py
In [11]: grid.best_params_
Out[11]: {'svm__C': 1, 'svm__gamma': 1}
```

但是回想一下，交叉验证分数可能过于乐观。为了了解分类器的真实性能，我们需要在测试集上对其进行评分:

```py
In [12]: grid.score(X_test, y_test)
Out[12]: 0.965034965034965
```

与我们之前进行的网格搜索相反，现在，对于交叉验证中的每个分割，`MinMaxScaler`仅使用训练分割进行重新调整，并且没有信息从测试分割泄露到参数搜索中。

这使得构建一个管道来链接各种步骤变得很容易！你可以在管道中随意混合匹配估计器，你只需要保证管道中的每一步都提供一个`transform`方法(除了最后一步)。这允许流水线中的估计器产生数据的新表示，这又可以用作下一步的输入。

The `Pipeline` class is not restricted to preprocessing and classification but can, in fact, join any number of estimators together. For example, we could build a pipeline containing feature extraction, feature selection, scaling, and classification, for a total of four steps. Similarly, the last step could be regression or clustering instead of classification.

# 摘要

在本章中，我们试图通过讨论模型选择和超参数调整的最佳实践来补充我们现有的机器学习技能。您已经学习了如何在 OpenCV 和 scikit-learn 中使用网格搜索和交叉验证来调整模型的超参数。我们还讨论了各种各样的评估指标，以及如何将算法链接到管道中。现在，你几乎准备好开始自己解决一些现实世界的问题了。

在下一章中，将向您介绍一个激动人心的新主题，即 OpenVINO toolkit，它是 OpenCV 4.0 的关键版本之一。