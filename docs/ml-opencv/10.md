# 十、分类的集成方法

到目前为止，我们已经研究了许多有趣的机器学习算法，从线性回归等经典方法到深度神经网络等更先进的技术。在不同的地方，我们指出每种算法都有自己的优点和缺点——我们注意到了如何发现和克服这些缺点。

然而，如果我们可以简单地将一堆平均分类器堆叠在一起，形成一个更强大的分类器**集成**，那不是很棒吗？

在本章中，我们将这样做。集成方法是将多个不同的模型绑定在一起以解决一个共享问题的技术。它们的使用已经成为竞争性机器学习的一种常见做法...

# 技术要求

可以从以下链接查阅本章代码:[https://github . com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/chapter 10](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter10)。

以下是软件和硬件要求的简短总结:

*   OpenCV 版本 4.1.x (4.1.0 或 4.1.1 都可以正常工作)。
*   Python 3.6 版本(任何 Python 3 . x 版本都可以)。
*   Anaconda Python 3，用于安装 Python 和所需的模块。
*   这本书可以使用任何操作系统——苹果操作系统、视窗操作系统或基于 Linux 的操作系统。我们建议您的系统中至少有 4 GB 内存。
*   你不需要一个图形处理器来运行书中提供的代码。

# 理解集成方法

集成方法的目标是结合用给定学习算法构建的几个独立估计器的预测，以解决一个共享问题。通常，一个合奏由两个主要部分组成:

*   一套模型
*   一组决策规则，用于控制如何将这些模型的结果组合成单个输出

The idea behind ensemble methods has much to do with the *wisdom of the crowd* concept. Rather than the opinion of a single expert, we consider the collective opinion of a group of individuals. In the context of machine learning, these individuals would be classifiers or regressors. The idea is that if we just ask a large enough number of classifiers, one of them ought to get ...

# 理解平均系综

平均方法在机器学习中有很长的历史，通常应用于分子动力学和音频信号处理等领域。这种集合通常被视为给定系统的精确复制品。

平均集合本质上是在同一数据集上训练的模型的集合。然后以多种方式汇总他们的结果。

一种常见的方法是创建多个模型配置，这些配置采用不同的参数子集作为输入。采用这种方法的技术统称为装袋方法。

装袋方法有许多不同的口味。但是，它们的不同之处通常在于它们绘制训练集随机子集的方式:

*   粘贴方法绘制样本的随机子集，而不替换数据样本。
*   Bagging 方法通过替换数据样本来抽取样本的随机子集。
*   随机子空间方法绘制特征的随机子集，但在所有数据样本上进行训练。
*   随机面片方法绘制样本和特征的随机子集。

Averaging ensembles can be used to reduce the variability of a model's performance.

在 scikit-learn 中，打包方法可以使用`BaggingClassifier`和`BaggingRegressor`元估计器来实现。这些是元估计器，因为它们允许我们从任何其他基础估计器构建一个集合。

# 实现打包分类器

例如，我们可以从 10 个 *k* -NN 分类器的集合中构建一个集成，如下所示:

```py
In [1]: from sklearn.ensemble import BaggingClassifier...     from sklearn.neighbors import KNeighborsClassifier...     bag_knn = BaggingClassifier(KNeighborsClassifier(),...                                 n_estimators=10)
```

`BaggingClassifier`类提供了许多选项来定制集合:

*   `n_estimators`:如前面的代码所示，这指定了集合中基本估计量的数量。
*   `max_samples`:这表示从数据集中抽取样本的数量(或分数)，以训练每个基本估计量。我们可以将`bootstrap=True`设置为替换取样(有效实施装袋)，也可以将`bootstrap=False`设置为实施...

# 实现打包回归器

同样，我们可以使用`BaggingRegressor`类来形成回归器的集合。

例如，我们可以构建一个决策树集合，从第 3 章、*的波士顿数据集预测房价，这是监督学习的第一步*。

在以下步骤中，您将学习如何使用 bagging 回归器来形成回归器的集合:

1.  语法几乎与设置 bagging 分类器相同:

```py
In [7]: from sklearn.ensemble import BaggingRegressor
...     from sklearn.tree import DecisionTreeRegressor
...     bag_tree = BaggingRegressor(DecisionTreeRegressor(),
...                                 max_features=0.5, n_estimators=10, 
...                                 random_state=3)
```

2.  当然，我们需要像对乳腺癌数据集那样加载和拆分数据集:

```py
In [8]: from sklearn.datasets import load_boston
...     dataset = load_boston()
...     X = dataset.data
...     y = dataset.target
In [9]: from sklearn.model_selection import train_test_split
...     X_train, X_test, y_train, y_test = train_test_split(
...         X, y, random_state=3
...     )
```

3.  然后，我们可以在`X_train`上拟合装袋回归器，并在`X_test`上进行评分:

```py
In [10]: bag_tree.fit(X_train, y_train)
...      bag_tree.score(X_test, y_test)
Out[10]: 0.82704756225081688
```

在前面的示例中，我们发现性能提升了大约 5%，从单个决策树的 77.3%准确率提升到 82.7%。

当然，我们不会就此打住。没有人说集合需要由 10 个独立的估计器组成，所以我们可以自由探索不同大小的集合。除此之外，`max_samples`和`max_features`参数允许大量定制。

A more sophisticated version of bagged decision trees is called random forests, which we will talk about later in this chapter.

# 理解增强合奏

另一种构建合奏的方法是通过增强。增强模型按顺序使用多个个体学习者来迭代地增强集成的性能。

通常，用于增强的学习者相对简单。一个很好的例子是只有一个节点的决策树——一个决策树桩。另一个例子可以是简单的线性回归模型。我们的想法不是拥有最强的个人学习者，而是相反的——我们希望个人成为弱学习者，这样我们只有在考虑大量个人时才能获得优异的表现。

在该过程的每次迭代中，训练集被调整，使得下一个分类器被应用于...

# 弱学习者

弱学习者是与实际分类只有轻微关联的分类器；它们可能比随机预测要好一些。相反，强有力的学习者与正确的分类任意相关。

这里的想法是，你不能只使用一个，而是使用一组广泛的弱学习者，每一个都比随机的稍好。弱学习者的许多实例可以使用 boosting、bagging 等集合在一起，以创建强集成分类器。好处是最终的分类器不会导致*在你的训练数据上过度拟合*。

例如，AdaBoost 在不同的加权训练数据上适合一系列弱学习者。它从预测训练数据集开始，并给予每个观察/样本同等的权重。如果第一个学习者的预测是不正确的，那么它会给予预测错误的观察/样本更高的权重。由于这是一个迭代过程，它会继续添加学习者，直到模型数量或准确性达到极限。

# 实现增强分类器

例如，我们可以从 10 棵决策树的集合中构建一个增强分类器，如下所示:

```py
In [11]: from sklearn.ensemble import GradientBoostingClassifier...      boost_class = GradientBoostingClassifier(n_estimators=10,...                                               random_state=3)
```

这些分类器支持二进制和多类分类。

类似于`BaggingClassifier`类，`GradientBoostingClassifier`类提供了许多选项来定制集合:

*   `n_estimators`:这表示集合中基础估计量的数量。大量的估计器通常会带来更好的性能。
*   `loss`:表示需要优化的损失函数(或成本函数)。设置`loss='deviance'`实现逻辑回归...

# 实现增强回归器

实现增强回归器遵循与增强分类器相同的语法:

```py
In [15]: from sklearn.ensemble import GradientBoostingRegressor
...      boost_reg = GradientBoostingRegressor(n_estimators=10,
...                                            random_state=3)
```

我们之前已经看到，在波士顿数据集上，单个决策树可以达到 79.3%的准确率。由 10 棵回归树组成的袋装决策树分类器达到 82.7%的准确率。但是一个被提升的回归者如何比较呢？

让我们重新加载波士顿数据集，并将其分成训练集和测试集。我们希望确保对`random_state`使用相同的值，以便最终在相同的数据子集上进行训练和测试:

```py
In [16]: dataset = load_boston()
...      X = dataset.data
...      y = dataset.target
In [17]: X_train, X_test, y_train, y_test = train_test_split(
...          X, y, random_state=3
...     )
```

事实证明，增强的决策树集成实际上比之前的代码表现更差:

```py
In [18]: boost_reg.fit(X_train, y_train)
...      boost_reg.score(X_test, y_test)
Out[18]: 0.71991199075668488
```

这个结果一开始可能会令人困惑。毕竟，我们使用的分类器比单一决策树多 10 倍。为什么我们的数字会变得更糟？

你可以看到这是一个专家分类器比一群弱学习者更聪明的好例子。一个可能的解决方案是让整体变大。事实上，在强化合奏中，习惯上使用大约 100 个弱学习者:

```py
In [19]: boost_reg = GradientBoostingRegressor(n_estimators=100)
```

然后，当我们在波士顿数据集上重新训练集成时，我们得到了 89.8%的测试分数:

```py
In [20]: boost_reg.fit(X_train, y_train)
...      boost_reg.score(X_test, y_test)
Out[20]: 0.89984081091774459
```

当你把数字增加到`n_estimators=500`时会发生什么？通过使用可选参数，我们可以做更多的事情。

如您所见，boosting 是一个强大的过程，它允许您通过组合大量相对简单的学习者来获得巨大的性能提升。

A specific implementation of boosted decision trees is the AdaBoost algorithm, which we will talk about later in this chapter.

# 理解堆叠集合

到目前为止，我们看到的所有集成方法都有一个共同的设计理念:将多个单独的分类器与数据相匹配，并借助一些简单的决策规则(如平均或提升)将其预测合并到最终预测中。

另一方面，堆叠系综构建了具有层次的系综。这里，个体学习者被组织成多个层，其中一层学习者的输出被用作下一层模型的训练数据。这样，就有可能成功地融合数百种不同的模式。

不幸的是，详细讨论堆叠集合超出了本书的范围。

然而，正如所见，这些模型可能非常强大，...

# 将决策树组合成随机森林

袋装决策树的一种流行变体是所谓的随机森林。这些本质上是决策树的集合，其中每个树与其他树略有不同。与袋装决策树不同，随机森林中的每棵树都是在稍微不同的数据特征子集上训练的。

尽管单个无限深度的树可能在预测数据方面做得相对较好，但它也容易过度拟合。随机森林背后的想法是建立大量的树，每个树都在数据样本和特征的随机子集上训练。由于过程的随机性，森林中的每棵树都会以稍微不同的方式对数据进行过度填充。过度拟合的影响可以通过对单棵树的预测取平均值来降低。

# 理解决策树的缺点

决策树经常遭受数据集过拟合的影响，通过一个简单的例子可以最好地证明这一点。

为此，我们将从 scikit-learn 的`datasets`模块返回到`make_moons`功能，我们之前在[第 8 章](08.html)、*使用无监督学习发现隐藏结构*将数据组织成两个交错的半圆。这里，我们选择生成属于两个半圆的 100 个数据样本，结合一些标准偏差为`0.25`的高斯噪声:

```py
In [1]: from sklearn.datasets import make_moons...     X, y = make_moons(n_samples=100, noise=0.25,...                       random_state=100)
```

我们可以使用 matplotlib 和`scatter`可视化这些数据

# 实现我们的第一个随机森林

在 OpenCV 中，可以使用`ml`模块中的`RTrees_create`功能构建随机森林:

```py
In [7]: import cv2
...     rtree = cv2.ml.RTrees_create()
```

树对象提供了许多选项，其中最重要的是:

*   `setMaxDepth`:这将设置集合中每棵树的最大可能深度。如果首先满足其他终止标准，实际获得的深度可能会更小。
*   `setMinSampleCount`:这设置了一个节点可以包含的最小样本数，以便进行拆分。
*   `setMaxCategories`:设置允许的最大类别数。将类别数设置为比数据中实际类别数小的值会导致子集估计。
*   `setTermCriteria`:设置算法的终止条件。这也是您设置森林中树木数量的地方。

Although we might have hoped for a `setNumTrees` method to set the number of trees in the forest (kind of the most important parameter of them all, no?), we instead need to rely on the `setTermCriteria` method. Confusingly, the number of trees is conflated with `cv2.TERM_CRITERA_MAX_ITER`, which is usually reserved for the number of iterations that an algorithm is run for, not for the number of estimators in an ensemble.

我们可以通过向`setTermCriteria`方法传递一个整数`n_trees`来指定森林中的树木数量。在这里，我们还想告诉算法，一旦分数从一次迭代到下一次迭代至少没有增加`eps`就退出:

```py
In [8]: n_trees = 10
...     eps = 0.01
...     criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,
...                 n_trees, eps)
...     rtree.setTermCriteria(criteria)
```

然后，我们准备在前面代码的数据上训练分类器:

```py
In [9]: rtree.train(X_train.astype(np.float32), cv2.ml.ROW_SAMPLE,
                    y_train);
```

测试标签可以用`predict`方法预测:

```py
In [10]: _, y_hat = rtree.predict(X_test.astype(np.float32))
```

使用 scikit-learn 的`accuracy_score`，我们可以在测试集上评估模型:

```py
In [11]: from sklearn.metrics import accuracy_score
...      accuracy_score(y_test, y_hat)
Out[11]: 0.83999999999999997
```

经过训练，我们可以将预测的标签传递给`plot_decision_boundary`功能:

```py
In [12]: plot_decision_boundary(rtree, X_test, y_test)
```

这将产生以下图:

![](Images/637629d2-7e17-4c9a-9a00-a58b68d8f480.png)

上图显示了随机森林分类器的决策场景。

# 用 scikit-learn 实现随机森林

或者，我们可以使用 scikit-learn 实现随机森林:

```py
In [13]: from sklearn.ensemble import RandomForestClassifier...      forest = RandomForestClassifier(n_estimators=10, random_state=200)
```

在这里，我们有许多选项来定制集合:

*   `n_estimators`:指定森林中的树木数量。
*   `criterion`:指定节点拆分的标准。设置`criterion='gini'`实现基尼杂质，设置`criterion='entropy'`实现信息增益。
*   `max_features`:指定每个节点分割时要考虑的特征数量(或分数)。
*   `max_depth`:指定每棵树的最大深度。
*   `min_samples`:指定最小数量...

# 实现极度随机化的树

随机森林已经相当随意了。但是如果我们想把随机性发挥到极致呢？

在极其随机的树中(见`ExtraTreesClassifier`和`ExtraTreesRegressor`类)，随机性甚至比随机森林更进一步。还记得决策树通常如何为每个特征选择一个阈值，以使节点分裂的纯度最大化吗？另一方面，极度随机化的树会随机选择这些阈值。然后，这些随机生成的阈值中的最佳阈值被用作拆分规则。

我们可以构建一个极其随机的树，如下所示:

```py
In [16]: from sklearn.ensemble import ExtraTreesClassifier
...      extra_tree = ExtraTreesClassifier(n_estimators=10, random_state=100)
```

为了说明单个决策树、随机森林和极随机树之间的区别，让我们考虑一个简单的数据集，例如 Iris 数据集:

```py
In [17]: from sklearn.datasets import load_iris
...      iris = load_iris()
...      X = iris.data[:, [0, 2]]
...      y = iris.target
In [18]: X_train, X_test, y_train, y_test = train_test_split(
...          X, y, random_state=100
...      )
```

然后，我们可以像以前一样对树对象进行拟合和评分:

```py
In [19]: extra_tree.fit(X_train, y_train)
...      extra_tree.score(X_test, y_test)
Out[19]: 0.92105263157894735
```

相比之下，使用随机林会产生相同的性能:

```py
In [20]: forest = RandomForestClassifier(n_estimators=10,
                                        random_state=100)
...      forest.fit(X_train, y_train)
...      forest.score(X_test, y_test)
Out[20]: 0.92105263157894735
```

事实上，对于单棵树也是如此:

```py
In [21]: tree = DecisionTreeClassifier()
...      tree.fit(X_train, y_train)
...      tree.score(X_test, y_test)
Out[21]: 0.92105263157894735
```

那么，它们之间有什么区别呢？要回答这个问题，我们必须看决策边界。幸运的是，我们已经在前面的部分中导入了我们的`plot_decision_boundary`助手函数，所以我们所需要做的就是将不同的分类器对象传递给它。

我们将构建一个分类器列表，其中列表中的每个条目都是一个元组，包含一个索引、分类器的名称和分类器对象:

```py
In [22]: classifiers = [
...          (1, 'decision tree', tree),
...          (2, 'random forest', forest),
...          (3, 'extremely randomized trees', extra_tree)
...      ]
```

然后，很容易将分类器列表传递给我们的助手函数，这样每个分类器的决策场景都绘制在自己的子场景中:

```py
In [23]: for sp, name, model in classifiers:
...      plt.subplot(1, 3, sp)
...      plot_decision_boundary(model, X_test, y_test)
...      plt.title(name)
...      plt.axis('off')

```

结果是这样的:

![](Images/84874c28-7c2e-4c95-ba4a-139c4b2d6901.png)

现在三个分类器之间的区别变得更加清晰。我们看到单棵树绘制了迄今为止最简单的决策边界，使用水平决策边界分割景观。随机森林能够更清楚地将决策图左下角的数据点云分开。然而，只有极其随机的树能够从四面八方将数据点云转向景观中心。

现在我们已经知道了树集合的所有不同变体，让我们继续看真实世界的数据集。

# 使用随机森林进行人脸识别

我们还没怎么讨论的一个流行数据集是奥利韦蒂人脸数据集。

奥利维蒂人脸数据集是剑桥美国电话电报公司实验室在 1990 年收集的。该数据集包括 40 个不同受试者在不同时间和不同光照条件下拍摄的面部图像。此外，受试者的面部表情(睁开/闭上眼睛、微笑/不微笑)和面部细节(戴眼镜/不戴眼镜)各不相同。

然后，图像被量化为 256 个灰度级，并存储为无符号 8 位整数。因为有 40 个不同的主题，所以数据集带有 40 个不同的目标标签。因此，识别人脸构成了多类分类任务的一个例子。

# 正在加载数据集

像许多其他经典数据集一样，可以使用 scikit-learn 加载 Olivetti 人脸数据集:

```py
In [1]: from sklearn.datasets import fetch_olivetti_faces
...     dataset = fetch_olivetti_faces()
In [2]: X = dataset.data
...     y = dataset.target
```

虽然原始图像由 92 x 112 像素的图像组成，但通过 scikit-learn 提供的版本包含缩小到 *64 x 64* 像素的图像。

为了了解数据集，我们可以绘制一些示例图像。让我们从数据集中随机选取八个索引:

```py
In [3]: import numpy as np
...     np.random.seed(21)
...     idx_rand = np.random.randint(len(X), size=8)
```

我们可以使用 matplotlib 绘制这些示例图像，但是在绘制之前，我们需要确保将列向量重塑为 64 x 64 像素的图像:

```py
In [4]: import matplotlib.pyplot as plt
...     %matplotlib inline
...     for p, i in enumerate(idx_rand):
...         plt.subplot(2, 4, p + 1)
... plt.imshow(X[i, :].reshape((64, 64)), cmap='gray')
...         plt.axis('off')
```

上述代码产生以下输出:

![](Images/f68b62ef-a7ec-476b-8467-87c6fc2ba850.png)

你可以看到所有的脸是如何在黑暗的背景下拍摄的，并且是肖像。不同图像的面部表情差异很大，这使得分类成为一个有趣的问题。尽量不要嘲笑他们中的一些人！

# 预处理数据集

在我们将数据集传递给分类器之前，我们需要按照[第 4 章](04.html)、*中表示数据和工程特征*的最佳实践对其进行预处理。

具体来说，我们希望确保所有示例图像具有相同的平均灰度级别:

```py
In [5]: n_samples, n_features = X.shape[:2]...     X -= X.mean(axis=0)
```

我们对每个图像重复此过程，以确保每个数据点(即`X`中的一行)的特征值都以零为中心:

```py
In [6]: X -= X.mean(axis=1).reshape(n_samples, -1)
```

预处理数据可以使用以下代码可视化:

```py
In [7]: for p, i in enumerate(idx_rand):...         plt.subplot(2, 4, p + 1)...         plt.imshow(X[i, :].reshape((64, 64)), cmap='gray')... plt.axis('off') ...
```

# 随机森林的训练和测试

我们继续遵循最佳实践，将数据分为训练集和测试集:

```py
In [8]: from sklearn.model_selection import train_test_split
...     X_train, X_test, y_train, y_test = train_test_split(
...         X, y, random_state=21
...     )
```

然后，我们准备对数据应用随机森林:

```py
In [9]: import cv2
...     rtree = cv2.ml.RTrees_create()
```

在这里，我们想要创建一个包含 50 棵决策树的集合:

```py
In [10]: n_trees = 50
...      eps = 0.01
...      criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,
...                  n_trees, eps)
...      rtree.setTermCriteria(criteria)
```

因为我们有大量的类别(即 40 个)，所以我们希望确保随机森林被设置为相应地处理它们:

```py
In [10]: rtree.setMaxCategories(len(np.unique(y)))
```

我们可以使用其他可选参数，例如节点在拆分前所需的数据点数量:

```py
In [11]: rtree.setMinSampleCount(2)
```

然而，我们可能不想限制每棵树的深度。这也是我们最终必须试验的一个参数。但是现在，让我们将其设置为一个大的整数值，使深度有效地不受约束:

```py
In [12]: rtree.setMaxDepth(1000)
```

然后，我们可以将分类器与训练数据进行匹配:

```py
In [13]: rtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train);
```

我们可以使用以下函数来检查生成的树的深度:

```py
In [13]: rtree.getMaxDepth()
Out[13]: 25
```

这意味着，虽然我们允许树上升到深度 1000，但最终只需要 25 层。

分类器的评估再次通过首先预测标签(`y_hat`)然后将它们传递给`accuracy_score`功能来完成:

```py
In [14]: _, y_hat = tree.predict(X_test)
In [15]: from sklearn.metrics import accuracy_score
...      accuracy_score(y_test, y_hat)
Out[15]: 0.87
```

我们发现 87%的准确率，这比使用单一决策树要好得多:

```py
In [16]: from sklearn.tree import DecisionTreeClassifier
...      tree = DecisionTreeClassifier(random_state=21, max_depth=25)
...      tree.fit(X_train, y_train)
...      tree.score(X_test, y_test)
Out[16]: 0.46999999999999997
```

还不错！我们可以玩可选的参数，看看我们是否会变得更好。最重要的似乎是森林中的树木数量。我们可以用一个由 1000 棵树而不是 50 棵树组成的森林重复这个实验:

```py
In [18]: num_trees = 1000
... eps = 0.01
... criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,
... num_trees, eps)
... rtree.setTermCriteria(criteria)
... rtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train);
... _, y_hat = rtree.predict(X_test)
... accuracy_score(y_test, y_hat)
Out[18]: 0.94
```

通过这种配置，我们获得了 94%的准确率！

Here, we tried to improve the performance of our model through creative trial and error: we varied some of the parameters we deemed important and observed the resulting change in performance until we found a configuration that satisfied our expectations. We will learn more sophisticated techniques for improving a model in [Chapter 11](11.html), *Selecting the Right Model with Hyperparameter Tuning*.

决策树集成的另一个有趣的用例是 AdaBoost。

# 实现 AdaBoost

当森林中的树是深度为 1 的树(也称为**决策树桩**)并且我们执行助推而不是装袋时，得到的算法被称为 **AdaBoost** 。

AdaBoost 通过执行以下操作在每次迭代时调整数据集:

*   选择决策树桩
*   增加决策树桩标注不正确的案例的权重，同时减少标注正确的案例的权重

这种迭代权重调整使得集成中的每个新分类器优先训练错误标记的案例。因此，该模型通过瞄准高度加权的数据点进行调整。

最终，树桩被组合成最终的分类器。

# 在 OpenCV 中实现 AdaBoost

虽然 OpenCV 提供了非常高效的 AdaBoost 实现，但它隐藏在哈尔级联分类器下。哈尔级联分类器是非常流行的人脸检测工具，我们可以通过 Lena 图像的例子来说明:

```py
In [1]: img_bgr = cv2.imread('data/lena.jpg', cv2.IMREAD_COLOR)
...     img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
```

在加载彩色和灰度图像后，我们加载一个预处理的哈尔级联:

```py
In [2]: import cv2
...     filename = 'data/haarcascade_frontalface_default.xml'
...     face_cascade = cv2.CascadeClassifier(filename)
```

然后，分类器将使用以下函数调用检测图像中出现的人脸:

```py
In [3]: faces = face_cascade.detectMultiScale(img_gray, 1.1, 5)
```

请注意，该算法仅适用于灰度图像。这就是为什么我们保存了两张 Lena 的图片，一张我们可以应用分类器(`img_gray`)，另一张我们可以在上面绘制结果包围盒(`img_bgr`):

```py
In [4]: color = (255, 0, 0)
...     thickness = 2
...     for (x, y, w, h) in faces:
...         cv2.rectangle(img_bgr, (x, y), (x + w, y + h),
...                       color, thickness)
```

然后，我们可以使用以下代码绘制图像:

```py
In [5]: import matplotlib.pyplot as plt
...     %matplotlib inline
...     plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB));
```

这将产生以下输出，面的位置由蓝色边界框指示:

![](Images/1e4d1b54-f99e-423a-a8fd-e09aefbc989c.png)

显然，这张截图只包含了一张脸。然而，前面的代码即使在可以检测到多个人脸的图像上也能工作。试试看！

# 在 scikit-learn 中实现 AdaBoost

在 scikit-learn 中，AdaBoost 只是另一个集成估计器。我们可以用 50 个决策树桩创建一个集合，如下所示:

```py
In [6]: from sklearn.ensemble import AdaBoostClassifier...     ada = AdaBoostClassifier(n_estimators=50,...                              random_state=456)
```

我们可以再次加载乳腺癌集，并将其拆分为 75-25:

```py
In [7]: from sklearn.datasets import load_breast_cancer...     cancer = load_breast_cancer()...     X = cancer.data...     y = cancer.targetIn [8]: from sklearn.model_selection import train_test_split...     X_train, X_test, y_train, y_test = train_test_split(...         X, y, random_state=456...     )
```

然后，使用熟悉的程序`fit`和`score` AdaBoost:

```py
In [9]: ada.fit(X_train, y_train)...     ada.score(X_test, y_test)
```

# 将不同的模型组合成投票分类器

到目前为止，我们已经看到了如何将同一个分类器或回归器的不同实例组合成一个集合。在这一章中，我们将把这个想法更进一步，将概念上不同的分类器组合成一个被称为**投票分类器**。

投票分类器背后的想法是，集成中的单个学习者不一定需要属于同一类型。毕竟，不管单个分类器如何得出它们的预测，最终，我们将应用一个决策规则，该规则集成了单个分类器的所有投票。这也被称为**投票方案**。

# 了解不同的投票方案

两种不同的投票方案在投票分类器中很常见:

*   在**硬投票**(也称**多数投票**)中，每个个体分类器为一个类投票，多数获胜。在统计学术语中，集合的预测目标标签是单独预测标签的分布模式。
*   在**软投票**中，每个单独的分类器提供特定数据点属于特定目标类的概率值。预测通过分类器的重要性进行加权，并进行汇总。然后，加权概率之和最大的目标标签赢得投票。

例如，让我们假设集合中有三个不同的分类器执行...

# 实现投票分类器

让我们看一个投票分类器的简单例子，它结合了三种不同的算法:

*   来自[第 3 章](03.html)*的逻辑回归分类器:监督学习的第一步*
*   来自[第 7 章](07.html)、*的高斯朴素贝叶斯分类器利用贝叶斯学习*实现垃圾邮件过滤器
*   本章中的随机森林分类器

我们可以将这三种算法组合成投票分类器，并通过以下步骤将其应用于乳腺癌数据集:

1.  加载数据集，并将其拆分为训练集和测试集:

```py
In [1]: from sklearn.datasets import load_breast_cancer
...     cancer = load_breast_cancer()
...     X = cancer.data
...     y = cancer.target
In [2]: from sklearn.model_selection import train_test_split
...     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)
```

2.  实例化各个分类器:

```py
 In [3]: from sklearn.linear_model import LogisticRegression
...     model1 = LogisticRegression(random_state=13)
 In [4]: from sklearn.naive_bayes import GaussianNB
...     model2 = GaussianNB()
In [5]: from sklearn.ensemble import RandomForestClassifier
...     model3 = RandomForestClassifier(random_state=13)
```

3.  将单个分类器分配给投票集合。这里，我们需要传递一个元组列表(`estimators`)，其中每个元组由分类器的名称(描述每个分类器的简称的一串字母)和模型对象组成。投票方案可以是`voting='hard'`也可以是`voting='soft'`。目前，我们将选择**T3:**

```py
In [6]: from sklearn.ensemble import VotingClassifier
...     vote = VotingClassifier(estimators=[('lr', model1),
...                                ('gnb', model2),('rfc', model3)],voting='hard')
```

4.  将集合与训练数据相匹配，并在测试数据上进行评分:

```py
In [7]: vote.fit(X_train, y_train)
...     vote.score(X_test, y_test)
Out[7]: 0.95104895104895104
```

为了让我们相信 95.1%是一个很好的准确率，我们可以将集成的性能与每个单独分类器的理论性能进行比较。我们通过将单个分类器与数据进行拟合来实现这一点。然后，我们将看到逻辑回归模型自身达到 94.4%的精度:

```py
In [8]: model1.fit(X_train, y_train)
...     model1.score(X_test, y_test)
Out[8]: 0.94405594405594406
```

类似地，朴素贝叶斯分类器达到 93.0%的准确率:

```py
In [9]:  model2.fit(X_train, y_train)
...      model2.score(X_test, y_test)
Out[9]:  0.93006993006993011
```

最后但同样重要的是，随机森林分类器也达到了 94.4%的准确率:

```py
In [10]: model3.fit(X_train, y_train)
... model3.score(X_test, y_test)
Out[10]: 0.94405594405594406
```

总之，通过将三个不相关的分类器组合成一个集成，我们能够获得很好的性能百分比。这些分类器中的每一个都可能在训练集上犯了不同的错误，但这没关系，因为平均来说，我们只需要三个分类器中的两个就能正确。

# 复数

在前几节中，我们讨论了集成方法。我们之前没有提到的是，结果是如何在集成技术准备的单个模型中聚合的。用于此的概念叫做**复数**、**T3】无非就是投票。一个班级获得的票数越高，成为最终班级的机会就越大。想象一下，如果我们在集成技术中准备了三个模型和 10 个可能的类(把它们想象成从 0 到 9 的数字)。每个模型将根据它获得的最高概率选择一个类。最后，得票最多的班级将被选中。这就是多元的概念。在实践中，多元化试图给 T4 和天真带来好处...**

# 摘要

在本章中，我们讨论了如何通过将各种分类器组合成一个集成来改进它们。我们讨论了如何使用 bagging 对不同分类器的预测进行平均，以及如何使用 boosting 让不同的分类器纠正彼此的错误。我们花了很多时间来讨论组合决策树的所有可能方法，无论是决策树桩(AdaBoost)、随机森林还是极随机树。最后，我们学习了如何通过构建投票分类器来组合集成中不同类型的分类器。

在下一章中，我们将更多地讨论如何通过深入模型选择和超参数调整的世界来比较不同分类器的结果。