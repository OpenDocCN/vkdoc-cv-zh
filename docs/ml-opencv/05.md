# 五、利用决策树的医学诊断

既然我们知道如何处理各种形状和形式的数据，无论是数字数据、分类数据、文本数据还是图像数据，现在是时候好好利用我们新获得的知识了。

在本章中，我们将学习如何构建一个可以进行医学诊断的机器学习系统。我们并不都是医生，但在我们一生中的某个时刻，我们可能都去过一次。通常，医生会获得尽可能多的关于患者病史和症状的信息，以做出明智的诊断。我们将借助所谓的**决策树**来模拟医生的决策过程。我们还将涵盖基尼系数、信息增益和方差减少，以及过度拟合和修剪。

决策树是一种简单而强大的监督学习算法，类似于流程图；我们将在一分钟内详细讨论这个问题。除了在医学领域，决策树通常用于天文学(例如，从哈勃太空望远镜图像中过滤噪声或对恒星-星系团进行分类)、制造和生产(例如，由波音公司发现制造过程中的缺陷)以及物体识别(例如，识别 3D 物体)等领域。

具体来说，我们希望在本章中了解以下内容:

*   从数据中构建简单的决策树，并将其用于分类或回归
*   使用基尼系数、信息增益和方差缩减来决定下一步做什么决定
*   规划决策树及其好处

但首先，让我们谈谈决策树实际上是什么。

# 技术要求

可以从以下链接查阅本章代码:[https://github . com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/chapter 05](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05)。

以下是软件和硬件要求的总结:

*   您将需要 OpenCV 版本 4.1.x (4.1.0 或 4.1.1 都可以)。
*   您将需要 Python 3.6 版本(任何 Python 3 . x 版本都可以)。
*   您将需要 Anaconda Python 3 来安装 Python 和所需的模块。
*   除了这本书，你可以使用任何操作系统——苹果操作系统、视窗操作系统和基于 Linux 的操作系统。我们建议您的系统中至少有 4 GB 内存。
*   你不需要一个图形处理器来运行本书提供的代码。

# 理解决策树

决策树是一个简单而强大的有监督学习问题的模型。顾名思义，我们可以把它想象成一棵树，其中信息沿着不同的分支流动——从树干开始，一直到单个的叶子，决定在每个连接点采取哪个分支。

这基本上是一个决策树！下面是决策树的一个简单示例:

![](Images/10e1cb79-f006-49d7-aab2-6b1b877e39f4.png)

决策树由关于数据(也称为**决策节点**)及其可能后果的问题或测试的层次结构组成。

构建决策树的真正困难之一是如何从数据中提取合适的特征。为了说明这一点，让我们用一个具体的例子。假设我们有一个由一封电子邮件组成的数据集:

```py
In [1]: data = [
...       'I am Mohammed Abacha, the son of the late Nigerian Head of '
...       'State who died on the 8th of June 1998\. Since i have been '
...       'unsuccessful in locating the relatives for over 2 years now '
...       'I seek your consent to present you as the next of kin so '
...       'that the proceeds of this account valued at US$15.5 Million '
...       'Dollars can be paid to you. If you are capable and willing '
...       'to assist, contact me at once via email with following '
...       'details: 1\. Your full name, address, and telephone number. '
...       '2\. Your Bank Name, Address. 3.Your Bank Account Number and '
...       'Beneficiary Name - You must be the signatory.'
...     ]
```

这封电子邮件可以像我们在上一章中所做的那样，使用 scikit-learn 的`CountVectorizer`进行矢量化:

```py
In [2]: from sklearn.feature_extraction.text import CountVectorizer
... vec = CountVectorizer()
... X = vec.fit_transform(data)
```

从上一章中，我们知道可以使用以下功能查看`X`中的特征名称:

```py
In [3]: function:vec.get_feature_names()[:5]
Out[3]: ['15', '1998', '8th', 'abacha', 'account']
```

为了清楚起见，我们只关注前五个单词，它们按字母顺序排序。然后，相应的出现次数可以如下所示:

```py
In [4]: X.toarray()[0, :5]
Out[4]: array([1, 1, 1, 1, 2], dtype=int64)
```

这告诉我们，五个单词中有四个只在邮件中出现一次，但单词`account`(最后一个列在`Out[3]`中)实际上出现了两次。在最后一章中，我们键入`X.toarray()`将稀疏数组`X`转换为人类可读的数组。结果是一个 2D 数组，其中行对应于数据样本，列对应于前面命令中描述的要素名称。由于数据集中只有一个样本，我们将自己限制在数组的第 0 行(即第一个数据样本)和数组的前五列(即前五个字)。

那么，我们如何检查电子邮件是否来自尼日利亚王子？

一种方法是查看电子邮件是否同时包含单词`nigerian`和`prince`:

```py
In [5]: 'nigerian' in vec.get_feature_names()
Out[5]: True
In [6]: 'prince' in vec.get_feature_names()
Out[6]: False
```

让我们惊讶的是，我们发现了什么？`prince`这个词在邮件中没有出现。

这是否意味着信息是合法的？

不，当然不是。这封电子邮件没有标上`'prince'`，而是标上了`head of state`，有效地绕过了我们过于简单的垃圾邮件检测器。

同样，我们甚至如何开始模拟树中的第二个决定:*想让我给他寄钱？*文本中没有直截了当的特征来回答这个问题。因此，这是一个特征工程的问题，即以允许我们回答这个问题的方式组合消息中实际出现的单词。当然，一个好的迹象是寻找像`US$`和`money`这样的字符串，但是我们仍然不知道这些单词被提到的上下文。据我们所知，也许它们是句子的一部分:*别担心，我不想你给我寄钱。*

更糟糕的是，事实证明，我们问这些问题的顺序实际上会影响最终结果。比如，如果我们先问最后一个问题:*我真的认识一个尼日利亚王子吗？*假设我们有一个尼日利亚王子作为叔叔，那么在电子邮件中找到*尼日利亚王子*可能就不再可疑了。

如你所见，这个看似简单的例子很快就失控了。

幸运的是，决策树背后的理论框架帮助我们找到正确的决策规则以及下一步要处理的决策。

然而，要理解这些概念，我们还得再深入一点。

# 构建我们的第一棵决策树

我认为我们已经为一个更复杂的例子做好了准备。如前所述，现在让我们进入医疗领域。

让我们考虑一个例子，几个病人患有同样的疾病，例如一种罕见的底栖病。让我们进一步假设这种疾病的真正原因至今仍不为人所知，并且我们所能获得的所有信息都由一系列生理测量组成。例如，我们可能可以访问以下信息:

*   患者的血压(`BP`)
*   患者的胆固醇水平(`cholesterol`)
*   患者的性别(`sex`)
*   患者年龄(`age`)
*   一个病人的血钠浓度(`Na`)
*   一个病人的血钾浓度(`K`)

基于所有的...

# 生成新数据

在继续下一步之前，让我们快速了解每个机器学习工程师的一个非常关键的步骤——数据生成。我们知道，所有的机器学习和深度学习技术都需要大量的数据——简单来说:越大越好。但是如果你没有足够的数据呢？你可能会得到一个不够精确的模型。常用的技术(如果您不能生成任何新数据)是使用大部分数据进行训练。这样做的主要缺点是，你的模型不是一般化的，或者换句话说，存在过度拟合的问题。

解决上述问题的一个办法是生成新数据，或者通常所说的合成数据。这里需要注意的关键点是，合成数据应该具有与真实数据相似的特征。它们与真实数据越相似，对作为 ML 工程师的你越有利。这种技术被称为**数据增强**，在这里我们使用各种技术，如旋转和镜像来生成基于现有数据的新数据。

由于我们在这里处理的是一个假设的情况，我们可以编写简单的 Python 代码来生成随机数据——因为这里没有为我们设置的特性。在现实世界中，您将使用数据扩充来生成看起来真实的新数据样本。让我们看看如何处理我们的案例。

在这里，数据集实际上是一个字典列表，其中每个字典构成一个数据点，包含患者的血液工作、年龄和性别，以及处方药物。因此，我们知道我们想要创建新的字典，我们知道在这本字典中使用的键。接下来要关注的是字典中值的数据类型。

我们从`age`开始，T0 是一个整数，然后是性别，不是`M`就是`F`。同样，对于其他值，我们可以推断数据类型，在某些情况下，使用常识，我们甚至可以推断要使用的值的范围。

It is very important to note that common sense and deep learning don't go well together most of the time. This is because you want your model to understand when something is an outlier. For example, we know that it's highly unlikely for someone to have an age of 130 but a generalized model should understand that this value is an outlier and should not be taken into account. This is why you should always have a small portion of data with such illogical values.

让我们看看如何为我们的案例生成一些合成数据:

```py
import random

def generateBasorexiaData(num_entries):
    # We will save our new entries in this list 
    list_entries = []
    for entry_count in range(num_entries):
        new_entry = {}
        new_entry['age'] = random.randint(20,100)
        new_entry['sex'] = random.choice(['M','F'])
        new_entry['BP'] = random.choice(['low','high','normal'])
        new_entry['cholestrol'] = random.choice(['low','high','normal'])
        new_entry['Na'] = random.random()
        new_entry['K'] = random.random()
        new_entry['drug'] = random.choice(['A','B','C','D'])
        list_entries.append(new_entry)
    return list_entries
```

如果我们想生成五个新条目，我们可以使用`entries = generateBasorexiaData (5)`调用前面的函数。

既然我们知道了如何生成数据，让我们看看我们可以用这些数据做些什么。我们能想出医生开药的理由`A`、`B`、`C`或`D`吗？我们能看到病人的血液值和医生开的药之间的关系吗？

很有可能，这个问题对你来说和对我来说一样难回答。尽管数据集乍一看可能是随机的，但事实上，我已经在患者的血液值和处方药物之间建立了一些明确的关系。让我们看看决策树是否能揭示这些隐藏的关系。

# 通过理解数据来理解任务

解决一个新的机器学习问题的第一步总是什么？

你完全正确:获得数据的感觉。我们越了解数据，就越了解我们试图解决的问题。在我们未来的努力中，这也将帮助我们选择合适的机器学习算法。

首先要意识到的是`drug`列实际上并不像所有其他列一样是一个特征值。由于我们的目标是根据患者的血液值预测将开出哪种药物，因此`drug`栏实际上成为了目标标签。换句话说，机器学习算法的输入将是 a 的血液值、年龄和性别...

# 预处理数据

为了让决策树算法理解我们的数据，我们需要将所有分类特征(`sex`、`BP`、`cholesterol`)转换为数字特征。最好的方法是什么？

没错:我们使用 scikit-learn 的`DictVectorizer`。就像我们在上一章中所做的那样，我们将想要转换的数据集输入到`fit_transform`方法中:

```py
In [10]: from sklearn.feature_extraction import DictVectorizer
...      vec = DictVectorizer(sparse=False)
...      data_pre = vec.fit_transform(data)
```

然后，`data_pre`包含预处理后的数据。如果要看第一个数据点(即`data_pre`的第一行)，我们将要素名称与对应的特征值进行匹配:

```py
In [12]: vec.get_feature_names()
Out[12]: ['BP=high', 'BP=low', 'BP=normal', 'K', 'Na', 'age',
...       'cholesterol=high', 'cholesterol=normal',
...       'sex=F', 'sex=M']
In [13]: data_pre[0]
Out[13]: array([ 1\. , 0\. , 0\. , 0.06, 0.66, 33\. , 1\. , 0\. ,
                 1\. , 0\. ])
```

由此，我们可以看到，三个分类变量——血压(`BP`)、胆固醇水平(`cholesterol`)和性别(`sex`)——已经使用一热编码进行了编码。

为了确保我们的数据变量与 OpenCV 兼容，我们需要将所有内容转换为浮点值:

```py
In [14]: import numpy as np
...      data_pre = np.array(data_pre, dtype=np.float32)
...      target = np.array(target, dtype=np.float32)
```

然后，剩下要做的就是将数据分成训练集和测试集，就像我们在[第 3 章](03.html)、*监督学习的第一步*中所做的那样。请记住，我们总是希望将训练集和测试集分开。由于在这个例子中我们只有 20 个数据点要处理，我们可能应该保留 10%以上的数据用于测试。15-5 的分成在这里似乎是合适的。我们可以明确地命令`split`函数产生正好五个测试样本:

```py
In [15]: import sklearn.model_selection as ms
...      X_train, X_test, y_train, y_test =
...      ms.train_test_split(data_pre, target, test_size=5,
...      random_state=42)
```

# 构建树

使用 OpenCV 构建决策树的工作方式与[第 3 章](03.html)、*监督学习的第一步*非常相似。回想一下，所有的机器学习功能都存在于 OpenCV 3.1 的`ml`模块中:

1.  我们可以使用以下代码创建一个空决策树:

```py
In [16]: import cv2...      dtree = cv2.ml.dtree_create()
```

2.  为了在训练数据上训练决策树，我们使用`train`方法。这就是我们之前将数据转换为浮点的原因——这样我们就可以在`train`方法中使用它:

```py
In [17]: dtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train)
```

这里，我们必须指定`X_train`中的数据样本是占据行(使用`cv2.ml.ROW_SAMPLE`)还是列(`cv2.ml.COL_SAMPLE`)。

3.  然后，我们可以预测...

# 可视化经过训练的决策树

如果你刚刚起步，不太关心引擎盖下发生的事情，OpenCV 对决策树的实现已经足够好了。然而，在接下来的部分中，我们将切换到 scikit-learn。它的实现允许我们定制算法，并使研究树的内部工作变得更加容易。它的用法也有更好的记录。

在 scikit-learn 中，决策树可以用于分类和回归。它们位于`tree`模块中:

1.  我们先从`sklearn`导入`tree`模块:

```py
In [21]: from sklearn import tree
```

2.  类似于 OpenCV，然后我们使用`DecisionTreeClassifier`构造函数创建一个空决策树:

```py
In [22]: dtc = tree.DecisionTreeClassifier()
```

3.  然后可以使用`fit`方法训练树:

```py
In [23]: dtc.fit(X_train, y_train)
Out[23]: DecisionTreeClassifier(class_weight=None, criterion='gini',
            max_depth=None, max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
```

4.  然后，我们可以使用`score`方法计算训练集和测试集的准确度分数:

```py
In [24]: dtc.score(X_train, y_train)
Out[24]: 1.0
In [25]: dtc.score(X_test, y_test)
Out[25]: 0.40000000000000002
```

现在，有一件很酷的事情:如果你想知道树的样子，你可以使用 GraphViz 从树结构中创建一个 PDF 文件(或者任何其他支持的文件类型)。为此，您需要首先安装 GraphViz。不要担心，因为它已经存在于我们在本书开头创造的环境中。

5.  然后，回到 Python 中，您可以使用 scikit-learn 的`export_graphviz`导出器将 GraphViz 格式的树导出到文件`tree.dot`:

```py
In [26]: with open("tree.dot", 'w') as f:
... tree.export_graphviz(clf, out_file=f)
```

6.  然后，回到命令行，您可以使用 GraphViz 将`tree.dot`转换为(例如)一个 PNG 文件:

```py
$ dot -Tpng tree.dot -o tree.png
```

或者，您也可以指定`-Tpdf`或任何其他支持的图像格式。前面的树的结果如下所示:

![](Images/3fa7b1d2-d71b-4531-b853-c1d3e05174cc.png)

这一切意味着什么？让我们一步一步地分解这个图表。

# 调查决策树的内部工作方式

我们之前已经确定决策树基本上是一个流程图，它对数据做出一系列决策。这个过程从根节点(位于最顶端的节点)开始，在这里，我们根据某种决策规则将数据分成两组(仅适用于二叉树)。然后，重复该过程，直到所有剩余的样本具有相同的目标标签，此时我们已经到达叶节点。

在前面的垃圾邮件过滤器示例中，通过询问真/假问题来做出决定。例如，我们问一封电子邮件是否包含某个单词。如果是的话，我们沿着标记为真的边问下一个问题。然而，这不仅适用于分类特征，...

# 评定功能的重要性

我还没有告诉你的是你如何选择分割数据的特征。前面的根节点按照 *Na < = 0.72，*拆分数据，但是谁让树先关注钠呢？还有，0.72 这个数字到底是从哪里来的？

显然，有些功能可能比其他功能更重要。事实上，scikit-learn 提供了一个对特征重要性进行评级的功能，这是一个介于每个特征的 *0* 和 *1* 之间的数字，其中 *0* 表示*根本不用于任何决策*和 *1* 表示*完美地预测了目标*。特征重要性被标准化，因此它们的总和为 1:

```py
In [27]: dtc.feature_importances_
Out[27]: array([ 0\.        , 0\.   , 0\.        , 0.13554217, 0.29718876,
                 0.24096386, 0\.   , 0.32630522, 0\.        , 0\. ])
```

如果我们提醒自己功能名称，就会清楚哪个功能似乎是最重要的。一个情节可能最能提供信息:

```py
In [28]: plt.barh(range(10), dtc.feature_importances_, align='center',
...      tick_label=vec.get_feature_names())
```

这将导致以下条形图:

![](Images/448d58c4-1960-44f1-847b-e1e2552d81d9.png)

现在，很明显，知道给患者服用哪种药物的最重要特征实际上是患者的胆固醇水平是否正常。年龄、钠水平和钾水平也很重要。另一方面，性别和血压似乎没有任何区别。然而，这并不意味着性别或血压没有信息。这仅仅意味着决策树没有选择这些特征，可能是因为另一个特征会导致同样的分裂。

但是，坚持住。如果胆固醇水平如此重要，为什么它没有被选为树的第一个特征(即根节点)？为什么会选择先在钠水平上进行拆分？这就是我需要告诉你图中那个不祥的`gini`标签的地方。

Feature importances tell us which features are important for classification, but not which class label they are indicative of. For example, we only know that the cholesterol level is important, but we don't know how that led to different drugs being prescribed. In fact, there might not be a simple relationship between features and classes.

# 理解决策规则

为了构建完美的树，您可能希望在信息量最大的特征处拆分树，从而得到最纯的子节点。然而，这个简单的想法带来了一些实际的挑战:

*   实际上并不清楚什么是最有信息量的。我们需要一个具体的值，一个分数函数，或者一个数学方程来描述一个特征的信息量。
*   为了找到最佳分割，我们必须在每个决策节点搜索所有的可能性。

幸运的是，决策树算法实际上为您完成了这两个步骤。scikit-learn 支持的两个最常用的标准如下:

*   `criterion='gini'`:基尼不纯是一种错误分类的度量，目的是...

# 控制决策树的复杂性

如果你继续种植一棵树，直到所有的叶子都是纯净的，你通常会得到一棵太复杂而无法解释的树。纯叶的存在意味着树在训练数据上是 100%正确的，就像我们前面展示的树一样。因此，该树在测试数据集上的表现很可能很差，就像我们前面展示的树一样。我们说树超过了训练数据。

有两种常见的方法可以避免过度拟合:

*   **预修剪**:这是提前停止树的创建的过程。
*   **后期修剪** **(或只是修剪)**:这是先构建树，然后移除或折叠只包含少量信息的节点的过程。

有几种方法可以预先修剪一棵树，所有这些方法都可以通过向`DecisionTreeClassifier`构造函数传递可选参数来实现:

*   通过`max_depth`参数限制树的最大深度
*   通过`max_leaf_nodes`限制叶节点的最大数量
*   通过`min_samples_split`需要节点中的最小点数来保持分裂

通常预修剪足以控制过度拟合。

试试我们的玩具数据集吧！你能让考试的分数有所提高吗？当您开始使用早期参数时，树布局如何变化？

In more complicated real-world scenarios, pre-pruning is no longer sufficient to control overfitting. In such cases, we want to combine multiple decision trees into what is known as a **random forest**. We will talk about this in [Chapter 10](10.html), *Ensemble Methods for Classification*.

# 利用决策树诊断乳腺癌

现在我们已经构建了第一个决策树，是时候将我们的注意力转向一个真实的数据集了:乳腺癌威斯康星数据集([https://archive . ics . UCI . edu/ml/datasets/乳腺癌+癌症+威斯康星+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) )。

该数据集是医学影像研究的直接结果，被认为是当今的经典。数据集是从健康(良性)和癌性(恶性)组织的数字化图像创建的。不幸的是，我没能从最初的研究中找到任何公共领域的例子，但是图像看起来类似于下面的截图:

![](Images/dafbad1a-c5df-4a4e-b9ec-e0f78e3d247d.png)

这项研究的目的是对组织进行分类...

# 正在加载数据集

完整数据集是 scikit-learn 示例数据集的一部分。我们可以使用以下命令导入它:

1.  首先，让我们使用`load_breast_cancer`函数加载数据集:

```py
In [1]: from sklearn import datasets
...     data = datasets.load_breast_cancer()
```

2.  与前面的示例一样，所有数据都包含在 2D 特征矩阵`data.data`中，其中行代表数据样本，列是特征值:

```py
In [2]: data.data.shape
Out[2]: (569, 30)
```

3.  通过查看提供的功能名称，我们认识到前面提到的一些功能:

```py
In [3]: data.feature_names
Out[3]: array(['mean radius', 'mean texture', 'mean perimeter',
               'mean area', 'mean smoothness', 'mean compactness',
               'mean concavity', 'mean concave points',
               'mean symmetry', 'mean fractal dimension',
               'radius error', 'texture error', 'perimeter error',
               'area error', 'smoothness error',
               'compactness error', 'concavity error',
               'concave points error', 'symmetry error',
               'fractal dimension error', 'worst radius',
               'worst texture', 'worst perimeter', 'worst area',
               'worst smoothness', 'worst compactness',
               'worst concavity', 'worst concave points',
               'worst symmetry', 'worst fractal dimension'], 
              dtype='<U23')
```

4.  由于这是一个二进制分类任务，我们希望找到两个目标名称:

```py
In [4]: data.target_names
Out[4]: array(['malignant', 'benign'], dtype='<U9')
```

5.  让我们保留大约 20%的数据样本用于测试:

```py
In [5]: import sklearn.model_selection as ms
...     X_train, X_test, y_train, y_test =
...     ms.train_test_split(data_pre, target, test_size=0.2,
...     random_state=42)
```

6.  你当然可以选择不同的比例，但大多数人通常使用 70-30、80-20 或 90-10 这样的比例。这完全取决于数据集的大小，但最终应该不会有太大的不同。将数据拆分为 80-20 应该会产生以下集合大小:

```py
In [6]: X_train.shape, X_test.shape
Out[6]: ((455, 30), (114, 30))
```

# 构建决策树

如前所示，我们可以使用 scikit-learn 的`tree`模块创建决策树。现在，我们不要指定任何可选参数:

1.  我们将从创建决策树开始:

```py
In [5]: from sklearn import tree...     dtc = tree.DecisionTreeClassifier()
```

2.  你还记得如何训练决策树吗？我们将为此使用`fit`函数:

```py
In [6]: dtc.fit(X_train, y_train)Out[6]: DecisionTreeClassifier(class_weight=None, criterion='gini',                               max_depth=None, max_features=None,                               max_leaf_nodes=None,                               min_impurity_split=1e-07,                               min_samples_leaf=1,                               min_samples_split=2,                               min_weight_fraction_leaf=0.0,                               presort=False, random_state=None,                               splitter='best')
```

3.  因为我们没有指定任何预修剪参数，所以我们希望这样...

# 使用决策树进行回归

虽然到目前为止，我们一直专注于在分类任务中使用决策树，但是您也可以将它们用于回归。但是您需要再次使用 scikit-learn，因为 OpenCV 不提供这种灵活性。因此，我们在此仅简要回顾其功能:

1.  假设我们想用决策树来拟合一个正弦波。为了让事情变得有趣，我们还将使用 NumPy 的随机数生成器向数据点添加一些噪声:

```py
In [1]: import numpy as np
...     rng = np.random.RandomState(42)
```

2.  然后我们创建 100 个随机间隔的 *x* 值，介于 0 和 5 之间，并计算相应的 sin 值:

```py
In [2]: X = np.sort(5 * rng.rand(100, 1), axis=0)
...     y = np.sin(X).ravel()
```

3.  然后，我们在`y`(使用`y[::2]`)中的每隔一个数据点添加噪声，由`0.5`缩放，因此我们不会引入太多抖动:

```py
In [3]: y[::2] += 0.5 * (0.5 - rng.rand(50))
```

4.  然后，您可以像以前创建任何其他树一样创建一个回归树。

一个小的区别是`gini`和`entropy`的分割标准不适用于回归任务。相反，scikit-learn 提供了两种不同的拆分标准:

5.  使用最小均方误差准则，我们将建立两棵树。让我们首先构建一个深度为 2 的树:

```py
In [4]: from sklearn import tree
In [5]: regr1 = tree.DecisionTreeRegressor(max_depth=2,
...     random_state=42)
...     regr1.fit(X, y)
Out[5]: DecisionTreeRegressor(criterion='mse', max_depth=2,
                              max_features=None, max_leaf_nodes=None,
                              min_impurity_split=1e-07,
                              min_samples_leaf=1, min_samples_split=2,
                              min_weight_fraction_leaf=0.0,
                              presort=False, random_state=42,
                              splitter='best')
```

6.  接下来，我们将构建一个最大深度为 5:

```py
In [6]: regr2 = tree.DecisionTreeRegressor(max_depth=5,
...     random_state=42)
...     regr2.fit(X, y)
Out[6]: DecisionTreeRegressor(criterion='mse', max_depth=5,
                              max_features=None, max_leaf_nodes=None,
                              min_impurity_split=1e-07,
                              min_samples_leaf=1, min_samples_split=2,
                              min_weight_fraction_leaf=0.0,
                              presort=False, random_state=42,
                              splitter='best')
```

然后我们可以像线性回归器一样使用决策树，从[第 3 章](03.html)、*监督学习的第一步*。

7.  为此，我们创建了一个测试集，其中 *x* 值在 0 到 5 的整个范围内密集采样:

```py
In [7]: X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
```

8.  预测的 *y* 值可以通过`predict`方法获得:

```py
In [8]: y_1 = regr1.predict(X_test)
...     y_2 = regr2.predict(X_test)
```

9.  如果我们将所有这些绘制在一起，我们可以看到决策树是如何不同的:

```py
In [9]: import matplotlib.pyplot as plt
... %matplotlib inline
... plt.style.use('ggplot')

... plt.scatter(X, y, c='k', s=50, label='data')
... plt.plot(X_test, y_1, label="max_depth=2", linewidth=5)
... plt.plot(X_test, y_2, label="max_depth=5", linewidth=3)
... plt.xlabel("data")
... plt.ylabel("target")
... plt.legend()
Out[9]: <matplotlib.legend.Legend at 0x12d2ee345f8>
```

这将产生以下图:

![](Images/fb76b100-17ab-494a-9e71-210bfbfdb4a8.png)

这里，粗红线代表深度为 2 的回归树。您可以看到树是如何使用这些粗略的步骤来近似数据的。较细的蓝线属于深度为 5 的回归树；增加的深度允许树进行许多更精细的近似。因此，该树可以更好地逼近数据。然而，由于这种额外的力量，树也更容易拟合噪声值，这尤其可以从图右侧的尖峰中看出。

# 摘要

在本章中，我们学习了决策树的所有知识，以及如何将它们应用于分类和回归任务。我们讨论了数据生成、过拟合以及通过调整修剪前和修剪后设置来避免这种现象的方法。我们还学习了如何使用基尼系数和信息增益等指标来评估节点分裂的质量。最后，我们将决策树应用于医学数据来检测癌组织。我们将在本书的最后回到决策树，届时我们将把多棵树组合成一个随机森林。但是现在，让我们进入一个新的话题。

在下一章中，我们将介绍机器学习领域的另一个主要内容:支持向量机...