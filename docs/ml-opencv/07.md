# 七、使用贝叶斯学习实现垃圾邮件过滤器

在我们开始掌握高级主题(如聚类分析、深度学习和集成模型)之前，让我们将注意力转向一个迄今为止被我们忽略的更简单的模型:朴素贝叶斯分类器。

朴素贝叶斯分类器源于贝叶斯推理，以著名的统计学家和哲学家托马斯·贝叶斯(1701-1761)的名字命名。贝叶斯定理以描述基于可能导致事件的条件的先验知识的事件概率而闻名。我们可以使用贝叶斯定理来建立一个统计模型，该模型不仅可以对数据进行分类，还可以为我们提供对我们的分类正确的可能性的估计。在我们的案例中，我们可以使用贝叶斯推断，以高可信度驳回作为垃圾邮件的电子邮件，并在筛查测试呈阳性的情况下，确定女性患乳腺癌的概率。

我们现在已经在实现机器学习方法的机制方面获得了足够的经验，因此我们不应该再害怕尝试和理解它们背后的理论。别担心，我们不会写一本关于它的书，但我们需要对理论有所了解才能理解模型的内部工作。之后，我相信你会发现贝叶斯分类器易于实现，计算效率高，并且在相对较小的数据集上表现得相当好。在本章中，我们将了解朴素贝叶斯分类器，然后实现我们的第一个贝叶斯分类器。然后，我们将使用朴素贝叶斯分类器对电子邮件进行分类。

在本章中，我们将涵盖以下主题:

*   理解朴素贝叶斯分类器
*   实现你的第一个贝叶斯分类器
*   使用朴素贝叶斯分类器对电子邮件进行分类

# 技术要求

可以从以下链接查阅本章代码:[https://github . com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/chapter 07](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter07)。

以下是软件和硬件要求的总结:

*   您将需要 OpenCV 版本 4.1.x (4.1.0 或 4.1.1 都可以)。
*   您将需要 Python 3.6 版本(任何 Python 3 . x 版本都可以)。
*   您将需要 Anaconda Python 3 来安装 Python 和所需的模块。
*   这本书可以使用任何操作系统——苹果操作系统、视窗操作系统和基于 Linux 的操作系统。我们建议您的系统中至少有 4 GB 内存。
*   你不需要一个图形处理器来运行本书提供的代码。

# 理解贝叶斯推理

虽然贝叶斯分类器实现起来相对简单，但它们背后的理论一开始可能会相当反直觉，尤其是如果你还不太熟悉概率论的话。然而，贝叶斯分类器的美妙之处在于，它们比我们迄今为止遇到的所有分类器都更好地理解底层数据。例如，标准分类器，如*k*-最近邻算法或决策树，可能能够告诉我们从未见过的数据点的目标标签。然而，这些算法不知道他们的预测是对是错的可能性有多大。我们称之为辨别模型。另一方面，贝叶斯模型了解导致数据的潜在概率分布。我们称它们为生成模型，因为它们不只是在现有的数据点上贴标签——它们还可以用相同的统计数据生成新的数据点。

如果这最后一段有点超出你的理解范围，你可能会喜欢下面关于概率论的简介。这对接下来的部分很重要。

# 绕过概率论一小段路

为了理解贝叶斯定理，我们需要掌握以下技术术语:

*   **随机变量**:这是一个值取决于偶然性的变量。一个很好的例子是抛硬币的行为，这可能会出现正面或反面。如果一个随机变量只能取有限数量的值，我们称之为离散(如掷硬币或掷骰子)；否则，我们称之为连续随机变量(如某一天的温度)。随机变量通常用大写字母排版。
*   **概率**:这是衡量一个事件发生的可能性。我们将事件发生的概率 *e* 表示为 *p(e)* ，它必须是 0 和 1 之间的数字(或介于 0 和 1 之间...

# 理解贝叶斯定理

在很多情况下，知道我们的分类器出错的可能性有多大真的很好。例如在[第 5 章](05.html)、*利用决策树进行医学诊断*中，我们训练了一个决策树，根据一些医学测试来诊断女性乳腺癌。你可以想象，在这种情况下，我们会不惜一切代价避免一次误诊；诊断一名患有乳腺癌的健康女性(假阳性)将是令人心碎的，并导致不必要的、昂贵的医疗程序，而错过一名女性的乳腺癌(假阴性)最终可能会让该女性付出生命。

很高兴知道我们可以依靠贝叶斯模型。让我们从[http://yudkowsky.net/rational/bayes](http://yudkowsky.net/rational/bayes)来看一个具体的(也是相当有名的)例子:

"1% of women at age forty who participate in routine screening have breast cancer. 80% of women with breast cancer will get positive mammographies. 9.6% of women without breast cancer will also get positive mammographies. A woman in this age group had a positive mammography in a routine screening. What is the probability that she actually has breast cancer?"

你认为答案是什么？

嗯，考虑到她的乳房 x 光检查是阳性的，你可能会认为她患癌症的概率很高(接近 80%)。这个女人属于 9.6%假阳性的可能性似乎要小得多，所以真正的概率可能在 70%到 80%之间。

恐怕这是不对的。

这是思考这个问题的一种方法。为了简单起见，让我们假设我们看到的是一些具体的病人数量，比如 10，000 人。在乳房 x 光检查之前，10，000 名妇女可以分为两组:

*   **第十组** : 100 名患有乳腺癌的女性
**   **Y 组**:9900 名女性*无*乳腺癌*

 *目前为止，一切顺利。如果我们将两组的数字相加，我们总共得到了 10，000 名患者，这证实了没有人在数学上输了。乳房 x 光检查后，我们可以将 10，000 名妇女分为四组:

*   **第 1 组** : 80 名乳腺癌患者，乳腺钼靶检查阳性
*   **第 2 组** : 20 名乳腺癌患者，乳腺钼靶检查阴性
*   **第 3 组**:约 950 名无乳腺癌且乳腺钼靶检查阳性的女性
*   **第 4 组**:约。8950 名无乳腺癌且乳房 x 光检查阴性的妇女

从前面的分析可以看出，四组之和都是一万。**第 1 组****第 2 组**(有乳腺癌)之和对应**第**组 **X** ，**第 3 组**和**第 4 组**(无乳腺癌)之和对应**第 Y 组**。

当我们把它画出来时，这可能会变得更清楚:

![](Images/bbf90aac-09df-48e2-8fd2-5be6460eccbb.png)

该图中，上半部分对应**X 组**，下半部分对应**Y 组**。类似地，左半部分对应于所有乳房 x 光检查阳性的妇女，右半部分对应于所有乳房 x 光检查阴性的妇女。

现在，更容易看到，我们正在寻找的只涉及图的左半部分。阳性结果的癌症患者在所有阳性结果患者组中的比例是第 1 组在第 1 组和第 3 组中的比例:

*80 / (80 + 950) = 80 / 1,030 = 7.8%*

换句话说，如果你为 10，000 名患者提供乳房 x 光检查，那么在 1030 名乳房 x 光检查阳性的患者中，将有 80 名乳房 x 光检查阳性的患者患有癌症。如果医生问她患乳腺癌的可能性，她应该给一个乳房 x 光检查阳性的病人答案:考虑到 13 个病人问这个问题，大约 13 个人中有 1 个会患癌症。

我们刚才计算的叫做一个**条件概率**:在(我们也说**给**一个阳性乳腺摄影的情况下，我们**对一个女性得乳腺癌**的信任度是多少？如最后一小节，我们用 *p(癌症|乳腺摄影)*或 *p(C|M)* 简称来表示。使用大写字母再次强调了健康和乳房 x 光检查都可能有几种结果的观点，这取决于几种潜在的(也可能是未知的)原因。因此，它们是随机变量。****

然后，我们可以用以下公式表示 *P(C|M)* :

![](Images/b561a41b-448a-49cc-8be0-3772a3e0f27f.png)

这里， *p(C，M)* 表示 *C* 和 *M* 都为真的概率(指女性既有癌症又有乳房 x 光检查阳性的概率)。如前所示，这相当于女性属于第 1 组的概率。

逗号(*、*)表示逻辑*、*，颚化符( *~* )表示逻辑*而非*。因此， *p(~C，M)* 表示 *C* 不为真的概率， *M* 为真的概率(指女性没有癌症但钼靶检查呈阳性的概率)。这相当于一个女性属于第三组的概率。所以，分母基本上加起来就是第 1 组( *p(C，M)* )和第 3 组( *p(~C，M)* )的女性。

但是等等！这两组加在一起只是表示女性乳房 x 光检查阳性的概率， *p(M)* 。因此，我们可以简化前面的等式:

![](Images/55a3d8a8-bb02-4a6f-888c-8f322fab1880.png)

贝叶斯版本是重新解读 *p(C，M)* 的含义。我们可以将 *p(C，M)* 表达如下:

![](Images/98a72194-6ede-4115-af51-7446f71a6a0e.png)

现在有点混乱了。这里， *p(C)* 简单来说就是女性患癌的概率(对应于前面提到的 X 组)。考虑到一名妇女患有癌症，她的乳房 x 光检查呈阳性的概率是多少？从问题题来看，我们知道是 80%。这是 *p(M|C)* ， *M* 给出 *C* 的概率。

用这个新公式代替第一个公式中的 *p(C，M)* ，我们得到如下公式:

![](Images/fc3c16dd-2798-44e8-976d-6a474464d8ac.png)

在贝叶斯世界中，这些术语都有其特定的名称:

*   *p(C|M)* 被称为**后路**，始终是我们要计算的东西。在我们的例子中，这对应于在乳房 x 光检查呈阳性的情况下，认为女性患有乳腺癌的程度。
*   *p(C)* 被称为**先验**，因为它对应于我们关于乳腺癌有多常见的初步知识。我们也称之为我们对 *C* 的最初信仰程度。
*   *p(M|C)* 称为**似**。
*   *p(M)* 称为**证据**。

因此，您可以再次重写等式，如下所示:

![](Images/bf3d09c1-5036-4405-b41b-2fab3f42f49e.png)

大多数情况下，只对该分数的分子感兴趣，因为分母不依赖于 *C，*，所以分母是常数，可以忽略不计。

# 理解朴素贝叶斯分类器

到目前为止，我们只谈了一个证据。然而，在大多数现实场景中，我们必须在多条证据(如随机变量*X<sub>1</sub>T5】和*X<sub>2</sub>T9】的情况下预测一个结果(如随机变量 *Y* )。所以，*不是计算*p(Y | X)*而是经常要计算 *p(Y|X <sub>1</sub> ，X <sub>2</sub> ，...，X <sub>n</sub> )* 。不幸的是，这使得数学非常复杂。对于两个随机变量， *X <sub>1</sub>* 和 *X <sub>2</sub>* ，联合概率的计算如下:*

![](Images/5e0699f8-d90a-44d6-930a-678af88be987.png)

丑陋的部分是术语 *p(X <sub>1</sub> |X <sub>2</sub> ，C)* ，表示 *X <sub>1</sub>* 的条件概率取决于所有其他变量，包括 *C* 。这就扯平了...

# 实现你的第一个贝叶斯分类器

但是算够了，让我们做一些编码吧！

在前一章中，我们学习了如何使用 scikit-learn 生成大量高斯斑点。你记得这是怎么做到的吗？

# 创建玩具数据集

我所指的功能位于 scikit-learn 的`datasets`模块中。让我们创建 100 个数据点，每个数据点属于两个可能的类之一，并将它们分组为两个高斯斑点。为了使实验具有可重复性，我们指定一个整数来为`random_state`挑选种子。你可以再次选择你喜欢的号码。在这里，我选择了托马斯·贝叶斯的出生年份(只是为了好玩):

```py
In [1]: from sklearn import datasets...     X, y = datasets.make_blobs(100, 2, centers=2,        random_state=1701, cluster_std=2)
```

让我们来看看我们刚刚使用我们值得信赖的朋友 Matplotlib 创建的数据集:

```py
In [2]: import matplotlib.pyplot as plt...     plt.style.use('ggplot')...     %matplotlib inlineIn [3]: plt.scatter(X[:, 0], X[:, ...
```

# 用普通贝叶斯分类器对数据进行分类

然后，我们将使用与前面章节相同的过程来训练一个**普通贝叶斯分类器**。等等，为什么不是朴素贝叶斯分类器？事实证明，OpenCV 并没有真正提供一个真正的朴素贝叶斯分类器。相反，它带有贝叶斯分类器，不一定期望特征是独立的，而是期望数据聚集成高斯斑点。这正是我们之前创建的数据集！

通过以下步骤，您将了解如何使用普通贝叶斯分类器构建分类器:

1.  我们可以使用以下函数创建一个新的分类器:

```py
In [5]: import cv2
...     model_norm = cv2.ml.NormalBayesClassifier_create()
```

2.  然后，通过`train`方法进行训练:

```py
In [6]: model_norm.train(X_train, cv2.ml.ROW_SAMPLE, y_train)
Out[6]: True
```

3.  一旦分类器训练成功，它将返回`True`。我们经历了预测和给分类器打分的过程，就像我们以前做过一百万次一样:

```py
In [7]: _, y_pred = model_norm.predict(X_test)
In [8]: from sklearn import metrics
...     metrics.accuracy_score(y_test, y_pred)
Out[8]: 1.0
```

4.  更好的是——我们可以重用上一章的绘图功能来检查决策边界！如果你还记得的话，这个想法是创建一个包含所有数据点的网格，然后对网格上的每个点进行分类。网格是通过同名的 NumPy 函数创建的:

```py
In [9]: def plot_decision_boundary(model, X_test, y_test):
...         # create a mesh to plot in
...         h = 0.02 # step size in mesh
...         x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() +
            1
...         y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() +
            1
...         xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
...                              np.arange(y_min, y_max, h))
```

5.  `meshgrid`函数将返回两个浮点矩阵`xx`和`yy`，它们包含网格上每个坐标点的 *x* 和 *y* 坐标。我们可以使用`ravel`函数将这些矩阵展平为列向量，并将它们堆叠起来形成一个新矩阵`X_hypo`:

```py
...         X_hypo = np.column_stack((xx.ravel().astype(np.float32),
...                                   yy.ravel().astype(np.float32)))
```

6.  `X_hypo`现在包含`X_hypo[:, 0]`中的所有 *x* 值和`X_hypo[:, 1]`中的所有 *y* 值。这是`predict`功能可以理解的格式:

```py
...         ret = model.predict(X_hypo)
```

7.  然而，我们希望能够同时使用 OpenCV 和 scikit-learn 的模型。两者的区别在于 OpenCV 返回多个变量(一个指示成功/失败的布尔标志和预测的目标标签)，而 scikit-learn 只返回预测的目标标签。因此，我们可以检查`ret`输出是否是一个元组，在这种情况下，我们知道我们正在处理 OpenCV。在这种情况下，我们存储元组的第二个元素(`ret[1]`)。否则，我们处理的是 scikit-learn，不需要索引到`ret`:

```py
...         if isinstance(ret, tuple):
...             zz = ret[1]
...         else:
...             zz = ret
...         zz = zz.reshape(xx.shape)
```

8.  剩下要做的就是创建一个等高线图，其中`zz`表示网格上每个点的颜色。除此之外，我们使用可靠的散点图绘制数据点:

```py
...         plt.contourf(xx, yy, zz, cmap=plt.cm.coolwarm, alpha=0.8)
...         plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=200)
```

9.  我们通过传递模型(`model_norm`)、特征矩阵(`X`)和目标标签向量(`y`)来调用函数:

```py
In [10]: plot_decision_boundary(model_norm, X, y)
```

输出如下所示:

![](Images/67d6aaae-c7d9-4b3f-ae04-df35a7fbd1cf.png)

目前为止，一切顺利。有趣的是，贝叶斯分类器还返回每个数据点被分类的概率:

```py
In [11]: ret, y_pred, y_proba = model_norm.predictProb(X_test)
```

该函数返回一个布尔标志(`True`表示成功，`False`表示失败)、预测目标标签(`y_pred`)和条件概率(`y_proba`)。这里，`y_proba`是一个 *N x* 2 矩阵，对于每一个 *N* 数据点，它被分类为 0 类或 1 类的概率为:

```py
In [12]: y_proba.round(2)
Out[12]: array([[ 0.15000001,  0.05      ],
                [ 0.08      ,  0\.        ],
                [ 0\.        ,  0.27000001],
                [ 0\.        ,  0.13      ],
                [ 0\.        ,  0\.        ],
                [ 0.18000001,  1.88      ],
                [ 0\.        ,  0\.        ],
                [ 0\.        ,  1.88      ],
                [ 0\.        ,  0\.        ],
                [ 0\.        ,  0\.        ]], dtype=float32)
```

这意味着，对于第一个数据点(顶行)，其属于 0 类(即 *p(C <sub>0</sub> |X)* )的概率为 0.15(或 15%)。同样，属于 1 类的概率为*p(C<sub>1</sub>| X)*=*0.05。*

The reason why some of the rows show values greater than 1 is that OpenCV does not really return probability values. Probability values are always between 0 and 1, and each row in the preceding matrix should add up to 1\. Instead, what is being reported is a **likelihood**, which is basically the numerator of the conditional probability equation, *p(C)* *p(M|C)*. The denominator, *p*(*M*), does not need to be computed. All we need to know is that *0.15 > 0.05* (top row). Hence, the data point most likely belongs to class 0.

# 用朴素贝叶斯分类器对数据进行分类

以下步骤将帮助您构建朴素贝叶斯分类器:

1.  我们可以通过向 scikit-learn 寻求帮助，将结果与真正的朴素贝叶斯分类器进行比较:

```py
In [13]: from sklearn import naive_bayes...      model_naive = naive_bayes.GaussianNB()
```

2.  像往常一样，通过`fit`方法训练分类器:

```py
In [14]: model_naive.fit(X_train, y_train)Out[14]: GaussianNB(priors=None)
```

3.  分类器的评分内置于:

```py
In [15]: model_naive.score(X_test, y_test)Out[15]: 1.0
```

4.  又是满分！然而，与 OpenCV 相反，这个分类器的`predict_proba`方法返回真实的概率值，因为所有的值都在 0 和 1 之间，并且因为所有的行加起来是 1:

```py
In [16]: yprob = model_naive.predict_proba(X_test) ...
```

# 可视化条件概率

通过参考以下步骤，您将能够可视化条件概率:

1.  为此，我们将稍微修改前面示例中的绘图函数。我们首先在(`x_min`、`x_max`)和(`y_min`、`y_max`)之间创建一个网格:

```py
In [18]: def plot_proba(model, X_test, y_test):
...          # create a mesh to plot in
...          h = 0.02 # step size in mesh
...          x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1
...          y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1
...          xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
...                               np.arange(y_min, y_max, h))
```

2.  然后，我们展平`xx`和`yy`，并按列将它们添加到特征矩阵中，`X_hypo`:

```py

...          X_hypo = np.column_stack((xx.ravel().astype(np.float32),
...                                    yy.ravel().astype(np.float32)))
```

3.  如果我们想让这个函数与 OpenCV 和 scikit-learn 一起工作，我们需要为`predictProb`(在 OpenCV 的情况下)和`predict_proba`(在 scikit-learn 的情况下)实现一个开关。为此，我们检查一下`model`是否有一个叫`predictProb`的方法。如果方法存在，我们可以调用它；否则，我们假设我们面对的是 scikit-learn 的模型:

```py
...          if hasattr(model, 'predictProb'):
...             _, _, y_proba = model.predictProb(X_hypo)
...          else:
...             y_proba = model.predict_proba(X_hypo)
```

4.  就像我们之前看到的`In [16]`一样，`y_proba`将是一个 2D 矩阵，对于每个数据点，包含数据属于 0 类(在`y_proba[:, 0]`中)和 1 类(在`y_proba[:, 1]`中)的概率。将这两个值转换成轮廓函数可以理解的颜色的一种简单方法是简单地取两个概率值的差:

```py
...          zz = y_proba[:, 1] - y_proba[:, 0]
...          zz = zz.reshape(xx.shape)
```

5.  最后一步是将`X_test`绘制为彩色网格顶部的散点图:

```py
... plt.contourf(xx, yy, zz, cmap=plt.cm.coolwarm, alpha=0.8)
... plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=200)
```

6.  现在，我们准备调用函数:

```py
In [19]: plot_proba(model_naive, X, y)
```

结果是这样的:

![](Images/9cb91905-247c-46b9-a208-fadaf2df1d9d.png)

上面的截图显示了朴素贝叶斯分类器的条件概率。

# 使用朴素贝叶斯分类器对电子邮件进行分类

这一章的最终任务将是把我们新获得的技能应用到真正的垃圾邮件过滤器上！该任务涉及使用朴素贝叶斯算法解决二进制分类问题。

朴素贝叶斯分类器实际上是一个非常流行的电子邮件过滤模型。他们的天真很好地适用于文本数据的分析，其中每个特征都是一个单词(或一袋单词**)，并且建立每个单词对其他单词的依赖模型是不可行的。**

 **有很多好的电子邮件数据集，例如:

*   惠普垃圾邮件数据库:[https://archive . ics . UCI . edu/ml/机器学习-数据库/spambase](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/)
*   恩罗-垃圾邮件数据集:[http://www.aueb.gr/users/ion/data/enron-spam...](http://www.aueb.gr/users/ion/data/enron-spam)

# 正在加载数据集

您可以参考以下步骤来加载数据集:

1.  如果你从 GitHub 下载了最新的代码，你会在`notebooks/data/chapter7`目录下找到几个`.zip`文件。这些文件包含原始电子邮件数据(带有“收件人”、“抄送”和“正文”字段)，这些数据要么被归类为垃圾邮件(带有`SPAM = 1`类别标签)，要么不被归类为垃圾邮件(也称为 ham，`HAM = 0`类别标签)。
2.  我们构建了一个名为`sources`的变量，它包含了所有的原始数据文件:

```py
In [1]: HAM = 0
...     SPAM = 1
...     datadir = 'data/chapter7'
...     sources = [
...        ('beck-s.tar.gz', HAM),
...        ('farmer-d.tar.gz', HAM),
...        ('kaminski-v.tar.gz', HAM),
...        ('kitchen-l.tar.gz', HAM),
...        ('lokay-m.tar.gz', HAM),
...        ('williams-w3.tar.gz', HAM),
...        ('BG.tar.gz', SPAM),
...        ('GP.tar.gz', SPAM),
...        ('SH.tar.gz', SPAM)
...     ]
```

3.  第一步是将这些文件提取到子目录中。为此，我们可以使用我们在上一章中编写的`extract_tar`函数:

```py
In [2]: def extract_tar(datafile, extractdir):
...         try:
...             import tarfile
...         except ImportError:
...             raise ImportError("You do not have tarfile installed. "
...                               "Try unzipping the file outside of "
...                               "Python.")
...         tar = tarfile.open(datafile)
...         tar.extractall(path=extractdir)
...         tar.close()
...         print("%s successfully extracted to %s" % (datafile,
...                                                    extractdir))
```

4.  要将该函数应用于源中的所有数据文件，我们需要运行一个循环。`extract_tar`函数需要一个到`.tar.gz`文件的路径，这是我们从`datadir`和`sources`中的一个条目构建的，以及一个将文件提取到的目录(`datadir`)。这将提取所有电子邮件，例如，`data/chapter7/beck-s.tar.gz`到`data/chapter7/beck-s/`子目录:

```py
In [3]: for source, _ in sources:
...         datafile = '%s/%s' % (datadir, source)
...         extract_tar(datafile, datadir)
Out[3]: data/chapter7/beck-s.tar.gz successfully extracted to data/chapter7
        data/chapter7/farmer-d.tar.gz successfully extracted to
            data/chapter7
        data/chapter7/kaminski-v.tar.gz successfully extracted to
            data/chapter7
        data/chapter7/kitchen-l.tar.gz successfully extracted to
            data/chapter7
        data/chapter7/lokay-m.tar.gz successfully extracted to
            data/chapter7
        data/chapter7/williams-w3.tar.gz successfully extracted to
            data/chapter7
        data/chapter7/BG.tar.gz successfully extracted to data/chapter7
        data/chapter7/GP.tar.gz successfully extracted to data/chapter7
        data/chapter7/SH.tar.gz successfully extracted to data/chapter7
```

现在棘手的是。这些子目录中的每一个都包含许多文本文件所在的其他目录。因此，我们需要编写两个函数:

*   `read_single_file(filename)`:这是一个从名为`filename`的单个文件中提取相关内容的函数。
*   `read_files(path)`:这是一个从一个名为`path`的特定目录下的所有文件中提取相关内容的功能。

要从单个文件中提取相关内容，我们需要了解每个文件的结构。我们唯一知道的是，电子邮件的标题部分(发件人:，收件人:，和抄送:)和正文由一个换行符`'\n'`隔开。因此，我们可以做的是迭代文本文件中的每一行，只保留那些属于主文本主体的行，这些行将存储在变量行中。我们还想在周围保留一个布尔标志`past_header`，它最初被设置为`False`，但是当我们通过标题部分时，它将被翻转到`True`:

1.  我们从初始化这两个变量开始:

```py
In [4]: import os
...     def read_single_file(filename):
...         past_header, lines = False, []
```

2.  然后，我们检查名称为`filename`的文件是否存在。如果有，我们就开始一行一行地循环:

```py
...         if os.path.isfile(filename):
...             f = open(filename, encoding="latin-1")
...             for line in f:
```

你可能已经注意到了`encoding="latin-1"`部分。由于某些电子邮件不是 Unicode 格式，这是为了正确解码文件。

我们不想保留标题信息，所以我们一直循环，直到遇到`'\n'`字符，此时我们将`past_header`从`False`翻转到`True`。

3.  此时，满足以下`if-else`子句的第一个条件，我们将文本文件中剩余的所有行追加到`lines`变量中:

```py
...                 if past_header:
...                     lines.append(line)
...                 elif line == '\n':
...                     past_header = True
...             f.close()
```

4.  最后，我们将所有行连接成一个字符串，用换行符分隔，并返回文件的完整路径和文件的实际内容:

```py
...         content = '\n'.join(lines)
...         return filename, content
```

5.  第二个功能的工作是循环一个文件夹中的所有文件，并对它们调用`read_single_file`:

```py
In [5]: def read_files(path):
...         for root, dirnames, filenames in os.walk(path):
...             for filename in filenames:
...                 filepath = os.path.join(root, filename)
...                 yield read_single_file(filepath)
```

这里`yield`是一个类似于`return`的关键词。不同的是`yield`返回一个生成器，而不是实际值，如果您期望有大量的项目要迭代，这是可取的。

# 用熊猫建立数据矩阵

现在，是时候介绍 Python Anaconda 预装的另一个基本数据科学工具了:**熊猫**。pandas 建立在 NumPy 之上，提供了几种有用的工具和方法来处理 Python 中的数据结构。就像我们一般用别名`np`导入 NumPy 一样，用`pd`别名导入熊猫也很常见:

```py
In [6]: import pandas as pd
```

熊猫提供了一个有用的数据结构，称为数据帧，可以理解为 2D NumPy 数组的推广，如下所示:

```py
In [7]: pd.DataFrame({...         'model': [...             'Normal Bayes',...             'Multinomial Bayes',...             'Bernoulli Bayes'...         ],...         'class': [...             'cv2.ml.NormalBayesClassifier_create()',...             'sklearn.naive_bayes.MultinomialNB()',... 'sklearn.naive_bayes.BernoulliNB()' ...
```

# 预处理数据

Scikit-learn 在编码文本特征时提供了几个选项，我们在[第 4 章](04.html)、*表示数据和工程特征*中讨论过。大家可能还记得，编码文本数据最简单的方法之一就是**字数**；对于每个短语，你要计算每个单词在其中出现的次数。在 scikit-learn 中，使用`CountVectorizer`可以轻松完成此操作:

```py
In [10]: from sklearn import feature_extraction
...      counts = feature_extraction.text.CountVectorizer()
...      X = counts.fit_transform(data['text'].values)
...      X.shape
Out[10]: (52076, 643270)
```

结果是一个巨大的矩阵，它告诉我们，我们总共收集了 52，076 封电子邮件，总共包含 643，270 个不同的单词。然而，scikit-learn 很聪明，它将数据保存在稀疏矩阵中:

```py
In [11]: X
Out[11]: <52076x643270 sparse matrix of type '<class 'numpy.int64'>'
                 with 8607632 stored elements in Compressed Sparse Row 
                 format>
```

为了构建目标标签向量(`y`)，我们需要访问熊猫数据框中的数据。这可以通过将数据框视为字典来实现，其中`values`属性将为我们提供对底层 NumPy 数组的访问:

```py
In [12]: y = data['class'].values
```

# 训练一个正常的贝叶斯分类器

从现在开始，事情(几乎)像往常一样。我们可以使用 scikit-learn 将数据分成训练集和测试集(让我们保留 20%的数据点用于测试):

```py
In [13]: from sklearn import model_selection as ms...      X_train, X_test, y_train, y_test = ms.train_test_split(...          X, y, test_size=0.2, random_state=42...      )
```

我们可以用 OpenCV 实例化一个新的普通贝叶斯分类器:

```py
In [14]: import cv2...      model_norm = cv2.ml.NormalBayesClassifier_create()
```

然而，OpenCV 不知道稀疏矩阵(至少它的 Python 接口不知道)。如果我们像前面一样将`X_train`和`y_train`传递给`train`函数，OpenCV 会抱怨数据矩阵不是 NumPy 数组。...

# 在整个数据集上进行训练

但是，如果您想要对整个数据集进行分类，我们需要一种更复杂的方法。我们转向 scikit-learn 的朴素贝叶斯分类器，因为它了解如何处理稀疏矩阵。事实上，如果你之前没有像对待每一个 NumPy 数组一样去关注和对待`X_train`，你甚至可能不会注意到有什么不同:

```py
In [17]: from sklearn import naive_bayes
...      model_naive = naive_bayes.MultinomialNB()
...      model_naive.fit(X_train, y_train)
Out[17]: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
```

这里，我们使用了`naive_bayes`模块中的`MultinomialNB`，这是最适合处理分类数据(如字数)的朴素贝叶斯分类器版本。

分类器几乎立即被训练，并返回训练集和测试集的分数:

```py
In [18]: model_naive.score(X_train, y_train)
Out[18]: 0.95086413826212191
In [19]: model_naive.score(X_test, y_test)
Out[19]: 0.94422043010752688
```

这就是我们的答案:测试集的准确率为 94.4%！除了使用缺省值之外没有做太多事情，这很好，不是吗？

然而，如果我们对自己的工作超级挑剔，并想进一步提高结果呢？我们可以做几件事。

# 使用 n 克来提高结果

一件事就是用*n*-克数代替普通字数。到目前为止，我们一直依赖于所谓的单词包:我们只是把一封电子邮件的每个单词都扔进一个包里，并计算它出现的次数。然而，在真实的电子邮件中，单词出现的顺序可以携带大量信息！

这正是 *n* 克数想要传达的信息。你可以把一个 *n* 克想象成一个短语，它有 *n* 个单词长。比如短语*统计有其矩*包含以下 1 克:*统计有*、*有*、*有其*、*矩*。还有以下 2 克:*统计有*、*有其*、*有其瞬间*。它还有两个 3 克(*统计有它的*和*有它的时刻*)并且只有一个...***