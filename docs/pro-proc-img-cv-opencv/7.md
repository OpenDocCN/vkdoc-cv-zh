# 7.特征检测和匹配

本章用更复杂的跟踪方法继续上一章的运动探索。在上一章中，您比较和分析了帧之间的整个图像，以识别运动信息。结果，从这些方法跟踪的运动细节是通用的，没有利用图像中的特定结构元素。在本章中，您将首先研究如何定位感兴趣的检测点。它们的通用术语是特征点。然后，您将尝试跟踪这些特征点如何在帧之间移动。这些功能主要在 OpenCV 的`features2d`模块中提供。除了特征点，您还将探索如何使用`objdetect`模块检测面部特征和人物。以下是本章涵盖的主题:

*   角点检测
*   稀疏光流
*   特征检测
*   特征匹配
*   人脸检测
*   人物检测

## 角点检测

在前面的章节中，您已经了解到，在`imgproc`模块中，`Canny()`功能可以有效地检测数字图像中的边缘。在本章中，您将进一步检测数字图像中的角点。这个概念就像边缘检测。如图 [7-1](#Fig1) 所示，角点是那些在不同方向上颜色发生显著变化的像素。

![A436326_1_En_7_Fig1_HTML.jpg](A436326_1_En_7_Fig1_HTML.jpg)

图 7-1。

Corner detection

第一个练习`Chapter07_01`，演示了由 Chris Harris 和 Mike Stephens 在 1988 年创建的 Harris 角点检测方法。为了加快执行速度，在本练习中，您将按比例因子 10 缩小原始网络摄像头图像。检测到角点后，将结果矩阵归一化为 8 位分辨率，并遍历它以识别值高于阈值的角点像素。

```
// Harris corner detection
import processing.video.*;
import org.opencv.core.*;
import org.opencv.imgproc.Imgproc;

// Threshold value for a corner
final int THRESH = 140;
Capture cap;
CVImage img;
// Scale down the image for detection.
float scaling;
int w, h;

void setup() {
  size(640, 480);
  background(0);
  scaling = 10;
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width, height);
  cap.start();
  w = floor(cap.width/scaling);
  h = floor(cap.height/scaling);
  img = new CVImage(w, h);
  smooth();
}

void draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  Mat grey = img.getGrey();
  // Output matrix of corner information
  Mat corners = Mat.zeros(grey.size(), CvType.CV_32FC1);
  Imgproc.cornerHarris(grey, corners, 2, 3, 0.04,
    Core.BORDER_DEFAULT);
  // Normalize the corner information matrix.
  Mat cor_norm = Mat.zeros(grey.size(), CvType.CV_8UC1);
  Core.normalize(corners, cor_norm, 0, 255,
    Core.NORM_MINMAX, CvType.CV_8UC1);
  image(cap, 0, 0);
  pushStyle();
  noFill();
  stroke(255, 0, 0);
  strokeWeight(2);
  // Draw each corner with value greater than threshold.
  for (int y=0; y<cor_norm.rows(); y++) {
    for (int x=0; x<cor_norm.cols(); x++) {
      if (cor_norm.get(y, x)[0] < THRESH)
        continue;
      ellipse(x*scaling, y*scaling, 10, 10);
    }
  }
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
  grey.release();
  corners.release();
  cor_norm.release();
}

```

主要功能是来自`imgproc`模块的`cornerHarris()`功能。第一个参数是输入灰度图像，`grey`。第二个参数是输出矩阵`corners`，它表示每个像素成为角点的可能性。其余参数的技术解释超出了本书的范围。有兴趣可以在 [`http://docs.opencv.org/3.1.0/d4/d7d/tutorial_harris_detector.html`](http://docs.opencv.org/3.1.0/d4/d7d/tutorial_harris_detector.html) 找到 OpenCV 官方教程。第三个参数是用于计算梯度(像素强度的变化)的 2×2 邻域大小。第四个参数是 Sobel 导数的 3×3 孔径大小，如 OpenCV 文档中的 [`http://docs.opencv.org/3.1.0/d2/d2c/tutorial_sobel_derivatives.html`](http://docs.opencv.org/3.1.0/d2/d2c/tutorial_sobel_derivatives.html) 所示。第五个参数是 Harris 检测器参数，如前面提到的 Harris 检测器教程所示，最后一个参数是边框类型指示器。图 [7-2](#Fig2) 显示了程序的运行示例。

![A436326_1_En_7_Fig2_HTML.jpg](A436326_1_En_7_Fig2_HTML.jpg)

图 7-2。

Harris corner detection

## 稀疏光流

您在第 [6](6.html) 章中学习了如何使用密集光流功能。在本节中，我将解释如何使用稀疏光流进行运动检测。在密集光流中，您检查并跟踪缩减像素采样图像中的所有像素，而在稀疏光流中，您只检查选定数量的像素。这些是您感兴趣跟踪的点，称为特征点。一般来说，它们是角点。以下是您需要遵循的步骤:

1.  识别特征点。
2.  提高分的准确性。
3.  计算这些点的光流。
4.  可视化流程信息。

### 识别特征点

下一个练习`Chapter07_02`，将使用时剑波和卡洛·托马西在 1994 年开发的函数`goodFeaturesToTrack()`。该函数返回数字图像中最突出的角点。

```
// Feature points detection
import processing.video.*;
import org.opencv.core.*;
import org.opencv.imgproc.Imgproc;

Capture cap;
CVImage img;

void setup() {
  size(1280, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width/2, height);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  smooth();
}

void draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  Mat grey = img.getGrey();
  MatOfPoint corners = new MatOfPoint();
  // Identify the good feature points.
  Imgproc.goodFeaturesToTrack(grey, corners,
    100, 0.01, 10);
  Point [] points = corners.toArray();
  pushStyle();
  noStroke();
  // Draw each feature point according to its
  // original color of the pixel.
  for (Point p : points) {
    int x = (int)constrain((float)p.x, 0, cap.width-1);
    int y = (int)constrain((float)p.y, 0, cap.height-1);
    color c = cap.pixels[y*cap.width+x];
    fill(c);
    ellipse(x+cap.width, y, 10, 10);
  }
  image(img, 0, 0);
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
  grey.release();
  corners.release();
}

```

在`draw()`函数中，获得灰度图像后，将它传递给`goodFeaturesToTrack()`函数。它将返回名为`corners`的`MatOfPoint`变量中的特征点信息。剩余的三个参数是检测的点的最大数量、检测的质量水平和每个特征点之间的最小距离。将`corners`变量转换成名为`points`的`Point`数组后，循环遍历它，用从原始视频捕获图像中提取的颜色将每个角绘制成一个圆。图 [7-3](#Fig3) 显示了该程序的示例截图。

![A436326_1_En_7_Fig3_HTML.jpg](A436326_1_En_7_Fig3_HTML.jpg)

图 7-3。

Good features to track

### 提高准确性

获得特征点列表后，可以使用 OpenCV 函数来提高点的位置精度。即使您正在处理像素位于整数位置的数字图像，拐角也可能出现在两个相邻像素之间的位置。也就是在子像素位置。以下练习`Chapter07_03`探究了此函数`cornerSubPix()`，以提高角点位置的准确性:

```
// Feature points detection with subpixel accuracy
import processing.video.*;
import org.opencv.core.*;
import org.opencv.imgproc.Imgproc;

Capture cap;
CVImage img;
TermCriteria term;
int w, h;
float xRatio, yRatio;

void setup() {
  size(800, 600);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  w = 640;
  h = 480;
  xRatio = (float)width/w;
  yRatio = (float)height/h;
  cap = new Capture(this, w, h);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  term = new TermCriteria(TermCriteria.COUNT | TermCriteria.EPS,
    20, 0.03);
  smooth();
}

void

draw() {
  if (!cap.available())
    return;
  background(200);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  Mat grey = img.getGrey();
  MatOfPoint corners = new MatOfPoint();
  // Detect the initial feature points.
  Imgproc.goodFeaturesToTrack(grey, corners,
    100, 0.01, 10);
  MatOfPoint2f c2 = new MatOfPoint2f(corners.toArray());
  Imgproc.cornerSubPix(grey, c2,
    new Size(5, 5),
    new Size(-1, -1), term);
  Point [] points = corners.toArray();
  pushStyle();
  noFill();
  stroke(100);
  // Display the original points.
  for (Point p : points) {
    ellipse((float)p.x*xRatio, (float)p.y*yRatio, 20, 20);
  }
  points = c2.toArray();
  stroke(0);
  // Display the more accurate points.
  for (Point p : points) {
    ellipse((float)p.x*xRatio, (float)p.y*yRatio, 20, 20);
  }
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
  grey.release();
  corners.release();
  c2.release();
}

```

在程序中，您使用较大的草图画布尺寸和较小的视频捕获尺寸来显示旧(像素级)和新(子像素级)角位置之间的差异。在`draw()`函数中，在`goodFeaturesToTrack()`函数之后，你得到一个名为`corners`的`MatOfPoint`变量中的特征点列表。新函数`cornerSubPix()`将使用相同的输入，即`grey`图像和`corners`矩阵。角点将用作输入和输出，以亚像素精度存储新的特征点。为了提高精度，输入角必须采用新的浮点格式`MatOfPoint2f`。对于`cornerSubPix()`函数，第三个参数`Size(5, 5)`是搜索窗口大小的一半。第四个是`Size(-1, -1)`，是搜索窗口中没有搜索的区域的一半大小。负值表示没有这样的区域。最后一个`term`，是迭代过程的终止标准。它确定迭代过程，例如`cornerSubPix()`何时结束，或者达到最大计数 20，或者达到 0.03 像素的期望精度。在本例中，您在`setup()`函数中将其指定为最大计数 20，所需精度为 0.03 像素。图 [7-4](#Fig4) 是运行程序的截图。灰色圆圈表示像素级拐角，而黑色圆圈表示子像素级拐角。

![A436326_1_En_7_Fig4_HTML.jpg](A436326_1_En_7_Fig4_HTML.jpg)

图 7-4。

Subpixel accuracy feature points

### 计算光流

当你有了特征点的准确位置后，下一个程序`Chapter07_04`将会跟踪这些特征点的流向。主要函数是 OpenCV 的`video`模块中的`calcOpticalFlowPyrLK()`。它是基于 Jean-Yves Bouguet 在 2000 年发表的论文“Lucas Kanade 特征跟踪器的金字塔式实现”的实现。

```
// Sparse optical flow
import processing.video.*;
import org.opencv.core.*;
import org.opencv.video.Video;
import org.opencv.imgproc.Imgproc;

final int CNT = 2;
// Threshold to recalculate the feature points
final int MIN_PTS = 20;
// Number of points to track
final int TRACK_PTS = 150;

Capture

cap;
CVImage img;
TermCriteria term;
// Keep the old and new frames in greyscale.
Mat [] grey;
// Keep the old and new feature points.
MatOfPoint2f [] points;
// Keep the last index of the buffer.
int last;
// First run of the program
boolean first;

void setup() {
  size(1280, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width/2, height);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  term = new TermCriteria(TermCriteria.COUNT | TermCriteria.EPS,
    20, 0.03);
  // Initialize the image and keypoint buffers.
  grey = new Mat[CNT];
  points = new MatOfPoint2f[CNT];
  for (int i=0; i<CNT; i++) {
    grey[i] = Mat.zeros(cap.height, cap.width, CvType.CV_8UC1);
    points[i] = new MatOfPoint2f();
  }
  last = 0;
  first = true;
  smooth();
}

void draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  if (first) {
    // Initialize feature points in first run.
    findFeatures(img.getGrey());
    first = false;
    return

;
  }
  int idx1, idx2;
  idx1 = last;
  idx2 = (idx1 + 1) % grey.length;
  last = idx2;
  grey[idx2] = img.getGrey();
  // Keep status and error of running the
  // optical flow function.
  MatOfByte status = new MatOfByte();
  MatOfFloat err = new MatOfFloat();
  Video.calcOpticalFlowPyrLK(grey[idx1], grey[idx2],
    points[idx1], points[idx2], status, err);
  Point [] pts = points[idx2].toArray();
  byte [] statArr = status.toArray();
  pushStyle();
  noStroke();
  int count = 0;
  for (int i=0; i<pts.length; i++) {
    // Skip error cases.
    if (statArr[i] == 0)
      continue;
    int x = (int)constrain((float)pts[i].x, 0, cap.width-1);
    int y = (int)constrain((float)pts[i].y, 0, cap.height-1);
    color c = cap.pixels[y*cap.width+x];
    fill(c);
    ellipse(x+cap.width, y, 10, 10);
    count++;
  }
  // Re-initialize feature points when valid points
  // drop down to the threshold.
  if (count < MIN_PTS)
    findFeatures(img.getGrey());
  image(img, 0, 0);
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
  status.release();
  err.release();
}

void

findFeatures(Mat g) {
  // Find feature points given the greyscale image g.
  int idx1, idx2;
  idx1 = last;
  idx2 = (idx1 + 1) % grey.length;
  last = idx2;
  grey[idx2] = g;
  MatOfPoint pt = new MatOfPoint();
  // Calculate feature points at pixel level.
  Imgproc.goodFeaturesToTrack(grey[idx2], pt,
    TRACK_PTS, 0.01, 10);
  points[idx2] = new MatOfPoint2f(pt.toArray());
  // Recalculate feature points at subpixel level.
  Imgproc.cornerSubPix(grey[idx2], points[idx2],
    new Size(10, 10),
    new Size(-1, -1), term);
  grey[idx2].copyTo(grey[idx1]);
  points[idx2].copyTo(points[idx1]);
  pt.release();
}

void keyPressed() {
  if (keyCode == 32) {
    // Press SPACE to initialize feature points.
    findFeatures(img.getGrey());
  }
}

```

关于数据结构，程序在名为`grey`的数组变量中保存了两个连续的灰度帧。它还需要在称为`points`的`MatOfPoint2f`数组中保存两个连续的特征点列表。您使用整数变量`last`来跟踪数组中哪个索引是最后一个图像帧数据。`boolean`变量`first`表示是否第一次运行`draw()`循环。在第一次运行的情况下，它将通过调用`findFeatures()`找到特征点，并更新前一帧和当前帧信息。功能`findFeatures()`与您在之前的练习`Chapter07_03`中所做的相同。

在`draw()`函数中，将索引`idx1`更新到最后一帧，将`idx2`更新到当前帧。更新后，使用主函数`Video.calcOpticalFlowPyrLK()`计算上一帧和当前帧之间的光流信息。函数的四个输入参数是前一帧、当前帧、前一帧特征点和当前帧特征点。该函数有两个输出。第一个是`MatOfByte`变量`status`，当找到相应的流时返回 1，否则返回 0。第二个输出是当前练习中未使用的误差度量。然后,`for`循环将遍历所有有效的流程，并在当前帧特征点绘制小圆圈。该程序还对有效的流数据进行计数，如果该数目低于阈值`MIN_PTS`，它将启动`findFeatures()`功能来重新计算当前视频图像的特征点。图 [7-5](#Fig5) 是程序的样例截图。

![A436326_1_En_7_Fig5_HTML.jpg](A436326_1_En_7_Fig5_HTML.jpg)

图 7-5。

Optical flow visualization

### 可视化流程信息

代替在屏幕上绘制当前的特征点，您可以生成更有创造性的光流信息的可视化。下一个例子，`Chapter07_05`，是一个流信息的交互动画。逻辑很简单。将每对特征点从前一个位置连接到其当前位置。

```
// Optical flow animation

import processing.video.*;
import org.opencv.core.*;
import org.opencv.video.Video;
import org.opencv.imgproc.Imgproc;

final int CNT = 2;
final int TRACK_PTS = 200;
final int MAX_DIST = 100;

Capture cap;
CVImage img;
TermCriteria term;
// Keep two consecutive frames and feature
// points list.
Mat [] grey;
MatOfPoint2f [] points;
int last;
boolean first;

void

setup() {
  size(1280, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width/2, height);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  term = new TermCriteria(TermCriteria.COUNT | TermCriteria.EPS,
    20, 0.03);
  grey = new Mat[CNT];
  points = new MatOfPoint2f[CNT];
  for (int i=0; i<CNT; i++) {
    grey[i] = Mat.zeros(cap.height, cap.width, CvType.CV_8UC1);
    points[i] = new MatOfPoint2f();
  }
  last = 0;
  first = true;
  smooth();
}

void draw() {
  if (!cap.available())
    return;
  fillBack();
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();

  if (first) {
    findFeatures(img.getGrey());
    first = false;
    return;
  }
  int idx1, idx2;
  idx1 = last;
  idx2 = (idx1 + 1) % grey.length;
  last = idx2;
  grey[idx2] = img.getGrey();
  MatOfByte status = new MatOfByte();
  MatOfFloat err = new MatOfFloat();
  Video.calcOpticalFlowPyrLK(grey[idx1], grey[idx2],
    points[idx1], points[idx2], status, err);
  // pt1 - last feature points list
  // pt2 - current feature points list
  Point [] pt1 = points[idx1].toArray();
  Point [] pt2 = points[idx2].toArray();
  byte [] statArr = status.toArray();
  PVector p1 = new PVector(0, 0);
  PVector p2 = new PVector(0, 0);
  pushStyle();
  stroke(255, 200);
  noFill();
  for (int i=0; i<pt2.length; i++) {
    if (statArr[i] == 0)
      continue;
    // Constrain the points inside the video frame.
    p1.x = (int)constrain((float)pt1[i].x, 0, cap.width-1);
    p1.y = (int)constrain((float)pt1[i].y, 0, cap.height-1);
    p2.x = (int)constrain((float)pt2[i].x, 0, cap.width-1);
    p2.y = (int)constrain((float)pt2[i].y, 0, cap.height-1);
    // Discard the flow with great distance.
    if (p1.dist(p2) > MAX_DIST)
      continue;
    line(p1.x+cap.width, p1.y, p2.x+cap.width, p2.y);
  }
  // Find

new feature points for each frame.
  findFeatures(img.getGrey());
  image(img, 0, 0);
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
  status.release();
  err.release();
}

void findFeatures(Mat g) {
  grey[last] = g;
  MatOfPoint pt = new MatOfPoint();
  Imgproc.goodFeaturesToTrack(grey[last], pt,
    TRACK_PTS, 0.01, 10);
  points[last] = new MatOfPoint2f(pt.toArray());
  Imgproc.cornerSubPix(grey[last], points[last],
    new Size(5, 5),
    new Size(-1, -1), term);
  pt.release();
}

void fillBack() {
  // Set background color with transparency.
  pushStyle();
  noStroke();
  fill(0, 0, 0, 80);
  rect(cap.width, 0, cap.width, cap.height);
  popStyle();
}

```

要创建运动模糊效果，不要将背景颜色完全清除为黑色。在`fillBack()`功能中，你用一个半透明的矩形填充背景来创建线条的运动轨迹。图 [7-6](#Fig6) 显示了动画的截图。

![A436326_1_En_7_Fig6_HTML.jpg](A436326_1_En_7_Fig6_HTML.jpg)

图 7-6。

Optical flow animation

在创造性编码中，你经常没有正确和明确的答案。在大多数情况下，你只是不停地问“如果呢？”问题。从前面的练习开始，你可以问，如果你不把屏幕清成黑色会怎么样？如果你从视频图像中提取线条的颜色会怎么样？如果使用不同的笔画粗细会怎样？下一个练习`Chapter07_06`，通过将流动动画积累成一种手势绘画的形式来说明这些想法。你可以很容易地将这些效果与杰森·布拉克等画家的动作绘画联系起来。

```
// Optical flow drawing

import processing.video.*;
import org.opencv.core.*;
import org.opencv.video.Video;
import org.opencv.imgproc.Imgproc;

final int CNT = 2;
final int TRACK_PTS = 150;
final int MAX_DIST = 100;

Capture

cap;
CVImage img;
TermCriteria term;
Mat [] grey;
MatOfPoint2f [] points;
int last;
boolean first;

void setup() {
  size(1280, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width/2, height);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  term = new TermCriteria(TermCriteria.COUNT | TermCriteria.EPS,
    20, 0.03);
  // Initialize the buffers for the 2 images and 2 keypoint lists.
  grey = new Mat[CNT];
  points = new MatOfPoint2f[CNT];
  for (int i=0; i<CNT; i++) {
    grey[i] = Mat.zeros(cap.height, cap.width, CvType.CV_8UC1);
    points[i] = new MatOfPoint2f();
  }
  last = 0;
  first = true;
  smooth();
}

void draw() {
  // Note that we do not clear the background.
  if (!cap.available())
    return;
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();

  if (first) {
    findFeatures(img.getGrey());
    first = false;
    return

;
  }
  int idx1, idx2;
  idx1 = last;
  idx2 = (idx1 + 1) % grey.length;
  last = idx2;
  grey[idx2] = img.getGrey();
  MatOfByte status = new MatOfByte();
  MatOfFloat err = new MatOfFloat();
  Video.calcOpticalFlowPyrLK(grey[idx1], grey[idx2],
    points[idx1], points[idx2], status, err);
  Point [] pt2 = points[idx2].toArray();
  Point [] pt1 = points[idx1].toArray();
  byte [] statArr = status.toArray();
  PVector p1 = new PVector(0, 0);
  PVector p2 = new PVector(0, 0);
  pushStyle();
  noFill();
  for (int i=0; i<pt2.length; i++) {
    if (statArr[i] == 0)
      continue;
    p1.x = (int)constrain((float)pt1[i].x, 0, cap.width-1);
    p1.y = (int)constrain((float)pt1[i].y, 0, cap.height-1);
    p2.x = (int)constrain((float)pt2[i].x, 0, cap.width-1);
    p2.y = (int)constrain((float)pt2[i].y, 0, cap.height-1);
    if (p1.dist(p2) > MAX_DIST)
      continue;
    color c = cap.pixels[(int)p2.y*cap.width+(int)p2.x];
    stroke(red(c), green(c), blue(c), (int)random(100, 160));
    strokeWeight(random(3, 6));
    line(p1.x+cap.width, p1.y, p2.x+cap.width, p2.y);
    c = cap.pixels[(int)p1.y*cap.width+(int)p1.x];
    stroke(red(c), green(c), blue(c), (int)random(120, 240));
    strokeWeight(random(1, 4));
    line(p1.x+cap.width, p1.y, p2.x+cap.width, p2.y);
  }
  findFeatures(img.getGrey());
  image(img, 0, 0);
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
  status.release();
  err.release();
}

void

findFeatures(Mat g) {
  // Re-initialize the feature points.
  grey[last] = g;
  MatOfPoint pt = new MatOfPoint();
  Imgproc.goodFeaturesToTrack(grey[last], pt,
    TRACK_PTS, 0.01, 10);
  points[last] = new MatOfPoint2f(pt.toArray());
  Imgproc.cornerSubPix(grey[last], points[last],
    new Size(10, 10),
    new Size(-1, -1), term);
  pt.release();
}

```

这个程序类似于上一个，除了你没有清除背景。在绘制流数据的`for`循环中，首先从实时视频图像中选取颜色，然后绘制两条线而不是一条线。第一条线比较粗，颜色比较透明。第二条线更细，更不透明。它创造了一种更有绘画感的效果。图 [7-7](#Fig7) 包含两张光流绘制的效果图截图。我的作品时间运动，第一部分( [`http://www.magicandlove.com/blog/artworks/movement-in-time-v-1/`](http://www.magicandlove.com/blog/artworks/movement-in-time-v-1/) )是一个使用稀疏光流从经典好莱坞电影序列中生成手势绘画的例子。

![A436326_1_En_7_Fig7_HTML.jpg](A436326_1_En_7_Fig7_HTML.jpg)

图 7-7。

Optical flow drawing

## 特征检测

在前面的章节中，您尝试通过使用 Harris 角点方法和带有 Shi 和 Tomasi 方法的`goodFeaturesToTrack()`函数来定位关键特征点。OpenCV 为您提供了通用的关键点处理来检测、描述它们，并在连续的帧之间进行匹配。在本节中，您将首先学习如何使用`features2d`模块中的`FeatureDetector`类来识别关键点。下一个练习`Chapter07_07`将演示该类的基本操作:

```
// Features detection
import processing.video.*;
import org.opencv.core.*;
import org.opencv.imgproc.Imgproc;
import org.opencv.features2d.FeatureDetector;

final float MIN_RESP = 0.003;
Capture cap;
CVImage img;
FeatureDetector fd;

void setup() {
  size(1280, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width/2, height);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  // Create the instance of the class.
  fd = FeatureDetector.create(FeatureDetector.ORB);
  smooth();
}

void

draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  Mat grey = img.getGrey();
  MatOfKeyPoint pt = new MatOfKeyPoint();
  // Detect keypoints from the image.
  fd.detect(grey, pt);
  image(cap, 0, 0);
  CVImage out = new CVImage(cap.width, cap.height);
  out.copyTo(grey);
  tint(255, 100);
  image(out, cap.width, 0);
  noTint();
  pushStyle();
  noFill();
  stroke(255, 200, 0);
  KeyPoint [] kps = pt.toArray();
  for (KeyPoint kp : kps) {
    // Skip the keypoints that are less likely.
    if (kp.response < MIN_RESP)
      continue;
    float x1 = (float)kp.pt.x;
    float y1 = (float)kp.pt.y;
    float x2 = x1 + kp.size*cos(radians(kp.angle))/2;
    float y2 = y1 + kp.size*sin(radians(kp.angle))/2;
    // size is the diameter of neighborhood.
    ellipse(x1+cap.width, y1, kp.size, kp.size);
    // Draw also the orientation direction.
    line(x1+cap.width, y1, x2+cap.width, y2);
  }
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
  grey.release();
  pt.release();
}

```

您使用`FeatureDetector`类实例`fd`来处理主要任务。在`setup()`函数中，您用`FeatureDetector.create()`函数创建了实例`fd`。该参数指示您使用的检测器类型。在 OpenCV 3.1 的 Java 版本中，有以下类型:

*   `AKAZE`，`DYNAMIC_AKAZE`，`GRID_AKAZE`，`PYRAMID_AKAZE`，
*   `BRISK`，`DYNAMIC_BRISK`，`GRID_BRISK`，`PYRAMID_BRISK`，
*   `FAST`，`DYNAMIC_FAST`，`GRID_FAST`，`PYRAMID_FAST`，
*   `GFTT`，`DYNAMIC_GFTT`，`GRID_GFTT`，`PYRAMID`，`GFTT`，
*   `HARRIS`，`DYNAMIC_HARRIS`，`GRID_HARRIS`，`PYRAMID_HARRIS`，
*   `MSER`，`DYNAMIC_MSER`，`GRID_MSER`，`PYRAMID_MSER`，
*   `ORB`，`DYNAMIC_ORB`，`GRID_ORB`，`PYRAMID_ORB`，
*   `SIMPLEBLOB`、`DYNAMIC_SIMPLEBLOB`、`GRID_SIMPLEBLOB`、`PYRAMID_SIMPLEBLOB`

在当前的练习中，您将使用类型`FeatureDetector.ORB`。各种探测器类型的详细描述超出了本书的范围。然而，您可以参考本章后面的图 [7-9](#Fig9) 来比较各种探测器类型。

在`draw()`函数中，您使用方法`fd.detect(grey, pt)`来执行关键点检测，并将结果存储在名为`pt`的`MatOfKeyPoint`实例中。将`pt`转换成`KeyPoint`数组`kps`后，使用`for`循环遍历每个`KeyPoint`对象。对于每个`KeyPoint`，属性`pt`是点的位置。属性`response`描述了它成为关键点的可能性。您将它与阈值`MIN_RESP`进行比较，以跳过值较小的那些。属性`size`是关键点邻域的直径。属性`angle`显示关键点方向。使用一个圆来表示关键点及其邻域大小，使用一条直线来表示方向。图 [7-8](#Fig8) 显示了一个示例截图。灰度图像以较暗的色调显示，与关键点圆圈形成较高的对比度。

![A436326_1_En_7_Fig8_HTML.jpg](A436326_1_En_7_Fig8_HTML.jpg)

图 7-8。

Feature detection in features2d

图 [7-9](#Fig9) 显示了使用不同`FeatureDetector`类型检测到的关键点的集合。

![A436326_1_En_7_Fig9_HTML.jpg](A436326_1_En_7_Fig9_HTML.jpg)

图 7-9。

Comparison of different FeatureDetector types

您可以使用关键点信息进行创造性的可视化。然而，在下一节中，您将学习 OpenCV 中的通用特征匹配，以便进行后续跟踪。在使用特征匹配之前，还有一个步骤:关键点描述。您将使用来自`features2d`模块的`DescriptorExtractor`类来计算关键点的描述符。下一个练习`Chapter07_08`，将说明描述符的用法:

```
// Keypoint descriptor

import processing.video.*;
import org.opencv.core.*;
import org.opencv.imgproc.Imgproc;
import org.opencv.features2d.FeatureDetector;

Capture

cap;
CVImage img;
FeatureDetector fd;
DescriptorExtractor de;

void setup() {
  size(1280, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width/2, height);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  fd = FeatureDetector.create(FeatureDetector.AKAZE);
  // Create the instance for the descriptor
  de = DescriptorExtractor.create(DescriptorExtractor.AKAZE);
  smooth();
}

void draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  Mat grey = img.getGrey();
  image(cap, 0, 0);
  CVImage out = new CVImage(cap.width, cap.height);
  out.copyTo(grey);
  tint(255, 200);
  image(out, cap.width, 0);
  noTint();
  MatOfKeyPoint pt = new MatOfKeyPoint();
  fd.detect(grey, pt);
  Mat desc = new Mat();
  // Compute the descriptor from grey and pt.
  de.compute(grey, pt, desc);
  pushStyle();
  noFill();
  stroke(255, 200, 0);
  KeyPoint [] kps = pt.toArray();
  for (KeyPoint kp : kps) {
    float x = (float)kp.pt.x;
    float y = (float)kp.pt.y;
    ellipse(x+cap.width, y, kp.size, kp.size);
  }
  popStyle();
  pt.release();
  grey.release();
  desc.release();
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
}

```

节目就像上一个。它只添加了一个新类`DescriptorExtractor`和它的实例`de`。它使用`DescriptorExtractor.create()`方法在`setup()`函数中创建一个实例。在`draw()`函数中，它使用`compute()`方法在`Mat`中创建名为`desc`的描述符。加工窗口中的显示与图 [7-8](#Fig8) 相似，除了您在屏幕上生成更多的关键点，因为您没有跳过那些响应较低的关键点。对于`pt`中的每个关键点，在`desc`中将有一个条目用于描述该关键点。一旦在`desc`中有了描述符信息，就可以开始下一部分的匹配了。

## 特征匹配

特征匹配通常涉及两组信息。第一组由已知图像的特征点和描述符组成。你可以把它称为训练集。第二个包括来自新图像的特征点和描述符，通常来自实时捕获图像。您可以将其称为查询集。特征匹配的工作是在训练集和查询集之间进行特征点匹配。进行特征匹配的目的是从训练集中识别已知模式，并跟踪该模式在查询集中的移动位置。在接下来的练习中，您将首先在实时视频流的两个快照之间执行常规特征匹配，在第二个练习中，您将在快照中交互选择一个模式，并尝试跟踪它在实时视频流中的移动位置。

接下来的练习`Chapter07_09`，是匹配的准备。它将显示经过训练的快照图像和实时查询图像，以及关键点信息。您可以按下鼠标按钮来切换训练动作。

```
// Features matching
import processing.video.*;
import org.opencv.core.*;
import org.opencv.imgproc.Imgproc;
import org.opencv.features2d.FeatureDetector;

Capture cap;
CVImage img;
FeatureDetector fd;
DescriptorExtractor de;
// Two sets of keypoints: train, query
MatOfKeyPoint trainKp, queryKp;
// Two sets of descriptor: train, query
Mat trainDc, queryDc;
Mat grey

;
// Keep if training started.
boolean trained;
// Keep the trained image

.
PImage trainImg;

void setup() {
  size(1280, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width/2, height);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  trainImg = createImage(cap.width, cap.height, ARGB);
  fd = FeatureDetector.create(FeatureDetector.BRISK);
  de = DescriptorExtractor.create(DescriptorExtractor.BRISK);
  trainKp = new MatOfKeyPoint();
  queryKp = new MatOfKeyPoint();
  trainDc = new Mat();
  queryDc = new Mat();
  grey = Mat.zeros(cap.height, cap.width, CvType.CV_8UC1);
  trained = false;
  smooth();
}

void draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();

  if (trained) {
    image(trainImg, 0, 0);
    image(cap, trainImg.width, 0);
    img.copy(cap, 0, 0, cap.width, cap.height,
      0, 0, img.width, img.height);
    img.copyTo();
    grey = img.getGrey();
    fd.detect(grey, queryKp);
    de.compute(grey, queryKp, queryDc);
    drawTrain();
    drawQuery();
  } else {
    image(cap, 0, 0);
    image(cap, cap.width, 0);
  }
  pushStyle();
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
}

void

drawTrain() {
  // Draw the keypoints for the trained snapshot.
  pushStyle();
  noFill();
  stroke(255, 200, 0);
  KeyPoint [] kps = trainKp.toArray();
  for (KeyPoint kp : kps) {
    float x = (float)kp.pt.x;
    float y = (float)kp.pt.y;
    ellipse(x, y, kp.size, kp.size);
  }
  popStyle();
}

void

drawQuery() {
  // Draw the keypoints for live query image.
  pushStyle();
  noFill();
  stroke(255, 200, 0);
  KeyPoint [] kps = queryKp.toArray();
  for (KeyPoint kp : kps) {
    float x = (float)kp.pt.x;
    float y = (float)kp.pt.y;
    ellipse(x+trainImg.width, y, kp.size, kp.size);
  }
  popStyle();
}

void

mousePressed() {
  // Press mouse button to toggle training.
  if (!trained) {
    arrayCopy(cap.pixels, trainImg.pixels);
    trainImg.updatePixels();
    img.copy(trainImg, 0, 0, trainImg.width, trainImg.height,
      0, 0, img.width, img.height);
    img.copyTo();
    grey = img.getGrey();
    fd.detect(grey, trainKp);
    de.compute(grey, trainKp, trainDc);
    trained = true;
  } else {
    trained = false;
  }
}

```

这个程序相对简单。你保存了两对数据结构。第一对是用于训练图像的`MatOfKeyPoint`、`trainKp`和查询图像的`queryKp`。第二对由描述符`trainDc`和`queryDC`组成。当用户按下鼠标按钮时，它将拍摄当前视频流的快照，并使用该图像来计算训练的关键点`trainKp`和描述符`trainDc`。在`draw()`函数中，如果有经过训练的图像，程序将从实时视频图像中计算查询关键点`queryKp`和描述符`queryDc`。图像和关键点信息都将显示在处理窗口中。

图 [7-10](#Fig10) 显示了运行程序的示例截图。左图是静止图像及其训练好的关键点。右图是现场视频图像及其查询要点。

![A436326_1_En_7_Fig10_HTML.jpg](A436326_1_En_7_Fig10_HTML.jpg)

图 7-10。

Feature points from the trained and query images

下一个练习`Chapter07_10`将引入匹配来识别训练图像和查询图像之间的对应关键点。

```
// Features matching
import processing.video.*;
import java.util.Arrays;
import org.opencv.core.*;
import org.opencv.imgproc.Imgproc;
import org.opencv.features2d.FeatureDetector;
import org.opencv.features2d.DescriptorExtractor;
import org.opencv.features2d.DescriptorMatcher;

final

int MAX_DIST = 200;
Capture cap;
CVImage img;
FeatureDetector fd;
DescriptorExtractor de;
MatOfKeyPoint trainKp, queryKp;
Mat trainDc, queryDc;
DescriptorMatcher match;
Mat grey;
boolean trained;
PImage trainImg;

void

setup() {
  size(1280, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width/2, height);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  trainImg = createImage(cap.width, cap.height, ARGB);
  fd = FeatureDetector.create(FeatureDetector.ORB);
  de = DescriptorExtractor.create(DescriptorExtractor.ORB);
  match = DescriptorMatcher.create(DescriptorMatcher.BRUTEFORCE_L1);
  trainKp = new MatOfKeyPoint();
  queryKp = new MatOfKeyPoint();
  trainDc = new Mat();
  queryDc = new Mat();
  grey = Mat.zeros(cap.height, cap.width, CvType.CV_8UC1);
  trained = false;
  smooth();
}

void

draw() {
  if (!cap.available())
    return

;
  background(0);
  cap.read();

  if (trained) {
    image(trainImg, 0, 0);
    image(cap, trainImg.width, 0);
    img.copy(cap, 0, 0, cap.width, cap.height,
      0, 0, img.width, img.height);
    img.copyTo();
    grey = img.getGrey();
    fd.detect(grey, queryKp);
    de.compute(grey, queryKp, queryDc);
    MatOfDMatch pairs = new MatOfDMatch();
   // Perform key point matching.
    match.match(queryDc, trainDc, pairs);

    DMatch [] dm = pairs.toArray();
    KeyPoint [] tKp = trainKp.toArray();
    KeyPoint [] qKp = queryKp.toArray();
    // Connect the matched key points.
    for (DMatch d : dm) {
    // Skip those with large distance.
      if (d.distance>MAX_DIST)
        continue;
      KeyPoint t = tKp[d.trainIdx];
      KeyPoint q = qKp[d.queryIdx];
      line((float)t.pt.x, (float)t.pt.y,
        (float)q.pt.x+cap.width, (float)q.pt.y);
    }
    drawTrain();
    drawQuery();
    pairs.release();
  } else

{
    image(cap, 0, 0);
    image(cap, cap.width, 0);
  }
  pushStyle();
  fill(0);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
}

void

drawTrain() {
  pushStyle();
  noFill();
  stroke(255, 200, 0);
  KeyPoint [] kps = trainKp.toArray();
  for (KeyPoint kp : kps) {
    float x = (float)kp.pt.x;
    float y = (float)kp.pt.y;
    ellipse(x, y, kp.size, kp.size);
  }
  popStyle();
}

void

drawQuery() {
  pushStyle();
  noFill();
  stroke(255, 200, 0);
  KeyPoint [] kps = queryKp.toArray();
  for (KeyPoint kp : kps) {
    float x = (float)kp.pt.x;
    float y = (float)kp.pt.y;
    ellipse(x+trainImg.width, y, kp.size, kp.size);
  }
  popStyle();
}

void

mousePressed() {
  if (!trained) {
    arrayCopy(cap.pixels, trainImg.pixels);
    trainImg.updatePixels();
    img.copy(trainImg, 0, 0, trainImg.width, trainImg.height,
      0, 0, img.width, img.height);
    img.copyTo();
    grey = img.getGrey();
    fd.detect(grey, trainKp);
    de.compute(grey, trainKp, trainDc);
    trained = true;
  } else {
    trained = false;
  }
}

```

大部分代码与前面的程序`Chapter07_09`相同。然而，在这个代码中你有一些新的条目。在`setup()`函数中，您必须用下面的语句初始化`DescriptorMatcher`类实例`match`:

```
match = DescriptorMatcher.create(DescriptorMatcher.BRUTEFORCE_L1);

```

静态`create()`方法内部的参数是匹配方法。支持以下暴力破解方法的变体:`BRUTEFORCE`、`BRUTEFORCE_HAMMING`、`BRUTEFORCE_HAMMINGLUT`、`BRUTEFORCE_L1`和`BRUTEFORCE_SL2`。如果点击加工图像内部，将在`draw()`函数内部执行以下语句:

```
match.match(queryDc, trainDc, pairs);

```

`match()`功能将执行实时图像的关键点描述符`queryDc`和左侧存储图像的关键点描述符`trainDc`之间的匹配。变量`pairs`将所有匹配的关键点对存储为一个`MatOfDMatch`实例。`DMatch`是一种数据结构，用于维护存储在查询和训练关键点列表`queryKp`和`trainKp`中的关键点`queryIdx`和`trainIdx`的匹配索引。之后的`for`循环将枚举所有的关键点匹配对，并为匹配距离`d.distance`小于`MAX_DIST`阈值的匹配对绘制匹配线。图 [7-11](#Fig11) 显示了执行的结果截图。

![A436326_1_En_7_Fig11_HTML.jpg](A436326_1_En_7_Fig11_HTML.jpg)

图 7-11。

Feature matching

在许多情况下，您不会通过网络摄像头使用整个图像作为训练图像模式。您可以只选择图像的一部分作为您想要跟踪的图案。在下一个练习`Chapter07_11`中，您将使用鼠标绘制一个矩形，仅选择实时图像的一部分进行跟踪。这类似于大多数图形软件中的矩形选框工具。您单击并拖动以定义一个矩形区域作为训练图像，并仅使用该区域内的那些关键点来匹配来自实况视频的查询图像中的那些关键点。为了简化主程序，您定义了一个单独的类`Dragging`，从这里处理鼠标交互。

```
import org.opencv.core.Rect;

// Define 3 states of mouse drag action.
public enum State {
    IDLE,
    DRAGGING,
    SELECTED
}
// A class to handle the mouse drag action
public class Dragging {
  PVector p1, p2;
  Rect roi;
  State state;

  public Dragging() {
    p1 = new PVector(Float.MAX_VALUE, Float.MAX_VALUE);
    p2 = new PVector(Float.MIN_VALUE, Float.MIN_VALUE);
    roi = new Rect(0, 0, 0, 0);
    state = State.IDLE;
  }

  void init(PVector m) {
    empty(m);
    state = State.DRAGGING;
  }

  void update(PVector m) {
    p2.set(m.x, m.y);
    roi.x = (int)min(p1.x, p2.x);
    roi.y = (int)min(p1.y, p2.y);
    roi.width = (int)abs(p2.x-p1.x);
    roi.height = (int)abs(p2.y-p1.y);
  }

  void move(PVector m) {
    update(m);
  }

  void stop(PVector m) {
    update(m);
    state = State.SELECTED;
  }

  void empty(PVector m) {
    p1.set(m.x, m.y);
    p2.set(m.x, m.y);
    roi.x = (int)m.x;
    roi.y = (int)m.y;
    roi.width = 0;
    roi.height = 0;
  }

  void

reset(PVector m) {
    empty(m);
    state = State.IDLE;
  }

  boolean
isDragging() {
    return (state == State.DRAGGING);
  }

  boolean isSelected() {
    return (state == State.SELECTED);
  }

  boolean isIdle() {
    return (state == State.IDLE);
  }

  Rect getRoi() {
    return roi;
  }
}

```

该类定义了鼠标交互的三种状态:`IDLE`，当没有选择开始时；`DRAGGING`，用户点击并开始拖动鼠标时；和`SELECTED`，当用户释放鼠标按钮以确认选择矩形时，`roi`。该类维护两个`PVector`对象:`p1`，选择矩形的左上角，和`p2`，选择矩形的右下角。当用户开始点击-拖动动作时，程序将调用`init()`方法。在拖动动作过程中，它会调用`move()`方法。当用户停止并释放鼠标按钮时，它将调用`stop()`方法。当用户点击而没有任何拖动时，它将通过调用`reset()`方法清除选择。该类还为用户提供了三个布尔方法(`isIdle()`、`isDragging()`和`isSelected()`)来查询交互的状态。

主程序类似于`Chapter07_10`练习，除了您有额外的代码来处理选择交互和消除选择矩形外的关键点的方法。

```
// Features matching with selection
import processing.video.*;
import java.util.Arrays;
import org.opencv.core.*;
import org.opencv.imgproc.Imgproc;
import org.opencv.features2d.FeatureDetector;
import org.opencv.features2d.DescriptorExtractor;
import org.opencv.features2d.DescriptorMatcher;
import org.opencv.calib3d.Calib3d;

Capture cap;
CVImage img;
// Feature detector, extractor and matcher
FeatureDetector fd;
DescriptorExtractor de;
DescriptorMatcher match;
// Key points and descriptors for train and query
MatOfKeyPoint trainKp, queryKp;
Mat trainDc, queryDc;
// Buffer for the trained image
PImage trainImg;
// A class to work with mouse drag & selection
Dragging drag;
Mat hg;
MatOfPoint2f trainRect, queryRect;

void setup() {
  size(1280, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width/2, height);
  cap.start();
  img = new CVImage(cap.width, cap.height);
  trainImg = createImage(cap.width, cap.height, ARGB);
  fd = FeatureDetector.create(FeatureDetector.ORB);
  de = DescriptorExtractor.create(DescriptorExtractor.ORB);
  match = DescriptorMatcher.create(DescriptorMatcher.BRUTEFORCE_HAMMING);
  trainKp = new MatOfKeyPoint();
  queryKp = new MatOfKeyPoint();
  trainDc = new Mat();
  queryDc = new Mat();
  hg = Mat.eye(3, 3, CvType.CV_32FC1);
  drag = new Dragging();
  smooth();
  trainRect = new MatOfPoint2f();
  queryRect = new MatOfPoint2f();
}

void

draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  Mat grey = img.getGrey();
  image(trainImg, 0, 0);
  image(cap, trainImg.width, 0);

  if (drag.isDragging()) {
    drawRect(cap.width);
  } else if (drag.isSelected()) {
    drawRect(0);
    matchPoints(grey);
    drawTrain();
    drawQuery();
  }
  pushStyle();
  fill(80);
  text(nf(round(frameRate), 2), 10, 20);
  popStyle();
  grey.release();
}

void matchPoints(Mat im) {
  // Match the trained and query key points.
  fd.detect(im, queryKp);
  de.compute(im, queryKp, queryDc);
  // Skip if the trained or query descriptors are empty.
  if (!queryDc.empty() &&
    !trainDc.empty()) {
    MatOfDMatch pairs = new MatOfDMatch();
    match.match(queryDc, trainDc, pairs);
    DMatch [] dm = pairs.toArray();
    // Convert trained and query MatOfKeyPoint to array.
    KeyPoint [] tKp = trainKp.toArray();
    KeyPoint [] qKp = queryKp.toArray();
    float minDist = Float.MAX_VALUE;
    float maxDist = Float.MIN_VALUE;
    // Obtain the min and max distances of matching.
    for (DMatch d : dm) {
      if (d.distance < minDist) {
        minDist = d.distance;
      }
      if (d.distance > maxDist) {
        maxDist = d.distance;
      }
    }
    float

thresVal = 2*minDist;
    ArrayList<Point> trainList = new ArrayList<Point>();
    ArrayList<Point> queryList = new ArrayList<Point>();
    pushStyle();
    noFill();
    stroke(255);
    for (DMatch d : dm) {
      if (d.queryIdx >= qKp.length ||
        d.trainIdx >= tKp.length)
        continue;
      // Skip match data with distance larger than
      // 2 times of min distance.
      if (d.distance > thresVal)
        continue;
      KeyPoint t = tKp[d.trainIdx];
      KeyPoint q = qKp[d.queryIdx];
      trainList.add(t.pt);
      queryList.add(q.pt);
      // Draw a line for each pair of matching key points.
      line((float)t.pt.x, (float)t.pt.y,
        (float)q.pt.x+cap.width, (float)q.pt.y);
    }
    MatOfPoint2f trainM = new MatOfPoint2f();
    MatOfPoint2f queryM = new MatOfPoint2f();
    trainM.fromList(trainList);
    queryM.fromList(queryList);
    // Find the homography matrix between the trained
    // key points and query key points.
    // Proceed only with more than 5 key points.
    if (trainList.size() > 5 &&
      queryList.size() > 5) {
      hg = Calib3d.findHomography(trainM, queryM, Calib3d.RANSAC, 3.0);
      if (!hg.empty()) {
        // Perform perspective transform to the
        // selection rectangle with the homography matrix.
        Core.perspectiveTransform(trainRect, queryRect, hg);
      }
      pairs.release();
      trainM.release();
      queryM.release();
      hg.release();
    }
    if (!queryRect.empty()) {
      // Draw the transformed selection matrix.
      Point [] out = queryRect.toArray();
      stroke(255, 255, 0);
      for (int i=0; i<out.length; i++) {
        int j = (i+1) % out.length;
        Point p1 = out[i];
        Point p2 = out[j];
        line((float)p1.x+cap.width, (float)p1.y,
          (float)p2.x+cap.width, (float)p2.y);
      }
    }
  }
  popStyle();
}

void

drawRect(float ox) {
  // Draw the selection rectangle.
  pushStyle();
  noFill();
  stroke(255, 255, 0);
  rect(drag.getRoi().x+ox, drag.getRoi().y,
    drag.getRoi().width, drag.getRoi().height);
  popStyle();
}

void

drawTrain() {
  // Draw the trained key points.
  pushStyle();
  noFill();
  stroke(255, 200, 0);
  KeyPoint [] kps = trainKp.toArray();
  for (KeyPoint kp : kps) {
    float x = (float)kp.pt.x;
    float y = (float)kp.pt.y;
    ellipse(x, y, 10, 10);
  }
  popStyle();
}

void drawQuery() {
  // Draw live image key points.
  pushStyle();
  noFill();
  stroke(255, 200, 0);
  KeyPoint [] kps = queryKp.toArray();
  for (KeyPoint kp : kps) {
    float x = (float)kp.pt.x;
    float y = (float)kp.pt.y;
    ellipse(x+trainImg.width, y, 10, 10);
  }
  popStyle();
}

void mouseClicked() {
  // Reset the drag rectangle.
  drag.reset(new PVector(0, 0));
}

void mousePressed() {
  // Click only on the right hand side of the window
  // to start the drag action.
  if (mouseX < cap.width || mouseX >= cap.width*2)
    return;
  if (mouseY < 0 || mouseY >= cap.height)
    return;
  drag.init(new PVector(mouseX-cap.width, mouseY));
}

void

mouseDragged() {
  // Drag the selection rectangle.
  int x = constrain(mouseX, cap.width, cap.width*2-1);
  int y = constrain(mouseY, 0, cap.height-1);
  drag.move(new PVector(x-cap.width, y));
}

void mouseReleased() {
  // Finalize the selection rectangle.
  int x = constrain(mouseX, cap.width, cap.width*2-1);
  int y = constrain(mouseY, 0, cap.height-1);
  drag.stop(new PVector(x-cap.width, y));

  // Compute the trained key points and descriptor.
  arrayCopy(cap.pixels, trainImg.pixels);
  trainImg.updatePixels();
  CVImage tBGR = new CVImage(trainImg.width, trainImg.height);
  tBGR.copy(trainImg, 0, 0, trainImg.width, trainImg.height,
    0, 0, tBGR.width, tBGR.height);
  tBGR.copyTo();
  Mat temp = tBGR.getGrey();
  Mat tTrain = new Mat();
  // Detect and compute key points and descriptors.
  fd.detect(temp, trainKp);
  de.compute(temp, trainKp, tTrain);
  // Define the selection rectangle.
  Rect r = drag.getRoi();
  // Convert MatOfKeyPoint to array.
  KeyPoint [] iKpt = trainKp.toArray();
  ArrayList<KeyPoint> oKpt = new ArrayList<KeyPoint>();
  trainDc.release();
  // Select only the key points inside selection rectangle.
  for (int i=0; i<iKpt.length; i++) {
    if (r.contains(iKpt[i].pt)) {
      // Add key point to the output list.
      oKpt.add(iKpt[i]);
      trainDc.push_back(tTrain.row(i));
    }
  }
  trainKp.fromList(oKpt);
  // Compute the selection rectangle as MatOfPoint2f.
  ArrayList<Point> quad = new ArrayList<Point>();
  quad.add(new Point(r.x, r.y));
  quad.add(new Point(r.x+r.width, r.y));
  quad.add(new Point(r.x+r.width, r.y+r.height));
  quad.add(new Point(r.x, r.y+r.height));
  trainRect.fromList(quad);
  queryRect.release();
  tTrain.release();
  temp.release();
}

```

在处理窗口中，屏幕上将有两个图像。左边的是当用户通过鼠标拖动动作执行选择时的训练图像。右边是现场视频图像。当用户想要做出选择时，用户需要点击并拖动右边的实时图像。当选择矩形被确认时，它将与实时视频图像的快照一起被发送到左侧。处理事件处理器`mouseClicked()`、`mousePressed()`、`mouseDragged()`和`mouseReleased()`管理交互选择过程。在`mouseReleased()`方法中，你有额外的代码来首先从实时视频图像的灰度版本中检测关键点；其次计算关键点的描述符；第三遍所有的关键点，只选择那些在选择矩形内的，`drag.getRoi()`；第四，准备已训练的关键点列表`trainKp`和描述符`trainDc`；最后将选择矩形转换为名为`trainRect`的`MatOfPoint2f`变量。

在`draw()`函数中，您只需在`DRAGGING`状态下绘制临时选择矩形。在`SELECTED`状态下，你将调用`matchPoints()`函数，这是程序中最复杂的函数。在这个函数中，它首先从实时视频图像中检测关键点，并计算描述符。当训练描述符和查询描述符都不为空时，它执行关键点匹配。注意，经过训练的描述符`trainDc`仅包含选择矩形内的关键点描述。匹配后，该函数将遍历所有匹配对，找出名为`pairs`的`MatOfDMatch`对象内的最小和最大距离。在随后的循环中，只处理距离小于最小距离值两倍的匹配对。在`for`循环之后，你将绘制连接所有匹配关键点的线，并从关键点列表中创建另外两个`MatOfPoint2f`变量`trainM`和`queryM`。当`trainM`和`queryM`都包含五个以上的关键点时，使用`Calib3d.findHomography()`方法从两个关键点列表中计算转换矩阵(单应矩阵)`hg`。通过单应矩阵`hg`，执行透视变换`Core.perspectiveTransform()`，将保存在`trainRect`中的选择矩形转换为`queryRect`。`queryRect`形状由转换后的矩形的四个角的坐标组成，位于窗口的右侧。本质上，四个角将定义跟踪图案的矩形。`matchPoints()`函数的最后一部分绘制了连接`queryRect`中四个角的四条直线。

图 [7-12](#Fig12) 显示了结果截图。右侧的四边形是通过使用从左侧的静态训练图像中检测到的模式来跟踪的区域。为了获得最佳效果，您选择的图案应该包含高对比度的视觉纹理。你也应该避免在背景中出现类似的纹理。在`matchPoints()`功能中，您建立一个阈值来跳过差异大于两倍最小距离的匹配关键点。您可以降低阈值来减少噪音条件。

![A436326_1_En_7_Fig12_HTML.jpg](A436326_1_En_7_Fig12_HTML.jpg)

图 7-12。

Key point matching with a selection rectangle

除了绘制四边形的轮廓之外，下一个练习`Chapter07_12`将在实时网络摄像头图像上绘制的四边形上执行纹理映射。我没有在这里列出整个练习的源代码，我只是在`Chapter07_11`中强调了原始版本的变化。您定义了一个全局的`PImage`变量`photo`，来保存您想要映射到被跟踪图案顶部的图像。在`setup()`函数中，你使用`P3D`渲染为`size(1280, 480, P3D)`，同时设置纹理模式为`textureMode(NORMAL)`正常。在`matchPoints()`函数的末尾，您有以下代码来绘制上一个练习`Chapter07_11`中的四边形:

```
if (!queryRect.empty()) {
      // Draw the transformed selection matrix.
      Point [] out = queryRect.toArray();
      stroke(255, 255, 0);
      for (int i=0; i<out.length; i++) {
        int j = (i+1) % out.length;
        Point p1 = out[i];
        Point p2 = out[j];
        line((float)p1.x+cap.width, (float)p1.y,
          (float)p2.x+cap.width, (float)p2.y);
      }
    }

```

在这个新版本中，`Chapter07_12`，你通过使用`beginShape()`和`endShape()`函数来绘制四边形。在形状定义中，使用四个`vertex()`函数通过纹理映射选项绘制四边形。

```
if (!queryRect.empty()) {
      // Draw the transformed selection matrix.
      Point [] out = queryRect.toArray();
      noStroke();
      fill(255);
      beginShape();
      texture(photo);
      vertex((float)out[0].x+cap.width, (float)out[0].y, 0, 0, 0);
      vertex((float)out[1].x+cap.width, (float)out[1].y, 0, 1, 0);
      vertex((float)out[2].x+cap.width, (float)out[2].y, 0, 1, 1);
      vertex((float)out[3].x+cap.width, (float)out[3].y, 0, 0, 1);
      endShape(CLOSE);
    }

```

生成的图像将类似于图 [7-13](#Fig13) 所示。

![A436326_1_En_7_Fig13_HTML.jpg](A436326_1_En_7_Fig13_HTML.jpg)

图 7-13。

Key points matching with texture mapped onto the rectangle

您可能会发现，前面的练习是构建无标记增强现实应用程序的基础。在更高级的使用中，`PImage`变量`photo`会被一个三维物体代替。然而，这超出了本书讨论细节的范围。如果感兴趣，可以在 OpenCV 相关文档中寻找 3D 姿态估计。

## 人脸检测

在交互式媒体制作中，艺术家和设计师经常求助于 OpenCV 的人脸检测功能。该功能是 OpenCV `objdetect`模块中的特性之一。该实现基于 Paul Viola 和 Michael Jones 在 2001 年发表的论文“使用简单特征的增强级联进行快速对象检测”。人脸检测是一个机器学习过程。这意味着，在执行人脸检测之前，您需要训练程序来学习有效和无效的人脸。然而，在 OpenCV 中，发行版包括保存在`data/haarcascades`文件夹中的预训练信息。您可以使用任何一个 XML 文件来检测特征，如正面脸、侧面脸、眼睛，甚至表情，如微笑。

在下一个练习`Chapter07_13`中，您将使用参数文件`haarcascade_frontalface_default.xml`检测用户的正面人脸。该文件位于`opencv-3.1.0/data/haarcascades`的 OpenCV 分发文件夹中。您需要将该文件从 OpenCV 发行版复制到加工草图的`data`文件夹中。

```
// Face detection
import processing.video.*;

import org.opencv.core.*;
import org.opencv.objdetect.CascadeClassifier;

// Detection image size

final

int W = 320, H = 240;
Capture cap;
CVImage img;
CascadeClassifier face;
// Ratio between capture size and
// detection size
float ratio;

void setup() {
  size(640, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width, height);
  cap.start();
  img = new CVImage(W, H);
  // Load the trained face information.
  face = new CascadeClassifier(dataPath("haarcascade_frontalface_default.xml"));
  ratio = float(width)/W;
}

void draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  image(cap, 0, 0);
  Mat grey = img.getGrey();
  // Perform face detction. Detection
  // result is in the faces.
  MatOfRect faces = new MatOfRect();
  face.detectMultiScale(grey, faces);
  Rect [] facesArr = faces.toArray();
  pushStyle();
  fill(255, 255, 0, 100);
  stroke(255);
  // Draw each detected face.
  for (Rect r : facesArr) {
    rect(r.x*ratio, r.y*ratio, r.width*ratio, r.height*ratio);
  }
  grey.release();
  faces.release();
  noStroke();
  fill(0);
  text(nf(round(frameRate), 2, 0), 10, 20);
  popStyle();
}

```

您在人脸检测中使用的参数属于`CascadeClassifier`类。首先，您必须定义这个类的一个实例`face`。在`setup()`功能中，你用来自文件`haarcascade_frontalface_default.xml`的训练过的正面面部细节创建新的实例，该文件被复制到`data`文件夹中。您还可以使用处理函数`dataPath()`来返回`data`文件夹的绝对路径。为了优化性能，您在以下语句中使用一个较小尺寸(320×240)的灰度图像`grey`进行检测:

```
face.detectMultiScale(grey, faces);

```

第一个参数是要检测人脸的灰度图像。结果会在第二个参数中，也就是`MatOfRect`变量`faces`。通过将它转换成一个`Rect`数组`facesArr`，您可以使用一个`for`循环来显示所有的边界矩形。图 [7-14](#Fig14) 显示了程序的一个示例显示。

![A436326_1_En_7_Fig14_HTML.jpg](A436326_1_En_7_Fig14_HTML.jpg)

图 7-14。

Face detection

一旦检测到人脸，您可以进一步检测人脸的边框内的面部特征。在下面的练习`Chapter07_14`中，您将在一张脸内执行微笑检测。这个节目就像上一个。检测到面部后，使用边框创建一个较小的图像，并在其中检测微笑面部特征。为了测试程序，您还需要将 OpenCV 发行版中的`haarcascade_smile.xml`文件复制到加工草图的`data`文件夹中。

```
// Smile detection

import processing.video.*;

import org.opencv.core.*;
import org.opencv.objdetect.CascadeClassifier;

// Face

detection size
final int W = 320, H = 240;
Capture cap;
CVImage img;
// Two classifiers, one for face, one for smile
CascadeClassifier face, smile;
float ratio;

void setup() {
  size(640, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width, height);
  cap.start();
  img = new CVImage(W, H);
  face = new CascadeClassifier(dataPath("haarcascade_frontalface_default.xml"));
  smile = new CascadeClassifier(dataPath("haarcascade_smile.xml"));
  ratio = float(width)/W;
}

void draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  noStroke();
  image(cap, 0, 0);
  Mat grey = img.getGrey();
  MatOfRect faces = new MatOfRect();
  // Detect the faces first.
  face.detectMultiScale(grey, faces, 1.15, 3,
    Objdetect.CASCADE_SCALE_IMAGE,
    new Size(60, 60), new Size(200, 200));
  Rect [] facesArr = faces.toArray();
  pushStyle();
  for (Rect r : facesArr) {
    fill(255, 255, 0, 100);
    stroke(255, 0, 0);
    float cx = r.x + r.width/2.0;
    float cy = r.y + r.height/2.0;
    ellipse(cx*ratio, cy*ratio,
      r.width*ratio, r.height*ratio);
    // For each face, obtain the image within the bounding box.
    Mat fa = grey.submat(r);
    MatOfRect m = new MatOfRect();
    // Detect smiling expression.
    smile.detectMultiScale(fa, m, 1.2, 25,
      Objdetect.CASCADE_SCALE_IMAGE,
      new Size(30, 30), new Size(80, 80));
    Rect [] mArr = m.toArray();
    stroke(0, 0, 255);
    noFill();
    // Draw the line of the mouth.
    for (Rect sm : mArr) {
      float yy = sm.y+r.y+sm.height/2.0;
      line((sm.x+r.x)*ratio, yy*ratio,
        (sm.x+r.x+sm.width)*ratio, yy*ratio);
    }
    fa.release();
    m.release();
  }
  noStroke();
  fill(0);
  text(nf(round(frameRate), 2, 0), 10, 20);
  popStyle();
  grey.release();
  faces.release();

}

```

在`setup()`函数中，您初始化两个分类器，一个用于您在前一个练习中使用的人脸。第二个分类器是一个新的分类器，其训练信息在`haarcascade_smile.xml`中。在`draw()`功能中，您还可以使用另一个版本的`detectMultiScale()`功能。前两个参数是相同的。第三个参数是图像在每个比例下缩小的比例因子。数字越大，检测速度越快，但这是以不太准确为代价的。第四个参数是保留的最小邻居数量。较大的数量将消除更多的错误检测。第五个参数是一个伪参数。最后两个参数是您想要检测的对象(面部或微笑)的最小和最大尺寸。

在第一个`for`循环中，显示所有椭圆形状的面。对于每个面，你使用包围矩形`r`创建一个子矩阵(感兴趣的区域)`fa`。然后你在这个小图像中检测出微笑，并在检测的中心画一条水平线。图 [7-15](#Fig15) 展示了一次成功的微笑检测。

![A436326_1_En_7_Fig15_HTML.jpg](A436326_1_En_7_Fig15_HTML.jpg)

图 7-15。

Successful smile detection

图 [7-16](#Fig16) 显示了另一个微笑检测不成功的试验。

![A436326_1_En_7_Fig16_HTML.jpg](A436326_1_En_7_Fig16_HTML.jpg)

图 7-16。

Unsuccessful smile detection

## 人物检测

除了常规的面部特征检测，OpenCV 中的`objdetect`模块还通过`HOGDescriptor`(梯度方向直方图)类提供了人物检测功能。你可以使用这个类从数字图像中检测整个人体。下面的练习`Chapter07_15`将演示如何使用`HOGDescriptor`功能从实时视频图像中检测人体。为了获得最佳效果，您需要在相对清晰的背景下检测整个身体。

```
// People detection
import processing.video.*;
import org.opencv.core.*;
import org.opencv.objdetect.HOGDescriptor;

// Detection size
final int W = 320, H = 240;

Capture

cap;
CVImage img;
// People detection
descriptor
HOGDescriptor hog;

float
ratio;

void setup() {
  size(640, 480);
  background(0);
  System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
  println(Core.VERSION);
  cap = new Capture(this, width, height);
  cap.start();
  img = new CVImage(W, H);
  // Initialize the descriptor.
  hog = new HOGDescriptor();
  // User the people detector.
  hog.setSVMDetector(HOGDescriptor.getDefaultPeopleDetector());
  ratio = float(width)/W;
}

void draw() {
  if (!cap.available())
    return;
  background(0);
  cap.read();
  img.copy(cap, 0, 0, cap.width, cap.height,
    0, 0, img.width, img.height);
  img.copyTo();
  image(cap, 0, 0);
  Mat grey = img.getGrey();
  MatOfRect found = new MatOfRect();
  MatOfDouble weight = new MatOfDouble();
  // Perform the people detection.
  hog.detectMultiScale(grey, found, weight);
  Rect [] people = found.toArray();
  pushStyle();
  fill(255, 255, 0, 100);
  stroke(255);
  // Draw the bounding boxes of people detected.
  for (Rect r : people) {
    rect(r.x*ratio, r.y*ratio, r.width*ratio, r.height*ratio);
  }
  grey.release();
  found.release();
  weight.release();
  noStroke();
  fill(0);
  text(nf(round(frameRate), 2, 0), 10, 20);
  popStyle();
}

```

相比人脸检测，程序更简单。您不需要加载任何经过训练的数据文件。您只需用下面的语句初始化`HOGDescriptor class`实例`hog`并设置默认的人员描述符信息:

```
hog.setSVMDetector(HOGDescriptor.getDefaultPeopleDetector());

```

在`draw()`函数中，使用`detectMultiScale()`方法从灰度图像`grey`中识别人物，并将结果保存在`MatOfRect`变量`found`中。最后一个参数是一个伪参数。在`for`循环中，用矩形绘制每个边界框`r`。图 [7-17](#Fig17) 为程序截图。

![A436326_1_En_7_Fig17_HTML.jpg](A436326_1_En_7_Fig17_HTML.jpg)

图 7-17。

People detection

## 结论

在这一章中，你看到了从图像中识别关键点的不同方法。使用从两个连续帧中识别的关键点，可以执行稀疏光流分析或通用关键点描述符匹配来跟踪帧之间的视觉模式。这项技术对增强现实应用很有用。除了关键点跟踪，您还探索了 OpenCV 中面部特征和全身检测的简单使用。这些方法有利于艺术家和设计师通过计算机视觉进行具体化交互。在下一章中，您将了解在部署应用程序时使用处理的专业实践。