# 5.深度学习和人工神经网络

本章将涵盖深度学习和人工神经网络。本章将通过工作代码示例来探讨这一主题，以展示如何在计算机视觉中应用深度学习概念。我们本章的学习目标如下:

*   理解神经网络、它们的架构以及在幕后工作的各种数学函数和算法。

*   在 TensorFlow 中编写代码来摄取图像，提取特征，并训练不同类型的神经网络。

*   编写代码并了解如何在图像分类中使用预训练和定制训练的模型。我们还将学习如何重新培训现有模型。

*   了解如何评估模型和调整参数，以优化模型在准确性方面的性能。

本章将包括一些数学概念和方程式。虽然没有必要对本章中列出的方程的数学有一个正式的理解，但是我们确实为你提供了一些参考资料来探索这些方程的数学处理。

## 人工神经网络导论

人工神经网络(ANN)是一种计算系统，它被设计成以人脑的方式工作。让我们用一个简单的例子来理解这一点。

假设你看到一个你从未见过的物体。有人告诉你这是一辆车。然后你会看到许多其他物体，并学会识别它们。然后你看到另一个物体，你试着猜它是什么。你可能会说，“我想我以前见过这个。”或者你可以说，“我猜这是一辆汽车。”这意味着你不能 100%确定物体的身份。现在，假设你看到许多不同形状、大小、方向和颜色的汽车。你受过识别“汽车”物体的全面训练。最有可能的是，你不会说“我猜”，但你会说，“这是一辆车。”这意味着，随着你通过观察大量汽车而更好地训练自己，你识别汽车的信心会增加。

这里所发生的是，当你只看一次或几次一辆车时，如果它以你以前看到的相同或相似的方式出现，你就学会了认出它。但是当你以各种各样的方式看到大量的样本时，你学会了以 100%或接近 100%的准确率来识别物体。让我们看看图 [5-1](#Fig1) 中的图表，看看信息在我们的大脑中是如何处理的(人脑功能的简化版)。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig1_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig1_HTML.jpg)

图 5-1

人类的眼睛作为传感装置，向储存模式的大脑输入信息

我们的眼睛是一个传感装置。当我们看到一个物体时，我们的眼睛捕捉到该物体的图像，并将其作为输入信号传递给大脑。我们大脑中的神经元对输入信号进行计算并产生输出。

如图 [5-2](#Fig2) 所示，树突接收输入信号(X)。神经元组合这些输入信号，并使用某种函数执行计算。输出被传送到轴突末梢。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig2_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig2_HTML.jpg)

图 5-2

人类神经元的信息处理

人体有数十亿个神经元，它们之间有数万亿个相互连接。这些相互连接的神经元被称为一个神经网络。

计算机科学家受到人类视觉系统的启发，试图通过创建一个像我们的大脑一样学习和运行的计算机系统来模仿神经网络。这个学习系统被称为*人工神经网络* (ANN)。

图 [5-3](#Fig3) 与图 [5-1](#Fig1) 相似。相机作为一种传感设备工作，就像我们的眼睛捕捉物体的图像一样。图像被传输到解释系统，如计算机，在那里它们以类似于神经元处理输入信号的方式被处理。其他感测设备的一些例子是 X 射线、CT 扫描和 MRI 机器；卫星成像系统；和文档扫描仪。诸如计算机之类的解释设备提供对由照相机获取的数据的处理。大多数与计算机视觉相关的计算，如特征提取和模式确定，都是在计算机中进行的。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig3_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig3_HTML.jpg)

图 5-3

向计算机输入图像的人工传感设备(照相机)

图 [5-4](#Fig4) 类似于图 [5-2](#Fig2) 所示的人类神经元。变量 x1，x2，..xn 是具有特定权重 w1，w2，..wn 与每个输入信号相关联。使用一些数学函数处理这些输入信号以产生输出。组合这些输入信号的处理单元被称为*神经元*，以人类神经元命名。计算神经元输出的数学函数被称为*激活函数*。在图 [5-4](#Fig4) 中，标有函数符号 *f(x)* 的圆圈就是神经元。输出 y 由神经元产生。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig4_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig4_HTML.png)

图 5-4

人工神经元

### 感知器

神经网络的单个神经元称为*感知器*。感知器执行数学功能，对输入信号进行运算并产生输出。图 [5-4](#Fig4) 是感知器的一个例子。感知器是最简单的神经网络。我们将在后面看到，用于机器学习的典型神经网络由几个神经元组成。神经元的输入或者来自源(照相机或传感设备)，或者来自其他神经元的输出。

#### 感知器如何学习

感知器的学习目标是确定每个输入信号的理想权重。学习算法任意地给每个输入信号分配权重。信号值乘以其相应的权重。将每个信号的乘积(加权乘以信号值)相加以计算输出。该计算由以下等式表示:

![$$ \boldsymbol{f}\left(\boldsymbol{x}\right)=\boldsymbol{w}\boldsymbol{1}\boldsymbol{x}\boldsymbol{1}+\boldsymbol{w}\boldsymbol{2}\boldsymbol{x}\boldsymbol{2}+\boldsymbol{w}\boldsymbol{3}\boldsymbol{x}\boldsymbol{3}+\dots +\boldsymbol{w}\boldsymbol{nxn} $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ1.png)

(等式 5-1)

有时一个偏差 x <sub>0</sub> 也被加到方程中，如下所示:

![$$ \boldsymbol{f}\left(\boldsymbol{x}\right)=\boldsymbol{x}\boldsymbol{0}+\boldsymbol{w}\boldsymbol{1}\boldsymbol{x}\boldsymbol{1}+\boldsymbol{w}\boldsymbol{2}\boldsymbol{x}\boldsymbol{2}+\boldsymbol{w}\boldsymbol{3}\boldsymbol{x}\boldsymbol{3}+\dots +\boldsymbol{w}\boldsymbol{nxn} $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ2.png)

(方程 5-2)

方程式 5-2 也可以写成:

![$$ {\displaystyle \begin{array}{c}i=\mathrm{n}\\ {}\mathrm{f}\left(\mathrm{x}\right)={\mathrm{X}}_0+\left[?\right]\left[?\right]\left[?\right]{\mathrm{W}}_{\mathrm{i}}{\mathrm{X}}_{\mathrm{i}}\\ {}i=1\end{array}} $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ3.png)

(方程式 5-3)

神经元使用等式 5-2 对大量输入进行计算。优化功能通过使用特定的数学算法优化权重，称为*优化器*，并使用新的权重重复计算。这种权重优化和计算以及重新优化在多次迭代中执行，直到权重对于给定的输入集完全优化。我们将在本章的后面了解关于这个优化函数的更多信息。完全优化的权重是神经元的实际学习。

### 多层感知器

就像人脑包含数十亿个神经元一样，人工神经网络包含几个神经元或感知机。输入由一组神经元处理。组中的每个神经元独立处理输入。这组神经元的输出被馈送到另一个神经元或另一组神经元进行进一步处理。你可以想象这些神经元排列成层，其中一层的输出作为输入输入到下一层。您可以根据需要设置任意多个层来训练您的神经网络。这种在神经网络中排列神经元的多层方法通常被称为*多层感知器* (MLP)。图 [5-5](#Fig5) 显示了一个 MLP 的例子。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig5_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig5_HTML.png)

图 5-5

多层感知器

#### 为什么是 MLP？

让我们考虑具有单个输入的单个神经元。等式 5-1 看起来会像下面这样:

![$$ \boldsymbol{f}\left(\boldsymbol{x}\right)=\boldsymbol{x}\boldsymbol{0}+\boldsymbol{w}\boldsymbol{1}\boldsymbol{x}\boldsymbol{1} $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equa.png)

这代表一条截距为 x <sub>0</sub> 且斜率(与水平线或*x*-轴的角度)等于 w <sub>1</sub> 的直线方程。

如果你不理解这个数学，不要担心。这是为了向你展示单个神经元模拟输入到输出的线性关系。机器学习算法，如线性回归和逻辑回归，对线性关系进行建模。大多数现实世界的问题并不呈现线性关系。多层感知器对非线性进行建模，可以比基于单个神经元的模型更准确地对现实世界的问题进行建模。

### 什么是深度学习？

*深度学习*是多层人工神经网络或多层感知器的别称。根据神经网络架构及其工作原理，我们有不同类型的深度学习系统。例如，前馈神经网络、卷积网络、递归神经网络、自动编码器和深度信念是不同类型的深度学习系统。

以下部分首先解释多层感知器的高级架构。在本书中，我们将互换使用 MLP 和深度学习。

### 深度学习或多层感知器架构

一个多层感知器至少由三类层组成:输入层、隐藏层、输出层(如图 [5-5](#Fig5) )。您可以有多个隐藏层。每层包含一个或多个神经元。神经元对它获得的输入进行一些计算，并产生输出。神经元的输出作为输入发送到下一层，但输出层除外，它生成最终输出供应用程序使用。

MLP 体系结构包括以下内容:

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig6_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig6_HTML.png)

图 5-6

具有偏置节点的多层感知器

*   *输入层*:神经网络的第一层称为*输入层*。这一层接受来自外部源的输入，例如来自传感设备的图像。该层的输入为特征(参见第 [4 章](04.html)了解特征详情)。

    输入层中的节点不做任何计算。这些节点只是将它们的输入传递给下一层。

    输入层中神经元的数量与特征的数量相同。有时，在每层中添加一个额外的节点。这个附加节点被称为*偏置节点*。添加偏置节点是为了控制该层的输出。在深度学习中，不需要偏差，但增加一个偏差是常见的做法。

    图 [5-6](#Fig6) 显示了带有偏置节点的神经网络架构。以橙色显示的节点是添加到每个层中的偏移节点。

    问题:神经网络输入层的神经元总数是多少？

    答案:输入层神经元数=无偏差输入特征数=(有偏差输入特征数+ 1)

*   *隐层*:输入层和输出层之间的神经元层称为*隐层*。神经网络必须至少有一个隐藏层。这是学习发生的一层。这一层的神经元进行学习所需的计算。在大多数情况下，一个隐藏层对于学习来说就足够了，但是您可以根据需要拥有任意多个层来模拟真实世界的情况。随着隐藏层数量的增加，计算复杂性也随着计算时间的相应增加而增加。

    隐藏层应该有多少个神经元？没有神奇的数字，有几种可行的策略。常见的做法是取前一层神经元数量的三分之二(或 66%)。例如，如果输入层中的神经元数量为 100，则第一个隐藏层中的神经元数量为 66，下一个隐藏层中的神经元数量为 43，依此类推。同样，没有神奇的数字，您应该根据模型的准确性来调整神经元的数量。

*   *输出层*:神经网络的最后一层是输出层。输出层从最后一个隐藏层获取输入。输出层中神经元的数量取决于您希望神经网络解决的问题类型，如下所述:

*   对于网络必须预测连续值(如股票收盘价)的回归问题，输出节点只有一个神经元。

*   对于分类问题，当网络必须预测许多类中的一个时，输出层具有与所有可能的类的数量一样多的神经元。例如，如果网络被训练来预测四类动物中的一类——猫、狗、狮子、公牛——输出层将有四个神经元，每类一个。

*   *边或权重连接*:权重也被称为*系数*或*输入乘数*。神经元的每个输入特征都乘以一个权重。形象地说，从输入到神经元的每个连接都用一条加权线连接。加权线表示特征在预测我们试图建模的结果中的贡献。将权重视为输入要素的贡献或重要性。权重越高，特征的贡献越大。如果权重为负，则该特征具有负面影响。如果权重为零，则输入特征不重要，可以从训练集中移除。

    神经网络的训练目标是为每层神经元的每个连接计算每个输入特征的最佳权重。在本章中，我们将了解神经网络如何通过调整权重来学习。如果使用偏差，神经网络也会学习偏差。

### 激活功能

决定一个神经元输出的数学函数叫做*激活函数*。

神经元使用以下线性方程对输入进行操作:

![$$ \mathrm{z}={\mathrm{X}}_0+\sum \limits_{i=0}^{i=n}{w}_i{x}_i $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ4.png)

(方程 5-4)

但是神经元的输出不是等式 5-4 的结果。激活函数对 z 值(根据等式 5-4 计算)进行操作，并确定神经元的输出。

激活函数根据神经元的输入是否与模型预测相关来确定它所连接的神经元是否应该被激活(打开或关闭)。实际上，激活函数将每个神经元的输出标准化到 0 和 1 之间或-1 和 1 之间的范围。

有几个数学函数被用作不同用途的激活。我们将探索 TensorFlow 开箱即用支持的以下激活功能。我们将在下一节学习更多关于张量流的知识。

#### 线性激活函数

线性激活函数根据等式 **f(x) = x0+ w1x1 + w2x2 + w3x3 +通过将权重乘以输入来计算神经元输出....+ wnxn** 。线性激活函数的输出从-∞到+∞，如图 [5-7](#Fig7) 所示。这意味着线性激活函数和没有激活一样好。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig7_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig7_HTML.jpg)

图 5-7

线性激活函数图

线性激活函数有以下两个主要问题，不用于深度学习:

*   深度学习使用一种叫做*反向传播*的方法(稍后会有更多介绍)，这种方法使用一种叫做*梯度下降*的技术。梯度下降需要计算输入的一阶导数，在线性激活的情况下，该导数是常数。常数的一阶导数为零。这意味着它与输入无关。因此，不可能返回并更新输入的权重。

*   如果使用线性激活函数，不管神经网络的层数是多少，最后一层都将是第一层的线性函数。换句话说，一个线性激活函数把你的网络变成一层。这意味着你的网络只能学习输入到输出的线性相关性，这不适合解决复杂的问题，如计算机视觉。

#### 乙状结肠或逻辑激活功能

sigmoid 激活函数使用 sigmoid 函数计算神经元输出，如下所示:

![$$ \sigma \left(\mathrm{z}\right)=1/\left(1+{e}^{-\mathrm{Z}}\right) $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ5.png)

(等式 5-5)

其中 z 使用等式 5-4 计算。

sigmoid 函数总是产生 0 到 1 之间的值。这使得当输入值波动时，输出平滑，没有很多跳跃。另一个优点是，这是一个非线性函数，不会从一阶导数中产生常数值。这使得它适用于基于梯度下降更新权重的反向传播深度学习。参见图 [5-8](#Fig8) 。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig8_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig8_HTML.jpg)

图 5-8

Sigmoid 激活函数图

sigmoid 函数的最大缺点是输出不会在大或小输入值之间变化，这使得它不适合特征向量包含大或小值的情况。克服这一缺点的一种方法是将特征向量归一化，使其值介于-1 和 1 之间或 0 和 1 之间。

从图 [5-8](#Fig8) 中你会注意到的另一个特征是 S 形曲线不是以零为中心。

#### 正切/双曲正切

TanH 类似于 sigmoid 激活函数，只是 TanH 以零为中心。参见图 [5-9](#Fig9) 并注意 S 形曲线穿过原点。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig9_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig9_HTML.jpg)

图 5-9

双曲正切激活函数图(零中心)

双曲正切函数使用以下公式计算神经元输出:

![$$ \tanh \left(\mathrm{z}\right)=\left({e}^{\mathrm{z}}-{e}^{-\mathrm{z}}\right)/\left({e}^{\mathrm{z}}+{e}^{-\mathrm{z}}\right) $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ6.png)

(等式 5-6)

因为双曲正切函数以零为中心，所以它使用具有小、大和中性值的输入进行建模。

#### 整流器线性单元

*整流线性单元* (ReLu)根据等式 5-4 计算的 z 值确定神经元输出。如果 z 的值为正，ReLU 将该值作为输出；否则，它输出为零。ReLU 的输出范围在 0 和+∞之间。ReLU 函数表示如下(参见图 [5-10](#Fig10) ):

![$$ f(z)=\max \left(0,z\right) $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ7.png)

(等式 5-7)

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig10_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig10_HTML.jpg)

图 5-10

ReLU 激活图(取值范围在 0 到无穷大之间)

ReLU 激活功能的优点是计算效率高，并且允许网络快速收敛。此外，ReLU 是非线性的，并且它具有导数函数，这使得它适合于在神经网络学习时用于权重调整的反向传播。

ReLU 函数的最大缺点是，对于零或负输入，函数的梯度变为零。这使得当输入具有负值时，它不适合反向传播。

ReLU 广泛用于大多数计算机视觉模型训练，因为图像像素没有负值。

#### 李奇注意到了

漏 ReLU 提供了 ReLU 的微小变化。它不是使 z 的负值(根据等式 5-3 计算)为零，而是将 z 的负值乘以一个小数值，如 0.01。图 [5-11](#Fig11) 描述了泄漏的 ReLU 输出。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig11_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig11_HTML.jpg)

图 5-11

泄漏 ReLU 图(通过取负值乘以一个小数字来修改 ReLU)

泄漏 ReLU 在负区域具有小斜率，并允许负输入反向传播。

缺点是泄漏 ReLU 的结果与负值不一致。

#### 标度指数线性单位

一个*比例指数线性单元* (SELU)使用下面的等式计算神经元输出:

![$$ f\left(\alpha, x\right)=\uplambda\;\left\{\begin{array}{l}\alpha \left({e}^x-1\right)\;\mathrm{for}\;x&lt;0\\ {}x\kern2.04em \mathrm{for}\;x\ge 0\end{array}\right. $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ8.png)

(等式 5-8)

其中λ的值= 1.05070098，𝞪的值= 1.67326324。这些值是固定的，在反向传播期间不会改变。

图 [5-12](#Fig12) 中的图表显示了 SELU 特性。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig12_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig12_HTML.jpg)

图 5-12

SELU 激活图

SELU 具有“自归一化”特性(参见参考文献 1 中关于 SELU 的原始论文)。SELU 的发明者已经用数学方法证明了 SELU 产生的输出是用平均值 0 和标准差 1 归一化的。

在 TensorFlow 或 Keras 中，如果您通过使用方法`tf.keras.initializers.lecun_normal` *、*将权重初始化方法用作以零为中心的截断正态分布，您将获得所有网络组件的归一化输出，例如各层的权重、偏差和激活。

那么，我们为什么关心网络产生的归一化输出呢？初始化函数`lecun_normal`将网络的参数初始化为正态分布或高斯分布。SELU 也生成标准化输出。这意味着整个网络表现出正常的行为。因此，最后一层的输出也是归一化的。

使用 SELU，学习是高度健壮的，并且允许训练网络有许多层。

由于使用 SELU，整个网络是自标准化的，因此它在计算方面是高效的，并且趋向于更快地收敛。另一个优点是，当输入特征太高或太低时，它克服了爆炸或消失梯度的问题。

#### Softplus 激活功能

*softplus* 激活函数将平滑应用于激活函数值 z(由等式 5-4 计算)。它使用指数的对数如下:

![$$ f(x)=\ln \left(1+{e}^z\right) $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ9.png)

(等式 5-9)

Softplus 也称为 SmoothReLU 函数。softplus 函数的一阶导数为 1/(1+e <sup>-z</sup> ，与 sigmoid 激活函数相同。见图 [5-13](#Fig13) 。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig13_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig13_HTML.jpg)

图 5-13

Softplus 激活图

#### Softmax(软件最大值)

*Softmax* 是一个函数，它采用实数的输入向量，将其归一化为概率分布，并生成范围为(0，1)的输出，输出值之和等于 1。

它最常用于激活分类神经网络的最后一层(输出层)。结果被解释为每个类别的预测概率。

使用以下公式计算 softmax 变换:

![$$ \sigma {\left(\mathrm{z}\right)}_i=\frac{e^{z_i}}{\sum_{j=1}^K{e}^{z_j}}\;\mathrm{for}\;i=1,\dots, K\; and\;\mathrm{z}=\left({z}_1,\cdots, {z}_K\right)\in {\mathrm{\mathbb{R}}}^K $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ10.png)

(等式 5-10)

前面等式的归一化输出始终在 0 和 1 之间。当您添加这些输出时，结果将是 1。

### 前馈

*前馈神经网络*是一种人工神经网络，其中神经元之间的连接不形成循环。到目前为止，我们所了解的网络是一个前馈神经网络。

前馈神经网络是最简单的神经网络。在这个网络中，信息沿一个方向(正向)流动，从输入层开始到隐藏层，一直到输出层。在这个网络中，没有回送或反馈机制。

图 [5-2](#Fig2) 和 [5-3](#Fig3) 所示的示例网络是前馈人工神经网络。

在本书的大部分内容中，我们将使用前馈网络。

### 误差函数

什么是错误？一个*误差*，在机器学习的上下文中，是预期结果和预测结果之间的差异。误差方程可以简化为如下形式:

***误差=预期结果-预测结果***

我们已经知道，神经网络的学习目标是计算权重的优化值。当误差最小(理想情况下为零)时，权重被认为是针对给定数据集优化的。我们已经看到，当网络开始学习过程时，它初始化权重，并通过使用其中一个激活函数来计算每个神经元的输出。然后，它计算误差，调整权重，计算输出，重新计算误差，并与先前计算的误差进行比较，直到找到最小误差。给出最小误差的权重作为最终权重。在这个阶段，网络被认为是“习得的”。

从微积分来看，如果一个函数的一阶导数为零，那么该点的函数不是最小就是最大。找到一阶导数为零的最小点是神经网络训练过程的目标。因此，神经网络必须有一个误差函数，它将计算一阶导数并找到误差函数最小的点(权重和偏差)。这个误差函数应该是什么取决于我们想要训练的模型的类型。误差函数也被称为*损失函数*，或者简称为*损失*。

计算导数和寻找权重最佳值的数学方法超出了本书的范围。我们将探讨一些常用的误差函数以及它们的应用场合。为了让本书专注于我们的学习目标:构建计算机视觉应用，我们不会深入这些误差函数背后的数学。如果你没有任何微积分背景，不要担心。只要确保你明白在解决计算机视觉问题时应该使用什么样的误差函数。

误差函数大致分为以下三类:

*   当我们希望训练模型来预测连续值结果(如股票价格和住房价格)时，会使用回归损失函数。

*   当我们希望训练模型来预测最多两个类别(例如猫对狗或癌症对非癌症)时，会使用二元分类损失函数。

*   当我们的模型需要预测两个以上的类别时，例如对象检测，使用多类别分类损失函数。

以下部分概述了不同的误差函数、它们的用法以及它们兼容的激活函数类型。使用本节作为指南，为您的特定建模工作确定适当的误差函数。

#### 回归损失函数

*误差函数名称*:均方误差(MSE)损失。

*简述*:这是回归问题的默认误差函数。如果目标变量的分布是正态或高斯分布，这是首选的损失函数。

*用在什么地方*:目标变量的分布为正态分布时。

*适用的激活功能* : `model.add(Dense(1, activation="linear"))`

*张量流示例* : `model.compile(loss='mean_squared_error') or model.compile(loss='mse')`

*误差函数名称*:均方对数误差(MSLE)损失。

*简述*:该函数首先计算预测值的对数，并计算均方误差。

*在哪里使用*:当目标变量有一系列值时，以及当预测一个大值时，您可能不希望像均方差那样严重地惩罚一个模型。这通常在模型预测未缩放值时使用。

*适用的激活功能* : `model.add(Dense(1, activation="linear"))`

*张量流示例* : `model.compile(loss='mean_squared_logarithmic_error')`

*误差函数名称*:平均绝对误差损失。

*简要说明*:这是预期值和预测值之间的绝对差值的平均值。

*用在什么地方*:当目标变量是正态分布且有一些异常值时。

*适用的激活功能* : `model.add(Dense(1, activation="linear"))`

*张量流示例* : `model.compile(loss='mean_absolute_error')`

#### 二元分类损失函数

*误差函数名*:二元交叉熵。

*简述*:这是二分类问题的默认损失函数，优先于其他函数。交叉熵计算一个分数，该分数总结了预测类别 1 的实际概率分布和预测概率分布之间的平均差异。分数被最小化，并且完美的交叉熵值被设置为 0。

*哪里用*:目标值在(0，1)范围内时。

*适用的激活功能* : `model.add(Dense(1, activation="sigmoid"))`

*张量流示例* : `model.compile(loss='binary_crossentropy', metrics=['accuracy'])`

*错误功能名称*:铰链丢失。

*简述*:主要用于支持基于向量机的二值分类。

*哪里用*:目标变量在范围(-1，1)内时。

*适用的激活功能* : `model.add(Dense(1, activation="tanh"))`

*张量流示例* : `model.compile(loss='hinge', metrics=['accuracy'])`

*误差函数名*:平方铰链损耗。

*简述*:该函数计算乐谱铰链损耗的平方。它平滑了误差函数的表面，使其在数值上更容易处理。

*哪里用*:目标变量在范围(-1，1)内时。

*适用的激活功能* : `model.add(Dense(1, activation="tanh"))`

*张量流示例* : `model.compile(loss='squared_hinge', metrics=['accuracy'])`

#### 多类分类损失函数

*错误函数名*:多类交叉熵损失。

*简述*:这是多类分类问题的默认损失函数，优于其他函数。交叉熵计算一个分数，该分数总结了预测类别 1 的实际概率分布和预测概率分布之间的平均差异。分数被最小化，并且完美的交叉熵值被设置为 0。

*哪里用*:目标值在集合{0，1，3，4，...，n}，其中每个类被分配一个唯一的整数值。

*适用的激活功能* : `model.add(Dense(4, activation="softmax"))`

*张量流示例* : `model.compile(loss='categorical_crossentropy', metrics=['accuracy'])`

*错误函数名*:稀疏多类交叉熵损失。

*简述*:稀疏交叉熵执行相同的误差交叉熵计算，而不要求目标变量在训练之前是热编码的。

*用在什么地方*:当你在目标中有大量的类的时候，比如预测字典单词。

*适用的激活功能* : `model.add(Dense(100, activation="softmax"))`

*张量流示例* : `model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])`

*误差函数名*:库尔贝克-莱布勒散度(KLD)损失。

*简要描述* : KLD 测量一个概率分布与基线分布的差异。KL 散度损失为 0 意味着分布是相同的。如果使用预测的概率分布来近似期望的目标概率分布，则它确定丢失了多少信息(以比特为单位)。

*在哪里使用*:这是用来解决复杂的问题，比如学习密集特征的自动编码器。如果这用于多类分类，它作为多类交叉熵工作。

*适用的激活功能* : `model.add(Dense(100, activation="softmax"))`

*张量流示例* : `model.compile(loss='kullback_leibler_divergence', metrics=['accuracy'])`

### 优化算法

神经网络的学习目标是确定损失最小的最佳权重(和偏差)。当网络开始学习时，它为每个输入连接分配权重。最初，这些权重很少被优化。通过测量损失(或误差)来确定权重偏离优化的程度。为了确定理想的权重，学习算法优化损失函数，以便找到使损失函数具有最小值的权重。权重(和偏差)被更新，并且该过程被重复，直到不再有优化的余地。优化损失函数的数学函数被称为*优化算法*或*优化器*。

有几种优化算法可以提供不同程度的准确性、速度和并行性。我们将在这一部分探讨一些最受欢迎的。我们将提供介绍性的信息，而不深入这些算法中使用的数学。您将很好地了解在哪里使用哪些优化算法。

#### 梯度下降

梯度下降是一种优化算法，在损失函数(也称为*成本函数*)为零或最小的地方寻找权重。梯度下降是一种寻找最小成本函数的技术。它是这样工作的:

1.  成本函数或误差函数由以下等式表示:

![$$ f(w)=\frac{1}{N}\sum \left({y}_i-{w}_i{x}_i\right) $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ11.png)

(方程式 5-11)

其中***y***<sub>***I***</sub>为实际/已知值***w***<sub>***I***</sub>为第 I 个样本的特征向量**<sub>***I***</sub>对应的权重。***w***<sub>***I***</sub>***x***<sub>***I***</sub>是实际值减去预测值**<sub>***I***</sub>来计算误差或损失。****

 ****从微积分中我们知道，函数在一点的一阶导数给出了函数在该点的斜率或梯度。如果绘制成本函数 f(w)，会看到一条多维曲线(如图 [5-14](#Fig14) )。计算导数以获得梯度，从而确定沿着曲线向哪个方向移动以获得新的权重集。因为目标是最小化成本，所以算法向负梯度的方向移动。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig14_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig14_HTML.jpg)

图 5-14

向最小值梯度移动的成本函数

例如，假设只有一个特征，因此我们只需要计算一个权重(w)。成本函数将如图 [5-14](#Fig14) 中的左图所示。

该算法首先计算初始重量的成本或损失，假设该损失为 f(w)，并假设该损失在图 [5-14](#Fig14) (左图)中的点 1 处计算。

1.  然后，该算法计算梯度(增量)并沿曲线向下移动；方向由负梯度决定。

2.  当它下降时，算法使用以下公式计算新的权重:

![$$ \boldsymbol{weight}=\boldsymbol{weight}+\boldsymbol{alpha}\ast \left(\hbox{-} \boldsymbol{delta}\right)=\boldsymbol{weight}\hbox{-} \boldsymbol{alpha}\ast \boldsymbol{delta} $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ12.png)

(方程式 5-12)

这里，α被称为*学习率*。学习率决定了梯度沿曲线下降到达最小点的步长。

1.  使用新的权重值再次计算误差，并且重复该过程，直到算法找到最终的最小成本。

##### 局部和全局最小值

为了简单起见，我们只考虑一个特征，因此只有一个权重。但是在实践中，可能有数十甚至数百个特征需要学习权重。图 [5-14](#Fig14) 右侧的图像显示了需要优化多个砝码时的误差曲线。在这种情况下，曲线可能有多个出现最小值的点，称为*局部最小值*。梯度下降算法的目标是找到全局最小值以优化权重。

##### 学习率

如等式 5-12 所示，参数`alpha`称为*学习率*。学习率决定了梯度下降算法沿着曲线向下移动以找到全局最小值的步长的大小。

这个学习率的值应该是多少？学习率的大值可能错过最小值点，并且可能来回振荡，并且永远不会找到最小值。另一方面，学习率的小值将需要许多步骤来达到最小点。

学习率小，会让学习变慢。图 [5-15](#Fig15) 显示了大学习率和小学习率的影响。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig15_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig15_HTML.jpg)

图 5-15

大小学习率的影响

所以一定要适当设定学习率。学习率的实用范围是 0.01 到 0.1。我们通常从这个范围内的学习率开始，并根据需要进行调整。

##### 正规化

如果其中一个特征的权重比所有其他特征都高，会发生什么？该特征将具有更高的权重，并且将在整体预测中具有显著的影响。正则化是一种控制一个或几个大权重的效果的方法。我们在成本函数中添加另一个参数，称为*正则化*，以平衡可能导致我们的预测受到严重影响的过多权重。正则化参数惩罚大的权重以减少其影响。

现在让我们保持简单。当我们编写一些代码来训练我们自己的模型时，我会解释正则化。

#### 随机梯度下降

梯度下降在每一步和每一次迭代中计算整个训练样本的梯度。这是大量的计算，并且它们需要时间来收敛。根据训练集的大小，在单台机器上运行该算法在计算上可能是不可行的，因为它必须将全部数据存储在内存(RAM)中。此外，对于并行计算，处理不能是分布式的。*随机梯度下降* (SGD)克服了这些问题。

SGD 计算训练集的一个小子集的梯度，它可以很容易地适应内存。

SGD 是这样工作的:

1.  随机化输入数据集以消除任何偏差。

2.  计算随机选择的单个数据或小批量数据的梯度。

3.  使用公式**weight = weight-alpha * delta**更新权重。

通常，SGD 中的权重更新是针对几个训练示例而不是单个示例计算的，因为这减少了导致稳定收敛的权重的方差。128 或 256 的小批量是一个很好的起点。对于不同的应用程序、架构和计算机硬件容量，最佳批量可能会有所不同。

##### 面向分布式和并行计算的 SGD

如果您有一个大的训练数据集，您可以将随机化的训练集分成小的小批。这些小批量可以分布在集群架构中的多台计算机上。SGD 可以在拥有少量数据的单个计算机上独立并行地计算权重。可以将来自各个计算机的结果合并到中央计算机，以获得最终的优化重量。

SGD 还可以通过在具有多个 CPU 或 GPU 的单台计算机中使用并行处理来优化权重。

使用 SGD 算法计算优化权重的分布式和并行操作有助于更快地收敛。

##### 带动量的 SGD

如果你绘制你的成本函数，你会看到峡谷形的曲线，有陡峭的壁和狭窄的底部，你应该考虑使用 SGD 的动量。峡谷在局部极小点周围更为突出。在这种情况下，SGD 在最小值附近波动，可能达不到目标。标准 SGD 通常会延迟转换，尤其是在几次迭代之后。参见图 [5-16](#Fig16) 。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig16_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig16_HTML.png)

图 5-16

带动量的 SGD

动量是一种通过控制梯度运动来控制振荡的方法。动量更新由下面的等式给出:

![$$ v= yv+\mathrm{alpha}\ast \mathrm{delta} $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equ13.png)

(等式 5-13)

其中，δ是使用 SGD 计算的梯度，α是学习率。

𝒗是与参数(或权重)具有相同维数的速度向量。

𝞬的值在(0，1)范围内，通常默认取 0.9。

最后，使用下面的等式更新权重:

![$$ \boldsymbol{weight}=\boldsymbol{weight}+\boldsymbol{v} $$](../images/493065_1_En_5_Chapter/493065_1_En_5_Chapter_TeX_Equb.png)

#### 自适应梯度算法

梯度下降和 SGD 要求我们手动设置和调整学习率。如果学习太高，算法会错过最小点，如果太低，算法会花很多时间收敛。找到一个完美的学习率是一个手工的过程。当神经网络具有多维性时，选择正确的学习速率尤其困难。一种选择是为每个维度设置不同的学习率。然而，大多数神经网络都有数百甚至数千个维度，这使得人工选择学习速率几乎是不可能的。

Adagrad 通过查看过去来计算每个参数的正确学习率，从而解决了这个问题。它对不常见的特征产生较大的学习率，而对较高频率的特征产生较低的学习率。这意味着每个参数都有自己的学习率，可以提高稀疏梯度问题的性能。

Adagrad 非常适合处理稀疏数据，例如在计算机视觉或 NLP 中。

Adagrad 的最大缺点之一是自适应学习率随着时间的推移会变得非常小。

#### RMSProp

还记得有气势的 SGD 吗？动量的引入控制了较陡曲线中的梯度运动。RMSProp 为 SGD 提供了动量增强。它限制渐变在垂直方向上的移动。这样想:如果你有一个陡峭的曲线，水平方向的小运动会引起垂直方向的大运动。RMSProp 控制垂直移动，以便垂直和水平方向上的移动不会不均匀，并导致更快地找到最小点。

#### 适应力矩(Adam)

Adam 优化算法专为深度学习而设计，是首选的优化器。它将 SGD 与 momentum 和 RMSProp 结合在一起。Adam 根据训练数据迭代更新网络权重。

Adam 没有像在 RMSProp 中那样基于平均一阶矩(平均值)来调整参数学习率，而是利用梯度的二阶矩的平均值。

亚当背后的数学问题超出了本书的范围(再次强调，要专注于本书的核心主题)。有关如何计算和更新梯度的更多详细信息，请参见位于 [`https://arxiv.org/pdf/1412.6980.pdf`](https://arxiv.org/pdf/1412.6980.pdf) 的原始论文。

该文件描述了自动协调机制的以下好处:

*   易于实施

*   计算效率高

*   几乎没有内存需求

*   对梯度的对角重标度不变

*   非常适合在数据和/或参数方面很大的问题

*   适用于非稳定目标

*   适用于有噪音和/或稀疏梯度的问题

*   超参数具有直观的解释，通常几乎不需要调整

### 反向传播

为了训练一个神经网络，我们需要以下三样东西:

*   输入数据或输入要素

*   一种前馈多层神经网络

*   误差函数

网络为每个输入特征分配初始权重。使用优化算法，例如 SGD 或 Adam，误差函数被优化以计算最小误差，并且权重被更新。

多层感知器至少包含三层:输入层、隐藏层和输出层。可以有多个隐藏层。

在前向网络中，神经元的输出是正向计算的，从第一个隐藏层开始，然后是第二个隐藏层，依此类推，最后是输出层。

下一步是估计误差，以便更新权重。在反向传播方法中，首先在最后一层计算权重的梯度，最后计算第一层的梯度。来自一个层的梯度的部分计算被重新用于前一层的梯度的计算。误差信息的这种反向流动允许有效计算每一层的梯度。换句话说，梯度计算不是在每一层独立完成的。

为什么首先计算最后一层的误差？原因很简单，隐藏层没有目标变量。输出图层映射到标注数据集的目标变量。因此，首先计算最后一层的误差非常有意义。

本节概述了神经网络的工作原理以及不同的算法在幕后的工作原理。我们还发现有几个参数，如学习率和动量，我们可以控制来调整我们的训练。我们可以设置或调整来训练一个好模型的参数被称为*超参数*。我们将在本章后面了解更多关于超参数的知识。

在下面几节中，我们将编写代码来实现本章前面几节中涉及的一些概念。我们将编写 Python 代码并使用 TensorFlow 来完成示例。我们将从 TensorFlow 的高级介绍开始，涵盖与计算机视觉相关的特性和功能。我们将在本章的剩余部分使用张量流代码，我们将在实现神经网络概念时提供相关的解释。

## 张量流简介

TensorFlow 是一个面向端到端机器学习的开源平台。它提供了一个高级且易于使用的 API 来创建机器学习模型。TensorFlow 是 Keras 的执行引擎，Keras 是用 Python 编写的高级神经网络 API。

在写这本书的时候，TensorFlow 版本 2 (TF2)是可用的。但本书中涉及的一些核心概念(如对象检测)仅适用于 TensorFlow 版本 1 (TF1)。在大多数情况下，我们将使用 TF2 和 TF1 主要用于对象检测。

### tensorflow 安装

如果您遵循了第 [1](01.html) 章中的说明，TensorFlow 和 Keras 应该已经安装在您的工作环境中。如果没有，检查第 [1](01.html) 章，并遵循 TensorFlow 的安装说明。

### 如何使用 TensorFlow

要在代码中使用 TensorFlow，必须按如下方式导入它:

```
import tensorflow as tf

```

您可以通过以下方式访问 Keras API:

```
tf.keras

```

在我们深入研究神经网络之前，让我们理解张量流的一些术语。

### 张量

*张量*是包含基本数据类型的 *n* 维数组的数据结构。

*   如果 *n* 的值为 0，则称为*标量*，标量的秩为 0 或 0 维。

*   如果 *n* 的值为 1，则称为*向量*，向量的秩为 1 或 1 维。

*   如果 *n* 的值为 2，则称之为*矩阵*，矩阵的秩为 2 或 2 维。

*   如果 *n* 的值为 3 或更大，则称为*张量*。取决于 n 的值，它的秩是 3 或更多。

因此，张量是向量和矩阵在高维空间的推广。表 [5-1](#Tab1) 总结了标量、矢量、矩阵和张量之间的区别。

表 5-1

标量、向量、矩阵和张量的定义

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

数据结构

 | 

维度或等级

( *n* 的值)

 | 

例子

 |
| --- | --- | --- |
| 数量 | Zero | `scalar_s = 231` |
| 矢量 | one | `vector_v = [1,2,3,4,5]` |
| [数]矩阵 | Two | `matrix_m = [[1,2,3],[4,5,6],[7,8,9]]` |
| 张量 | 3 或更多 | `tensor_3d = [``[[1,2,3],    [4,5,6],    [7,8,9]],``[[11,12,13], [14,15,16], [17,18,19]],``[[21,22,23], [24,25,26], [27,28,29]],``]` |

在内部，TensorFlow 定义、操作和计算张量。它提供了一个`Tensor`类，可通过以下方式访问:

```
tf.Tensor

```

`Tensor`类具有以下属性:

*   数据类型，例如`uint8`、`int32`、`float32`或`string`。张量的每个元素必须是相同的数据类型。

*   一个形状，它是维度的数量和每个维度的大小。

### 可变的

TensorFlow 有一个名为`Variable`的类，可以通过使用`tf.Variable`来访问。`tf.Variable`类表示一个张量，它的值由读取和修改等操作来操作。在本章的后面，你将了解到`tf.keras`使用`tf.Variable`来存储模型参数。清单 [5-1](#PC5) 展示了如何使用变量的 python 示例。

### 常数

TensorFlow 还支持常量，常量的值一旦初始化就不能更改。要创建常数，请调用此函数:

```
tf.constant(value, dtype=None, shape=None, name="Const")

```

在哪里

*   `value`是实际值或设置为常量的列表。

*   `dtype`是由常数表示的结果张量的数据类型。

*   `shape`是可选参数，表示结果张量的维数。

*   `name`是张量的名称。

如果不指定数据类型，`tf.constant()`将从常量的值中推断出它。

函数`tf.constant()`返回一个常量张量。

清单 [5-1](#PC5) 展示了一个创建`tensor`变量的简单代码示例。

```
Filename: Listing_5_1.py
1    import tensorflow as tf
2
3    # create a tensor variable with zero filled with default datatype float32
4    a_tensor = tf.Variable(tf.zeros([2,2,2]))
5
6    # Create a 0-D array or scalar variable with data type tf.int32
7    a_scalar = tf.Variable(200, tf.int32)
8
9    # Create a 1-D array or vector with data type tf.int32
10   an_initialized_vector = tf.Variable([1, 3, 5, 7, 9, 11], tf.int32)
11
12   # Create a 2-D array or matrix with default data type which is tf.float32
13   an_initialized_matrix = tf.Variable([ [2, 4], [5, 25] ])
14
15   # Get the tensor's rank and shape
16   rank = tf.rank(a_tensor)
17   shape = tf.shape(a_tensor)
18
19   # Create a constant initialized with a fixed value.
20   a_constant_tensor = tf.constant(123.100)
21   print(a_constant_tensor)
22   tf.print(a_constant_tensor)

Listing 5-1Creating a Tensor Variable

```

清单 [5-1](#PC5) 的第 1 行导入了 TensorFlow 包。第 4 行创建了一个用零填充的形状为[2，2，2]的张量。默认情况下，它创建一个数据类型为`tf.float32`的张量(如果在创建张量时没有指定数据类型，它将默认为`float32`)。但是，数据类型是从初始值推断出来的。

第 7 行创建了一个类型为`int32`的标量数据，第 10 行创建了一个数据类型为`int32`的向量，第 13 行创建了一个默认数据类型为`float32`的 2×2 矩阵。

第 16 行显示了如何获得张量的秩(见表 1-1)，第 17 行显示了如何获得形状。

第 20 行创建了一个常量张量，其值初始化为 123.100。它的数据类型由它的初始化值来解释。

第 20 行和第 21 行显示了打印张量的两种不同方式。执行代码，注意两个`print`语句的区别。

要计算张量，使用`Tensor.eval()`方法，该方法创建一个等价的 NumPy 数组，其形状与张量相同。请注意，仅当默认`tf.Session`处于活动状态时，张量才会被评估。

这本书不是关于张量流的。我们将只涵盖与编写构建计算机视觉和深度学习模型的代码相关的功能。您应该访问 TensorFlow 官方网站，并学习使用 TensorFlow 的 Python 函数。下面是 API 规范: [`https://www.tensorflow.org/api_docs/python/tf`](https://www.tensorflow.org/api_docs/python/tf) 。

我们将在接下来的几乎所有章节中重新讨论张量流。

## 我们第一个具有深度学习的计算机视觉模型:手写数字的分类

我们现在准备建立和训练我们的第一个计算机视觉模型。我们将从著名的“Hello World”类型的深度学习模型开始，并学习如何构建一个简单的多层感知器分类器。当你完成这一部分的时候，你将会有一个真正工作的计算机视觉模型。和以前一样，我们将逐行解释我们将编写的 TensorFlow 代码。在我们开始编写我们的第一个模型之前，让我们了解我们要构建什么以及步骤是什么。

我们的目标是使用人工神经网络训练一个模型来分类手写数字(0 到 9)的图像。

我们将建立一个神经网络来执行监督学习。对于任何监督学习，我们都需要一个包含标记数据的数据集。换句话说，我们需要已经标记了它们所代表的数字的图像。例如，如果图像包含手写数字 5，它将被标记为 5。同样，我们要在培训中使用的所有图像都必须标有相应的标签。

我们的数据集有十个类，每个类对应一个数字。类索引从 0 开始。因此，我们的类在范围(0，9)内。

标记的影像数据集通常以 70:30 的比例分为两部分。

*   *训练集*:70%的标记图像用于实际训练。为了获得好的结果，我们应该确保训练数据是平衡的，这意味着它几乎平等地代表了所有的类。

    如果你的训练集没有一个平衡的类呢？多数阶级会对模型产生更大的影响，而你的少数阶级可能永远不会或很少被预测到。

    为了平衡你的类，你可以做过采样或欠采样。在过采样中，您应该添加更多少数类的图像，并使它们接近等于多数类。在欠采样中，从多数类中移除图像，使其在数量上接近少数类。

    还有其他的合成方法来平衡你的类，但是不推荐用于计算机视觉。合成少数过采样技术(SMOTE)就是这样一种方法，但不推荐用于计算机视觉。然而，在 [`https://arxiv.org/pdf/1710.05381.pdf`](https://arxiv.org/pdf/1710.05381.pdf) 发表的研究论文得出结论，欠采样的性能与过采样相当，因此在计算效率方面应该是首选。

*   *测试集*:标记数据的 30%作为测试集。来自测试集的图像通过训练的模型，预测的结果与标签进行比较，以评估模型的准确性。

    确保测试集不具有也存在于训练集中的相同图像是很重要的。同样，测试集以相等的比例包含所有的类也是很重要的。

我们将执行以下任务来构建模型:

1.  从 [`https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz`](https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz) 下载包含手写数字及其标签的图像数据集。

2.  配置一个有四层的多层感知器分类器:输入层、两个隐藏层和输出层。

3.  用训练集拟合 MLP 模型。拟合模型意味着训练模型。

4.  使用测试集评估已训练的模型。

5.  使用不同数据集上的模型进行预测(不用于定型集或测试集中)并显示结果。

最后，我们到达了这样一个点，我们逐行查看 TensorFlow 代码，以了解如何训练基于深度学习的模型，用于对手写数字进行分类的计算机视觉。

让我们来探索清单 [5-2](#PC6) ，它演示了如何训练一个基于深度学习的计算机视觉模型。

```
Filename: Listing_5_2.py
1    import tensorflow as tf
2    import matplotlib.pyplot as plt
3    # Load MNIST data using built-in datasets download function
4    mnist = tf.keras.datasets.mnist
5    (x_train, y_train), (x_test, y_test) = mnist.load_data()
6
7    #Normalize the pixel values by dividing each pixel by 255
8    x_train, x_test = x_train / 255.0, x_test / 255.0
9
10   # Build the 4-layer neural network (MLP)
11   model = tf.keras.models.Sequential([
12    tf.keras.layers.Flatten(input_shape=(28, 28)),
13    tf.keras.layers.Dense(128, activation='relu'),
14    tf.keras.layers.Dense(60, activation='relu'),
15    tf.keras.layers.Dense(10, activation='softmax')
16   ])
17
18   # Compile the model and set optimizer,loss function and metrics
19   model.compile(optimizer='adam',
20                loss='sparse_categorical_crossentropy',
21                metrics=['accuracy'])
22
23   # Finally, train or fit the model
24   trained_model = model.fit(x_train, y_train, validation_split=0.3, epochs=100)
25
26   # Visualize loss  and accuracy history
27   plt.plot(trained_model.history['loss'], 'r--')
28   plt.plot(trained_model.history['accuracy'], 'b-')
29   plt.legend(['Training Loss', 'Training Accuracy'])
30   plt.xlabel('Epoch')
31   plt.ylabel('Percent')
32   plt.show();
33
34   # Evaluate the result using the test set.\
35   evalResult = model.evaluate(x_test,  y_test, verbose=1)
36   print("Evaluation", evalResult)
37   predicted = model.predict(x_test)
38   print("Predicted", predicted)

Listing 5-2Four-Layer MLP for Classification of Images with Handwritten Digits

```

第 1 行导入 TensorFlow 包。这个包提供了对 Keras 深度学习库和其他几个与深度学习相关的功能的访问。第 2 行导入 matplotlib。

第 4 行初始化`keras.datasets.mnist`模块。该模块提供了一个内置功能来下载修改后的国家标准与技术研究所(MNIST)手写数字图像数据。MNIST 数据库是大量手写数字的集合，广泛用于训练各种计算机视觉系统。数据库在 [`http://yann.lecun.com/exdb/mnist/`](http://yann.lecun.com/exdb/mnist/) 可用。

第 5 行下载 MNIST 数据集。`mnist`模块中的`load_data()`函数下载数字数据库并返回 NumPy 数组的元组。默认情况下，它会将数据库下载到您的主目录位置`~/.keras/datasets`，默认文件名为`mnist.npz`。您可以通过提供绝对文件路径下载到任何其他位置，例如在函数`load_data(path='/absolute/path/mnist.npz')`中。确保该目录已经存在。

`load_data()`函数返回一组 NumPy 数组，如下所示:

*   这个 NumPy 数组包含我们将用于训练的图像的像素值。

*   `y_train`:这个 NumPy 数组包含了`x_train`中每个图像的标签。

*   `x_test`和`y_test`:这些是测试数据集的图像和相应标签的像素值。

在第 8 行，我们知道图像的像素值范围从 0 到 255。我们需要标准化像素值，使它们在 0 和 1 之间。将每个像素除以 255 将使其正常化，如第 8 行所示。`x_train`和`x_test` NumPy 数组被标量 255 除，以归一化这些数组。

在本例中，我们使用 TensorFlow 中的内置函数下载公开可用的数据集。如果您的本地磁盘或任何分布式文件系统中有数据，TensorFlow 会提供加载数据的函数。我们将在本章的后面演示如何从本地文件系统加载文件。

在第 11 行到第 16 行，虽然这是一条语句，但为了清楚起见被分成了多行，这是我们定义神经网络的地方。让我们看看它的不同部分。

*   这是一个 TensorFlow 类，它提供了创建神经网络层的功能。在这个例子中，我们创建了四个层，并作为数组传递给`Sequential`类的构造函数。

*   `tf.keras.layers`:这个模块提供 API 来创建不同类型的神经网络层。在本例中:
    *   `tf.keras.layers.Flatten(input_shape=(28, 28))`通过初始化`Flatten()`功能定义输入层。我们的输入图像是单通道的 28×28 像素。此函数的参数是输入形状。这个 flatten 函数将在输入层创建 28×28 = 784 个神经元。请记住，输入层中神经元的数量与特征的数量相同(如果使用了偏差，则加 1)。我们的数字图像为 28×28 像素，每个像素值作为一个输入特征；因此，这一层的节点数是 784。在本章的后面，我们将会看到更多具有复杂特征的例子。现在让我们把事情简单化。

    *   `tf.keras.layers.Dense`在神经网络中创建密集层。密集层有两个重要的参数:神经元的数量和激活函数。请注意，在清单 [5-2](#PC6) 中，我们的神经网络有三个密集层。
        *   *隐层 1* :神经元个数为 128，激活函数为`relu`。

        *   *隐层 2* :神经元个数为 60，激活函数为`relu`。

        *   *输出层(最后一层)*:神经元个数为 10，激活函数为 softmax。

为什么激活功能在隐藏层`relu`？回想一下“激活功能”部分和图 [5-10](#Fig10) 中的内容，即`relu`总是产生从 0 到无穷大范围内的输出，并且不产生任何负数。归一化后的像素值在范围(0，1)内。因此，RELU 非常适合这一层。

softmax 为什么在输出层？记住，softmax 生成神经元输出的概率分布。输出层生成每个类别的概率。在本例中，对于每个输入图像，它将生成 10 个概率，每个类别一个。这些概率的总和将等于 1。具有最高概率的类别通常被作为输入图像的预测类别。

为什么我们在输出层只有十个神经元？是因为我们只有十位数要预测，分类问题的输出层要有和要预测的类别数一样多的神经元。

第 19 行到第 21 行调用`compile()`函数，用我们之前提供的配置构建神经网络。函数`compile()`采用以下内容:

*   `optimizer = ‘adam'`:试图寻找损失函数最小值的优化函数的名称。

*   `loss = ‘sparse_categorical_crossentropy'`:将被优化的损失函数。这是一个多类分类，我们选择的是`sparse_categorical_crossentropy`损失函数。

*   `metrics= [‘accuracy']`:在训练和测试过程中，模型要评估的指标列表。因为我们有一个单一的输出模型，并且这是一个分类问题，所以我们在这个列表中只传递一个度量，即“准确性”。

第 24 行实际上符合这个模型。当这一行执行时，模型开始学习。这需要这些参数:

*   `x_train`:像素的归一化值的 NumPy 表示

*   `y_train`:标签数量

*   `validation_split = 0.3`，告知算法保留 30%的训练数据用于验证

*   `epochs = 100`，训练迭代次数

如果您想要使用您的测试数据集，或者您可以访问的任何其他数据集来进行验证，而不是使用`validation_split`，那么您可以使用`validation_data=(x_test, y_test)`。

问题是，我们应该使用多少迭代或历元来训练我们的模型？通常，神经网络需要多次迭代来学习。这是您需要调整的参数之一。当您的模型开始学习时，您将看到控制台中打印的输出(例如，如果您在 PyCharm 中执行代码，则是 PyCharm 控制台)。它显示了每个历元的损失和精度。随着每个时代的到来，损耗应该下降，而精度应该上升。如果您开始注意到损失不再减少或精度不再增加，您应该将您的历元值设置在该水平。

图 [5-17](#Fig17) 显示了具有 100 个时期的样本训练输出。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig17_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig17_HTML.jpg)

图 5-17

每个历元的损失和精度的控制台输出示例

在第 27 行到第 32 行，我们想要绘制损失对时期和准确度对时期的图表，以了解我们的训练有多好。我们训练过的模型维护了每个时期的损失和准确度的历史，可通过使用`history[‘loss']`和`history[‘accuracy']`来访问。

在图 [5-18](#Fig18) 中，您会注意到损耗(红线所示)随着每个时期而减少，大约在第十个时期开始变得平缓。最有可能的是，更多的迭代不会进一步减少损失。因此，将 epoch 设置为大约 10，这样可以避免更多的计算。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig18_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig18_HTML.jpg)

图 5-18

训练损失和精度与历元的关系图

类似地，精度水平增加，并在几个时期后变得平坦。这两者——损失和准确性——将帮助您确定训练神经网络的迭代次数。

您可以通过调用`history.keys()`来打印`History`对象中的所有键。您可能还想绘制`val_acc`和`val_loss`图，看看您的模型如何评估 30%的验证数据。

第 35 行根据测试数据集评估模型。我们使用接受这些参数的`evaluate()`函数:

*   包含所有测试图像的归一化像素值的 NumPy

*   包含测试数据集标签的数量

*   `verbose =1`作为可选参数打印输出

正如您从示例输出中看到的，在图 [5-19](#Fig19) 中，我们的模型在测试数据集上的准确度是 0.9787 或 97.87%，这被认为是一个相当好的模型。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig19_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig19_HTML.png)

图 5-19

评估输出

图 [5-19](#Fig19) 显示了`evaluate()`功能的样本输出。我们的模型评估的总体准确率为 97.87%，损失为 0.2757%。

如果您有一个测试数据集，就像我们在这个例子中拥有的一样，您不需要像第 24 行那样保留 30%的训练集。如果您想像我们在第 35 行中那样使用测试数据执行评估，那么参数`validation_split = 0.3`是可选的。

在第 37 行，到目前为止，我们构建、训练和评估了神经网络。第 37 行使用训练的模型来预测在模型训练中没有使用的输入图像的类别。任何新的图像(具有归一化的像素值)都可以被输入到模型中以预测其类别。

为了预测一个类，我们使用函数`model.predict()`，它将图像 NumPy 作为一个参数。

`predict()`函数的输出是一组数组。这个数组的元素是每个类的概率。最大概率的指标是该图像的预测类别。

例如，带有手写数字的输入图像得到预测概率，如图 [5-20](#Fig20) 所示。从零开始，第六个指数(用黄色突出显示)的概率最大，为 0.99844。因此，如图所示，输入图像的预测类别是 7，这与手写数字相匹配。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig20_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig20_HTML.png)

图 5-20

输入图像和预测概率

恭喜你！你为计算机视觉建立并训练了你的第一个神经网络。在以下部分中，我们将学习如何评估我们的模型是好是坏，以及如何调整参数，使我们的模型在更低的损耗和更高的精度方面更好。

## 模型评估

在我们训练一个模型之后，我们通过分析损失和准确性来对它进行评估。这种损失和准确性是基于训练数据计算的。即使精确度很高，损失很小，我们也不能确定当一组新的数据输入模型时，模型会以同样的精确度进行预测。通过输入测试数据来分析模型的性能是很重要的，这些数据必须不同于训练集。下面是几种常用的评估方法，都在实践中。

### 过度拟合

过度拟合模型可以很好地学习训练数据，因此它在训练数据中表现良好，但在评估和测试数据中表现不佳。例如，如果具有训练数据的模型的准确性很高(比如 97%)，但是具有测试集或验证集的模型的准确性较低(比如 70%)，则该模型被称为过度拟合。图 [5-21](#Fig21) 描述了测试精度低于训练精度的过拟合情况。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig21_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig21_HTML.jpg)

图 5-21

过度拟合的示例

如何避免过度拟合？

有几种方法可以控制或避免过度拟合。

*   *正则化*:我们已经了解了什么是正则化，以及正则化如何影响模型。

*   *辍学*:辍学也是一种正规化技术。使用 dropout 时，神经元会被随机丢弃，这意味着被丢弃的神经元的输出不会作为下一层的输入。辍学是暂时的，仅适用于特定的通行证。这意味着权重更新不会应用于该特定过程中临时移除的神经元。

    在 TensorFlow 中，通过添加一个称为辍学层的层，并指定辍学比率或概率(例如，20%)来实现辍学。可以在输入图层或隐藏图层中添加下降图层。出于大多数实际目的，我们保持这个丢失概率很小，以避免丢失重要的特性。

    在清单 [5-2](#PC6) 中，我们可以添加一个脱落层，如清单 [5-3](#PC7) 所示。

```
....
model = tf.keras.models.Sequential([
 tf.keras.layers.Flatten(input_shape=(28, 28)),
 tf.keras.layers.Dense(128, activation='relu'),
 tf.keras.layers.Dropout(0.2),
 tf.keras.layers.Dense(60, activation='relu'),
 tf.keras.layers.Dense(10, activation='softmax')
])
.....

Listing 5-3Code Fragment to Show the Dropout Layer

```

#### 欠拟合

当一个模型不能从训练数据中捕捉到潜在的趋势时，它就被认为是不适合的。欠拟合模型仅仅意味着模型不能很好地拟合数据。当我们有一个小的数据集，或者数据集不是我们试图建模的实际场景的真实表示时，通常会发生这种情况。欠拟合模型的准确性对训练集和测试集都不好。应该避免这种模式。避免拟合不足的一个好方法是向训练集添加更多数据，或者拥有足够的数据，这些数据包含您试图建模的所有变化和趋势。此外，选择正确特征的特征工程有助于减少欠拟合。

#### 评估指标

还有其他重要的度量标准，你应该看看，以评估你的模型质量。这里对它们进行了描述。通过将预测结果与标签值进行比较，从测试数据集中计算出这些指标。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
|   | 

猫(预测)

 | 

狗(预测)

 |
| --- | --- | --- |
| cat(实际) | Eighty | Ten |
| 狗(实际) | eight | Ninety-two |

*   *真阳性率(TPR)* *或灵敏度*:如果预测值与标签值匹配，则称为*真阳性* (TP)。TPR 定义如下:

    TPR =所有 TPs 的总数/所有阳性病例的总数

*   *真阴性率【TNR】**或特异性*:TNR 定义如下:

    TNR =真阴性总数/阴性病例总数

*   *【FPR】**误报率*:FPR 定义如下:

    FPR =假阳性病例总数/阴性病例总数

*   *假阴性率* *或漏检率*:假阴性率定义如下:

    FNR =假阴性病例总数/阳性病例总数

*   *混淆矩阵*:混淆矩阵也叫*误差矩阵*。它以网格的形式显示了每个类的积极和消极的数量。例如，如果您有两个类，`dog`和`cat`，混淆矩阵可能如下所示:

在这个例子中，`cat`类有 80 个真阳性、10 个假阳性和 8 个假阴性。类似地，对于`dog`类，有 92 个真阳性、8 个假阳性和 10 个假阴性。

清单 [5-4](#PC8) 显示了计算混淆矩阵并以数组形式显示的代码示例。

```
.....

40   confusion = tf.math.confusion_matrix(y_test, np.argmax(predicted, axis=1), num_classes=10)
41   tf.print(confusion)
.....

Listing 5-4Confusion Matrix Calculation

```

清单 [5-4](#PC8) 是清单 [5-2](#PC6) 的延伸。清单 5-2 的第 37 行使用测试数据集从模型中进行预测。输出是每个输入的概率的 NumPy 数组。`np.argmax(predicted, axis=1)`获取数组中最大概率的索引。索引代表预测的类别。

在清单 [5-4](#PC8) 中，`tf.math.confusio_matrix()`计算混淆矩阵。它采用这些参数:

*   `x_test`:测试数据集的图像特征的数量

*   `np.argmax(predicted, axis=1)`:预测类

可选参数`num_classes = 10`表示我们希望模型预测的类的数量。

`confusion_matrix()`函数返回一个张量。如果你使用`print(confusion)`直接打印这个张量，它不会显示这个张量的值。您将需要执行张量，以便它在显示到控制台之前计算所有的值。

清单 [5-4](#PC8) 中的第 40 和 41 行显示了如何生成混淆矩阵，并使用`tf.print()`语句在控制台上打印出来。

图 [5-22](#Fig22) 显示了我们在本例中使用的测试集的样本混淆矩阵。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig22_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig22_HTML.png)

图 5-22

混淆矩阵输出样本

*   *精度*:精度定义为真阳性总数与预测阳性总数之比。

    精度=真阳性数/预测阳性数

    =真阳性/(真阳性+假阳性)

    = TP/(TP + FP)

    理想情况下，您的模型不应该有任何假阳性，即 FP = 0。那么，precision = 1，即 100%。换句话说，越精确，模型越好。

*   *召回*:召回是真阳性总数与实际阳性总数之比。召回率和真阳性率是一样的。计算召回的公式如下:

    召回数=真阳性总数/阳性总数

    =真阳性总数/(真阳性总数+假阴性总数)

    = TP / (TP +联合国)

    理想情况下，您的模型不应有任何假阴性，即 FN = 0。那么，回忆= 1，或者 100%。所以召回越多，模型越好。

*   *F1 得分*:从精确度和召回率两方面来看，我们看到这两个指标在理想模式下都应该接近 100%。如果精度和召回率中的一个比另一个小，你会如何判断你的模型？F1 分数有助于做出决定。F1 分数结合了精确度和召回率，以获得有助于判断我们的模型好坏的综合指标。F1 分数是精确度和召回率的调和平均值，使用以下公式计算:

    F1-Score = 2 ×精度×召回/(精度+召回)

*   准确度:准确度定义如下:

    *准确度* = (TP + TN) /总样本数

    = (TP + TN)/ (T + N)

    = （TP + TN）/ （TP + TN + FP + FN）

这些指标帮助我们决定该模型是否适合在生产中部署，或者调整参数并重新训练该模型。

## 超参数

超参数是我们在学习过程开始之前设置的神经网络模型的参数。这些被认为是外部参数，而不是算法根据训练数据计算的参数。在训练模型时，算法不能推断超参数。这些超参数影响模型的整体性能，包括准确性和训练执行时间。

以下是为计算机视觉训练神经网络时可能需要调整的一些常见超参数:

*   网络中的隐藏层数

*   隐藏层中的神经元数量

*   辍学率和学习率

*   优化算法

*   激活功能

*   损失函数

*   时期或迭代次数

*   验证集的分割

*   批量

*   动力

### 张量板

通常，您需要了解机器学习工作流运行时发生了什么。TensorBoard 是一个工具，它将帮助您可视化您的机器学习测量和指标。使用 TensorBoard，您将能够跟踪实验指标，如损失和准确性，可视化模型图，将嵌入投影到低维空间，等等。

TensorBoard 提供了一个 HParams 仪表板，帮助我们确定最佳实验或最有希望的超参数集。我们将采用我们在上一节中得出的同一个神经网络示例，并可视化各种超参数，以了解我们应该如何调整它们。

在您完成以下示例之前，请确保您安装了 TensorBoard。如果您在 virtualenv 命令提示符下，只需运行以下命令来检查 TensorBoard 安装:

```
(cv) username $: tensorboard --logdir mylogdir

```

如果一切顺利，您应该会看到类似这样的输出:

```
TensorBoard 2.1.0 at http://localhost:6006/ (Press CTRL+C to quit)

```

将您的浏览器指向`http://localhost:6066`，您应该会看到 TensorBoard web UI。

### 超参数调谐实验

清单 [5-5](#PC11) 中的代码示例演示了一个简单的实验，对于一个简单的神经网络只有三个超参数。出于学习目的，我们保持了示例的简单性。

我们的目标是用以下参数进行实验:

*   第一个隐藏层中的神经元数量

*   优化功能

*   辍学率

实验完成后，我们希望在 TensorBoard web UI 中可视化结果，并使用 HParams 仪表板来分析结果。

清单 [5-5](#PC11) 显示了代码流。

```
1    import tensorflow as tf
2    from tensorboard.plugins.hparams import api as hp
3
4    # Load MNIST data using built-in datasets download function
5    mnist = tf.keras.datasets.mnist
6    (x_train, y_train), (x_test, y_test) = mnist.load_data()
7
8    x_train, x_test = x_train / 255.0, x_test / 255.0
9
10   HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))
11   HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))
12   HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))
13
14   METRIC_ACCURACY = 'accuracy'
15
16   with tf.summary.create_file_writer('logs/hparam_tuning').as_default():
17    hp.hparams_config(
18      hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],
19      metrics=[hp.Metric(METRIC_ACCURACY, display_name="Accuracy")],
20    )
21
22
23   def train_test_model(hparams):
24        model = tf.keras.models.Sequential([
25            tf.keras.layers.Flatten(),
26            tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),
27            tf.keras.layers.Dropout(hparams[HP_DROPOUT]),
28            tf.keras.layers.Dense(10, activation=tf.nn.softmax),
29        ])
30        model.compile(
31            optimizer=hparams[HP_OPTIMIZER],
32            loss='sparse_categorical_crossentropy',
33            metrics=['accuracy'],
34        )
35
36        model.fit(x_train, y_train, epochs=5)
37        _, accuracy = model.evaluate(x_test, y_test)
38        return accuracy
39   def run(run_dir, hparams):
40    with tf.summary.create_file_writer(run_dir).as_default():
41      hp.hparams(hparams)  # record the values used in this trial
42      accuracy = train_test_model(hparams)
43      tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)
44
45   session_num = 0
46
47   for num_units in HP_NUM_UNITS.domain.values:
48    for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
49      for optimizer in HP_OPTIMIZER.domain.values:
50        hparams = {
51            HP_NUM_UNITS: num_units,
52            HP_DROPOUT: dropout_rate,
53            HP_OPTIMIZER: optimizer,
54        }
55        run_name = "run-%d" % session_num
56        print('--- Starting trial: %s' % run_name)
57        print({h.name: hparams[h] for h in hparams})
58        run('logs/hparam_tuning/' + run_name, hparams)
59        session_num += 1

Listing 5-5Hyperparameter Tuning and Visualization on HParams of TensorBoard

```

第 5 行到第 8 行加载了我们之前处理过的相同的 MNIST 数字数据。

第 10 行设置了神经元或单元数量的值:16 和 32。

第 11 行设置了辍学率:0.1 和 0.2。

第 12 行设置优化函数:`adam`和`sgd`。

代码结构的其余部分很简单，不需要任何解释。注意，对于三个超参数的每个组合，在嵌套的`for`循环中调用了`model.fit()`函数(第 47 行到第 59 行)。度量输出被写入一个日志文件`logs/hparam_tuning`。

实验成功执行后，使用以下命令启动 TensorBoard(确保您处于我们在本书中一直使用的名为`cv`的虚拟环境中):

***(cv)用户名$***:***tensor board-logdir logs/hparam _ tuning***

您可能需要将绝对路径传递给`logs/hparam_tuning`目录。

启动浏览器，指向`http://localhost:6006`。您应该会看到 TensorBoard web UI。从右上角的下拉列表中，选择 HPARAMS。您应该会看到类似于图 [5-23](#Fig23) 中的仪表盘。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig23_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig23_HTML.jpg)

图 5-23

显示 h 参数视图的张量板，包含对应于每个超参数组合的精度

从这个仪表板中，您可以看到给出最高准确性的超参数组合:32 个神经元的 96.160%的准确性，0.1 的漏失，以及 adam 优化器。

或者，点击平行坐标视图选项卡启动图 [5-24](#Fig24) 。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig24_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig24_HTML.jpg)

图 5-24

HPARAMS 的平行组合视图

如图 [5-24](#Fig24) 所示，点击最高精度(或您想要检查的任何精度)的链接，您将看到绿色高亮路径，该路径代表生成精度的超参数组合。

## 保存和恢复模型

通常情况下，您会希望保存训练好的模型，以便以后可以使用它来分类或预测新图像。毕竟你不想每次想用的时候都训练一个模型。

实际上，模型训练是一个耗时的过程。根据您的数据大小、硬件容量和神经网络配置，训练过程可能需要几个小时或几天。您可能希望在培训期间和培训结束后保存模型。如果培训被中断，您可以从中断的地方重新开始，避免培训被中断前的时间损失。

在本节中，我们将探讨如何训练和保存神经网络，稍后加载它，并在我们的应用程序中使用它。

### 培训期间保存模型检查点

清单 [5-6](#PC12) 几乎包含了我们在清单 [5-2](#PC6) 的第一个模型训练代码中看到的所有代码行。我们将突出显示不同的行，以及它们在保存训练权重的上下文中的意义。

```
Filename: Listing_5_6.py
1    import tensorflow as tf
2    import matplotlib.pyplot as plt
3    import os
4
5    # The file path where the checkpoint will be saved.
6    checkpoint_path = "cv_checkpoint_dir/mnist_model.ckpt"
7    checkpoint_dir = os.path.dirname(checkpoint_path)
8
9    # Create a callback that saves the model's weights.
10   cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
11                                                   save_weights_only=True,
12                                                   verbose=1)
13
14   # Load MNIST data using built-in datasets download function.
15   mnist = tf.keras.datasets.mnist
16   (x_train, y_train), (x_test, y_test) = mnist.load_data()
17
18   # Normalize the pixel values by dividing each pixel by 255.
19   x_train, x_test = x_train / 255.0, x_test / 255.0
20
21   # Build the ANN with 4-layers.
22   model = tf.keras.models.Sequential([
23    tf.keras.layers.Flatten(input_shape=(28, 28)),
24    tf.keras.layers.Dense(128, activation="relu"),
25    tf.keras.layers.Dense(60, activation="relu"),
26    tf.keras.layers.Dense(10, activation="softmax")
27   ])
28
29   # Compile the model and set optimizer,loss function and metrics
30   model.compile(optimizer='adam',
31                loss='sparse_categorical_crossentropy',
32                metrics=['accuracy'])
33
34   # Finally, train or fit the model, pass callbacks to save the model weights.
35   trained_model = model.fit(x_train, y_train, validation_split=0.3, epochs=10, callbacks=[cp_callback])
36
37   # Visualize loss  and accuracy history
38   plt.plot(trained_model.history['loss'], 'r--')
39   plt.plot(trained_model.history['accuracy'], 'b-')
40   plt.legend(['Training Loss', 'Training Accuracy'])
41   plt.xlabel('Epoch')
42   plt.ylabel('Percent')
43   plt.show();
44
45   # Evaluate the result using the test set.
46   evalResult = model.evaluate(x_test,  y_test, verbose=1)
47   print("Evaluation Result: ", evalResult)

Listing 5-6Model Weights Are Saved During the Training

```

第 3 行导入了提供文件系统相关函数的`os`包，这些函数用于将模型保存到文件路径。

第 6 行是将存储我们的模型权重的文件名。

第 7 行创建特定于操作系统的文件路径对象。

第 10 行通过传递以下参数初始化名为`ModelCheckpoint`的 TensorFlow 回调类:

*   这是我们在第 7 行创建的文件路径对象。

*   我们应该只保存权重，而不是在训练过程中保存整个模型。默认情况下，这被设置为`False`，这意味着保存整个模型。通过将此设置为`True`，我们让神经网络知道我们只想保存权重。

*   `verbose = 1`打印日志并在控制台上运行状态。否则，默认值 0 表示静音。

基于我们的意图，我们可能想要传递其他的论点。以下是附加参数列表:

*   `save_best_only`:默认为`False`。如果设置为`True`，该算法将评估并保存由我们传递的指标确定的最佳权重。

*   `save_frequency`:默认值为`epoch`，这意味着我们希望在每个时期结束时保存检查点。还可以传递一个整数来指示保存检查点的频率。例如，如果您设置了`save_frequency = 5`，这将意味着每五个时间点保存一次检查点。

您会注意到，在清单 [5-6](#PC12) 中，除了符合模型的第 35 行之外，所有其他行都与清单 [5-2](#PC6) 中的相同。

第 35 行对`fit()`函数增加了一个参数。附加参数`callbacks = [cp_callback]`是为了在模型训练期间保存检查点。

注意，我们在清单 [5-6](#PC12) 中设置了`epoch=10`。图 [5-25](#Fig25) 和图 [5-26](#Fig26) 显示了该模型的损耗和精度的部分输出样本。有测试数据的模型精度为 0.9775，损耗为 0.084755。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig26_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig26_HTML.jpg)

图 5-26

历元=2 的模型评估

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig25_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig25_HTML.jpg)

图 5-25

训练损失和准确性

### 手动保存重量

如果您想要手动保存权重，而不是每个时期或定期保存检查点，您可以简单地添加此函数:

```
# Save the model weights
checkpoint_path = "cv_checkpoint_dir/mnist_model.ckpt"
model.save_weights(checkpoint_path)

```

### 加载保存的权重并重新训练模型

如果您希望加载保存的权重，因为您希望在中断后恢复训练，或者因为您有更多的数据或任何其他原因，只需在创建/配置神经网络后添加以下行:

```
# Load saved weights
model.load_weights(checkpoint_path)

```

确保你已经像清单 [5-6](#PC12) 的第 22 行和第 30 行一样初始化了你的神经网络。请务必注意，网络体系结构必须与存储检查点的网络相同。

### 保存整个模型

调用`model.save()`函数保存整个模型，包括模型架构、权重和训练配置。确保在调用`fit()`方法后调用函数`model.save()`。即在清单 [5-6](#PC12) 的第 35 行后调用`save()`函数。下面是保存整个模型的代码片段:

```
# Save the entire model to a file name “my_ann_model.h5”.
# You can also give the absolute pass to save the model.

model.save('mv_ann_model.h5')

```

保存一个完整的功能模型是很有用的。

*   您可以从模型停止的地方加载并重新训练模型。

*   您可以与其他研究人员或团队成员共享该模型，以便在不同的系统上运行。

*   您可以在任何其他应用程序中使用该模型。

### 重新训练现有模型

如果您想用额外的数据重新训练现有的模型，下面的代码片段将帮助您做到这一点:

```
# Load and create the exact same model, including its weights and the optimizer
model = tf.keras.models.load_model('mv_ann_model.h5')

# Show the model architecture
model.summary()

#Retrain the model
retrained_model = model.fit(x_train, y_train, validation_split=0.3, epochs=10)

```

### 在应用程序中使用经过训练的模型

如果您已经有一个保存在文件系统中的训练模型，您可以加载该模型并调用`predict()`函数来使用该模型。这里有一个例子:

```
# Load and create the exact same model, including its weights and the optimizer
model = tf.keras.models.load_model('mv_ann_model.h5')

# Predict the class of the input image from the loaded model
predicted = model.predict(x_pixel_data)
print("Predicted", predicted)

```

## 卷积神经网络

*卷积神经网络* (CNN)是一种特殊的人工神经网络。CNN 与传统 ANN 的最大不同在于，特征工程是在 CNN 中自动执行的。

我们将学习 CNN 用来从输入图像中提取和选择特征的技术。一路上，我们将学习一些与 CNN 相关的常用术语。我们将编写 TensorFlow 代码来训练我们自己的 CNN 模型来对图像进行分类，像以前一样，我们将提供对代码的逐行解释。我们将通过一个示例对胸部 x 光进行分类，以检测肺炎。

### CNN 的体系结构

传统的人工神经网络或 MLP 由一个输入层、一个或多个隐藏层和一个输出层组成。CNN 有一组附加层，称为*卷积层*(见图 [5-27](#Fig27) )。输入图像被馈送到该卷积层的第一层。卷积层的输出被馈送到全连接 MLP 的“输入”层。卷积层实现一种算法，该算法执行输入图像的特征工程。MLP 实现了传统的深度学习算法来分类图像。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig27_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig27_HTML.png)

图 5-27

CNN 架构

卷积层有两个部分。

*   *卷积*:该层从图像中提取特征(特征提取)。

*   *二次采样*:该层从提取的特征中选择(特征选择)。

图 [5-28](#Fig28) 描绘了一个完整的 CNN。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig28_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig28_HTML.jpg)

图 5-28

具有卷积、子采样和全连接 MLP 层的 CNN

### CNN 是如何工作的

我们在第 [2](02.html) 章中看到，计算机将一幅单通道的黑白图像视为像素值的 2D 矩阵(如图 [5-28](#Fig28) )。具有 RGB 通道(三个通道)的彩色图像显示为这些 2D 矩阵的堆叠。这些矩阵的堆叠形成了一个 3D 张量(还记得张量吗？).图 [5-29](#Fig29) 和图 [5-30](#Fig30) 显示了 3D 图像张量的视觉呈现。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig30_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig30_HTML.jpg)

图 5-30

作为一堆 2D 矩阵的三通道彩色图像的张量表示

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig29_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig29_HTML.jpg)

图 5-29

一张黑白图像(左)被电脑视为 2D 矩阵(右)

在图像如何被表示为张量的背景下，让我们理解卷积过程。

#### 盘旋

想象一下，我们有一个用放大镜浏览的图像，记录下我们观察到的重要图案。这是卷积如何工作的一个很好的类比。

以下是使用卷积从图像中提取重要特征的步骤:

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig31_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig31_HTML.jpg)

图 5-31

卷积(图片由 Andrej Karpathy 提供)

1.  将图像分成大小为 *k* × *k* 像素的网格。这被称为*内核*，它被表示为一个 *k* × *k* 矩阵。

2.  定义一个或多个与内核维度相同的过滤器。

3.  取其中一个通道的第一个核(从 2D 矩阵的左上角开始)，用第一个滤波器进行逐元素乘法，并将乘法结果相加。对其他通道执行相同的操作，并将所有三个通道的结果相加，以获得新创建的要素的像素值。

    如图 [5-31](#Fig31) 所示。对于这个例子，我们取一个 7×7×3 的图像，其核大小为 3×3。我们有两组过滤器:W0 和 W1(显示为红色)。滤波器 W0 的偏差为 1，而滤波器 W1 没有任何偏差。输出特征显示在绿色网格中(显示在下方最右侧)。

输出计算如下所示:

```
Channel 1 Output = 0x(-1) +  0x(-1) + 0x1 + 0x1 + 2x1 + 1x0 + 0x0 + 0x(-1)+1x(-1) = 2
Channel 2 Output = 0x0 + 0x0 + 0x(-1) + 0x1 + 2x1 + 0x0 + 0x(-1) + 2x(-1) + 0x1 = 0
Channel 3 Output = 0x0 + 0x0 + 0x1 + 0x0 + 0x0 + 2x0 + 0x0 + 2x0 + 2x1 = 1
Feature Value = Channel 1 Output + Channel 2 Output + Channel 3 Output + bias
Feature Value    = 2 + 0 + 1 + 1 = 4

```

值 4 突出显示在顶部绿色网格的左上角。

1.  内核现在向右移动，特征值的计算如前所述。当内核一直向右移动时，它从该行最左边的像素开始向下移动到下一行。内核移动到水平和垂直方向扫描整个图像的步数称为*步距*。步幅表示为 *s* (例如 2 或 3 等。).步幅为 2 意味着内核将向右移动两步，当它到达图像的右边缘时，它将向下移动 2 个像素。

2.  当扫描整个图像时，创建特征矩阵。在我们的例子中，特征矩阵的维数是 3×3(对于 7×7×3 像素的图像，3×3 内核，2×2 步长)。该特征矩阵也称为*特征图*，如图 [5-31](#Fig31) 所示，位于顶部绿色 3×3 网格中(右侧)。

3.  对下一组滤波器重复相同的卷积过程，并且创建特征图。图 [5-31](#Fig31) 中底部的绿色网格显示了第二个过滤器的特征图。

4.  对所有过滤器重复该过程，并且从每个过滤器生成特征图。

#### 合并/子采样/缩减采样

卷积从图像中提取特征。这些特征被表示为 *n* × *n* 矩阵。这些特征或 *n* × *n* 矩阵被馈送到另一个层，称为*池层*，它执行“下采样”，很像特征选择。最大池和平均池是对要素进行缩减采样的两种常用方法。

##### 最大池化

在汇集层中，很像卷积阶段，特征矩阵被划分成具有步长为 *s* 的 *k* × *k* 的网格(例如，图 [5-32](#Fig32) 中的 2×2 像素的核)(例如，示例中的步长 1)。在最大池层中，获取每个内核区域的最大像素值，并生成缩减采样矩阵。对来自前一层的每个滤波器输出重复该过程。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig32_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig32_HTML.jpg)

图 5-32

最大池向下采样特征(图片由 Andrej Karpathy 提供)

##### 平均池

平均池的工作方式与最大池相同，只是平均池采用内核像素的平均值(不是最大值)来创建缩减像素采样矩阵。

CNN 通常由交替卷积层和池层以及多层感知器组成(如图 [5-33](#Fig33) 所示)。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig33_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig33_HTML.png)

图 5-33

CNN 层，交替卷积和 MLP 池层

### CNN 概念概述

以下是我们了解到的情况:

*   CNN 由交替的卷积层和汇集层组成，最后是 MLP。每个卷积图层不一定都有缩减像素采样图层。

*   卷积是卷积层中的特征提取过程。

*   定义了维度为 *k* × *k* 的核来将输入图像划分成网格。

*   与内核维数相同的滤波器与内核中的像素相乘，然后对每个像素和每个图像通道的结果求和。可选的偏差被添加到结果中以生成特征矩阵。

*   池化图层实施缩减采样算法(最大池化或平均池化)对要素进行缩减采样。

*   对每对卷积汇集层重复该过程，其中来自一个汇集层的输出作为输入被馈送到下一个卷积层。

*   最后一个卷积/池图层将要素矩阵提供给 MLP 的输入图层。

*   网络的 MLP 部分像传统的 MLP 网络一样学习。

### 训练一个 CNN 模型:从胸部 x 光检测肺炎

Keras 的 TensorFlow 使得训练 CNN 模型变得极其简单。只需几行代码，您就可以实现一个 CNN。

在本节中，我们将编写代码来训练一个模型，以便从胸部 X 射线中检测肺炎。这里介绍的模型是一个简单的 CNN 网络，用于学术和学习目的，不得用于诊断任何医疗状况。

#### 胸部 x 光数据集

我们已经从位于 Kaggle 网站 [`https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia`](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) 的公开数据集下载了胸部 x 光图像。这些图片在知识共享许可协议下可用， [`https://creativecommons.org/licenses/by/4.0/`](https://creativecommons.org/licenses/by/4.0/) 。

该数据集由代表正常胸部 X 射线(无疾病肺)和肺炎感染肺的图像组成。这些正常和肺炎图像被分开并存储在单独的目录中；所有正常图像存储在名为`NORMAL`的目录中，肺炎图像存储在`PNEUMONIA`目录中。此外，数据集分为训练集、测试集和验证集。从 Kaggle 的网站下载图像后，我们将它们保存在本地磁盘中。图 [5-34](#Fig34) 显示了一个示例目录结构。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig34_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig34_HTML.jpg)

图 5-34

胸部 x 光图像的目录结构

#### 代码结构

我们将保持我们的代码简单易懂。有更好的方法来组织代码，使其更加面向对象和可重用，这是生产质量工作的强烈建议。为了灵活性和可维护性，您必须将代码参数化，并避免任何硬编码。但是，为了便于学习，我们简化了下面的代码，并使用了一些硬编码的值来保持简单性。

#### CNN 模型训练

清单 [5-7](#PC19) 显示了用于训练 CNN 模型的代码示例，该模型根据胸部 x 光片预测肺炎。

```
1    import numpy as np
2    import pathlib
3    import cv2
4    import tensorflow as tf
5    import matplotlib.pyplot as plt
6
7
8    # Section1: Loading images from directories for training and test
9    trainig_img_dir ="images/chest_xray/train"
10  test_img_dir ="images/chest_xray/test"
11
12   # ImageDataGenerator class provides a mechanism to load both small and large dataset.
13   # Instruct ImageDataGenerator to scale to normalize pixel values to range (0, 1)
14   datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)
15   #Create a training image iterator that will be loaded in a small batch size. Resize all images to a #standard size.
16   train_it = datagen.flow_from_directory(trainig_img_dir, batch_size=8, target_size=(1024,1024))
17   # Create a training image iterator that will be loaded in a small batch size. Resize all images to a #standard size.
18   test_it = datagen.flow_from_directory(test_img_dir, batch_size=8, target_size=(1024, 1024))
19
20   # Lines 22 through 24 are optional to explore your images.
21   # Notice, next() function call returns both pixel and labels values as numpy arrays.
22   train_images, train_labels = train_it.next()
23   test_images, test_labels = test_it.next()
24   print('Batch shape=%s, min=%.3f, max=%.3f' % (train_images.shape, train_images.min(), train_images.max()))
25
26   # Section 2: Build CNN network and train with training dataset.
27   # You could pass argument parameters to build_cnn() function to set some of the values
28   # such as number of filters, strides, activation function, number of layers etc.
29   def build_cnn():
30      model =  tf.keras.models.Sequential()
31      model.add(tf.keras.layers.Conv2D(32, (3, 3), activation="relu", strides=(2,2), input_shape=(1024, 1024, 3)))
32      model.add(tf.keras.layers.MaxPooling2D((2, 2)))
33      model.add(tf.keras.layers.Conv2D(64, (3, 3), strides=(2,2),activation='relu'))
34      model.add(tf.keras.layers.MaxPooling2D((2, 2)))
35      model.add(tf.keras.layers.Conv2D(128, (3, 3), strides=(2,2),activation='relu'))
36      model.add(tf.keras.layers.Flatten())
37      model.add(tf.keras.layers.Dense(128, activation="relu"))
38      model.add(tf.keras.layers.Dense(2, activation="softmax"))
39      return model
40
41   # Build CNN model
42   model = build_cnn()
43   #Compile the model with optimizer and loss function
44   model.compile(optimizer='adam',
45                loss='categorical_crossentropy',
46                metrics=['accuracy'])
47
48   # Fit the model. fit_generator() function iteratively loads large number of images in batches
49   history = model.fit_generator(train_it, epochs=10, steps_per_epoch=16,
50                      validation_data=test_it, validation_steps=8)
51
52   # Section 3: Save the CNN model to disk for later use.
53   model_path = "models/pneumiacnn"
54   model.save(filepath=model_path)
55
56   # Section 4: Display evaluation metrics
57   print(history.history.keys())
58   plt.plot(history.history['accuracy'], label="accuracy")
59   plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
60   plt.plot(history.history['loss'], label="loss")
61   plt.plot(history.history['val_loss'], label = 'val_loss')
62
63   plt.xlabel('Epoch')
64   plt.ylabel('Metrics')
65   plt.ylim([0.5, 1])
66   plt.legend(loc='lower right')
67   plt.show()
68   test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
69   print(test_acc)

Listing 5-7Code to Train CNN Model to Predict Pneumonia from Chest X-rays

```

清单 [5-7](#PC19) 中用于 CNN 模型训练的代码在逻辑上分为以下四个部分:

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig36_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig36_HTML.jpg)

图 5-36

用于训练和评估的度量标准(一个时期内的损失和准确性)的样本图

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig35_HTML.png](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig35_HTML.png)

图 5-35

CNN 模型训练的样本输出

*   *加载图像(第 9 行到第 24 行)*:我们将训练和测试图像存储在前面描述的目录中。为了加载这些图像进行训练和验证，我们使用了 Keras 提供的一个强大的类`ImageDataGenerator`。下面是我们如何使用这个类的逐行解释:

    第 9 行和第 10 行是子目录中有训练和测试图像的目录。

    第 14 行初始化了`ImageDataGenerator`类。我们传递了参数`rescale = 1/255`,因为我们想要将像素值标准化到 0 和 1 之间的范围内。这种标准化是通过将图像的每个像素乘以 1/255 来完成的。我们称这条线为`datagen`,如变量名所示。

    第 16 行调用了`datagen`对象的`flow_from_directory()`函数。该功能从`training_img_directory`目录中批量加载图像(如`batch_size = 8`)，并将图像调整到`target_size`指定的尺寸(如 1024×1024px)。这是一个高度可扩展的功能，将能够加载数百万张图像，而无需将它们全部加载到内存中。它将一次加载尽可能多的图像，如`batch_size`参数所示。将所有图像调整到标准大小对于大多数机器学习练习都很重要。注意，这个函数的默认 resize 值是 256。如果省略 resize 参数，所有输入图像都将被调整为 256×256。

    第 17 行和第 16 行做的一样，只是它从`test`目录加载图像。虽然我们的目录中有验证数据(从 Kaggle 网站下载的数据集包含验证图像)，但数量很少，因此我们决定使用测试数据集进行验证。

    函数`flow_from_directory()`返回一个迭代器。如果对这个迭代器进行迭代，将得到一个由两个 NumPy 数组组成的元组——图像像素值数组和标签数组。

    Note that labels are interpreted from the subdirectories the images are read from. For example, all images from the `NORMAL` directories will get the label `NORMAL`, and similarly images belonging to the `PNEUMONIA` subdirectory will get the `PNEUMONIA` label. But wait. Aren’t these labels supposed to be numeric? These directory names are sorted by their names and indexed, starting from 0\. In our case, `NORMAL` will be indexed as 0 and `PNEUMONIA` as 1\. But, it does not stop here. The function `flow_from_directory()` takes an additional argument called `class_mode`. By default the value of `class_mode` is categorical. You could also pass a value to it as binary or sparse. The differences between these three are as follows:
    *   `categorical`将返回 2D 的独热编码标签。

    *   `binary`将返回 1D 二进制标签。

    *   `sparse`将返回 1D 整数标签。

    第 22 到 24 行是可选的，训练模型时不需要。我们提供它们是为了向您展示如何从迭代器中探索从`flow_from_directory()`函数返回的值。

*   *CNN 配置和训练(第 29 到 50 行)*:第 29 到 39 行实现了构建 CNN 的功能。这几行是我们这一节的重点。所以，让我们试着理解这是怎么回事。

    第 30 行创建了一个连续的神经网络，我们在上面堆叠了几层。回想一下，我们使用相同的`tf.keras.model.Sequential`类来创建顺序模型。`model`对象的`add()`函数用于按顺序添加图层——首先添加的图层首先执行，以此类推。

    Line 31 adds our first layer to the network. If you recall from our previous discussion on CNN, our first layer of the CNN must be a convolution layer that takes the input (image pixel values). Here we are using the `Conv2D` class to define our convolution layer. We are passing five important parameters to `Conv2D()`.
    *   过滤器，在我们的例子中是 64。

    *   内核维度，在本例中是 3×3 像素，并作为元组(3，3)传递。

    *   激活函数，在我们的例子中是`relu`(因为像素值的范围是从 0 到 1，并且从不为负)。

    *   下一个参数是设置步幅，如果没有设置，默认为(1，1)。在我们的例子中，我们将其设置为(2，2)。

    *   最后一个参数是设置输入大小。由于我们的图像被调整为 1024×1024 像素的彩色(具有三个通道)，因此，input_shape 为(1024，1024，3)。

    第 32 行添加了池层，`MaxPooling2D`。回想一下，除了 MLP 层之前的层之外，卷积层和池层是交替成对出现的。我们传递参数来设置网格或内核的大小。在我们的例子中，它被设置为(2，2)。

    第 33、34 和 35 行也是我们的卷积和池层。您可以根据需要拥有任意多的卷积和池化图层来实现所需的精度级别。

    卷积层(线 35)的输出被馈送到 MLP 的第一层。回想一下，MLP 的第一层称为输入层，接下来是隐藏层，最后是输出层。

    线 36 使线 35 的输出变平。

    第 37 行是 MLP 的隐藏层，已经在 ANN 部分解释过了。

    第 38 行是最后一层，即输出层。如前所述，我们在解决涉及两个类别的分类问题时使用了激活函数 softmax。

    第 42 行简单地调用了`build_cnn()`函数并创建了一个`model`对象。

    第 44 行编译模型，正如我们前面看到的 ann。你会注意到在损失函数中清单 [5-6](#PC12) 的第 44 行和第 30 行的区别。这里我们使用的是损失函数`categorical_crossentropy`，而不是我们在清单 [5-6](#PC12) 中使用的`sparse_categorical_crossentropy`。你能猜到原因吗？

    最后，我们在 49 线开始训练。注意，我们没有像在清单 [5-6](#PC12) 中那样调用函数`fit()`。我们正在调用`fit_generator()`函数。该功能与`ImageDataGenerator`配合使用，以小批量加载图像。如果使用简单的`fit()`函数，它将获取第一批输入并训练模型，而这显然不是我们想要的。函数`fit_generator()`有一个名为`steps_per_epoch`的重要参数，即它在每个时期内完成的批次数量。以下是官方定义:

    `steps_per_epoch`:在宣布一个时期结束并开始下一个时期之前，从`generator`(数据加载器)产生的总步骤数(样品批次)。它通常应该等于数据集的样本数除以批量大小。例如，如果您的训练集中有 1000 个文件，并且您的 batch_size 为 8，那么您应该将`steps_per_epoch`设置为 1000/8 = 125。

    该函数的另一个重要参数是`validation_steps`，定义如下:

    `validation_steps`:仅当`validation_data`是发电机时相关。它是停止前从`generator`(数据加载器)产生的总步骤数(样品批次)。

*   *将 CNN 模型保存到磁盘(第 53 和 54 行)*:第 54 行将训练好的模型保存到第 53 行指定的目录。你也可以省去训练关卡。

*   *评估和可视化(第 57 行到第 69 行)*:我们绘制了一个训练损失、验证损失、训练准确性和测试准确性相对于时期的图表。第 68 行评估模型，并简单地在第 69 行打印精度。

    图 [5-35](#Fig35) 显示了模型运行时的示例输出。图 [5-36](#Fig36) 显示了培训和验证指标的样本图。如图所示，训练和验证的损失随着时期数的增加而减少。此外，精确度随着时间的推移而提高。

#### 肺炎预测

清单 [5-8](#PC20) 显示了如何使用之前训练的 CNN 模型从一组新图像中预测肺炎。

```
1    import numpy as np
2    import pathlib
3    import cv2
4    import tensorflow as tf
5    import matplotlib.pyplot as plt
6
7    model_path = "models/pneumiacnn"
8
9    val_img_dir ="images/chest_xray/val"
10   # ImageDataGenerator class provides a mechanism to load both small and large dataset.
11   # Instruct ImageDataGenerator to scale to normalize pixel values to range (0, 1)
12   datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)
13   # Create a training image iterator that will be loaded in a small batch size. Resize all images to a #standard size.
14   val_it = datagen.flow_from_directory(val_img_dir, batch_size=8, target_size=(1024,1024))
15
16
17   # Load and create the exact same model, including its weights and the optimizer
18   model = tf.keras.models.load_model(model_path)
19
20   # Predict the class of the input image from the loaded model
21   predicted = model.predict_generator(val_it, steps=24)
22   print("Predicted", predicted)

Listing 5-8Code for Predicting Pneumonia by Using the Trained CNN Model

```

用于对存在肺炎的图像进行分类或预测的代码分为三个部分。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig37_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig37_HTML.jpg)

图 5-37

样本预测输出

*   *加载图像(第 9 行到第 14 行)*:从磁盘目录加载图像，如清单 [5-7](#PC19) 所述。第 14 行和我们之前一样使用了`flow_from_directory()`。

*   *加载保存的模型(第 18 行)*:从清单 [5-7](#PC19) 中回忆，我们在目录`models/pneumiacnn`中保存了训练好的模型。第 18 行从磁盘目录加载保存的模型。

*   *预测肺炎(第 21 行)*:第 21 行使用`model.predict_generator()`函数。该函数类似于`fit_generator()`函数，因为这两个函数都是批量从磁盘读取图像。`predict_generator()`功能通过批量加载图像来预测图像是否代表肺炎。

    预测的结果打印在第 22 行。

    图 [5-37](#Fig37) 显示了一个样本预测输出。

预测输出是一个 NumPy 数组，由每个图像的所有类别的概率组成。在前一个输出样本中，在第一个打印输出行中，第二类的概率最高。大约是 98%，因此第一个输入的预测类是 1(这是概率最高的类的索引)。

CNN 是计算机视觉中最强大的算法之一。在本节中，您了解了 CNN 的概念及其工作原理。我们还通过一些代码示例来训练我们自己的 CNN 模型来预测肺炎。

### 流行 CNN 的例子

我们在清单 [5-7](#PC19) 中构建的 CNN 不是一个生产质量的网络。我们建立了一个简单的网络来学习基础知识。让我们来看看一些在全球范围内被证明是成功的流行网络。

#### 莱内-5

LeNet-5 CNN 体系结构由 LeCun 等人在 1998 年的论文“基于梯度的学习应用于文档识别”中首次提出。该架构主要用于从文档中识别手写和机器生成的字符(光学字符识别[OCR])。该架构简单明了，因此在教学中广泛使用。以下是 LeNet-5 架构的显著特征:

*   这是一个 CNN 网络，它由七层组成。

*   在这七层中，有三个卷积层(C1、C3 和 C5)。

*   有两个子采样层(S2 和 S4)。

*   有一个全连接层(F6)和一个输出层。

*   卷积层使用步长为 1 的 5×5 卷积核。

*   子采样层是 2×2 平均池层。

*   除了输出层使用 softmax 外，整个网络都使用 TanH 激活函数。

图 [5-38](#Fig38) 显示了 LeNet-5 网络。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig38_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig38_HTML.jpg)

图 5-38

LeNet-5(图片礼遇 of〔t0〕〔http://yann〕。莱孔。页:1。pdf

这里有一个练习:修改清单 [5-7](#PC19) 中的 TensorFlow 代码并实现 LeNet-5。

#### 阿勒克斯网

AlexNet 是由 Alex Krizhevsky 等人设计的卷积神经网络架构。当 AlexNet 在 2012 年参加 ImageNet 大规模视觉识别挑战赛时，它变得流行起来，并取得了 15.3%的前五名错误，比亚军低了 10.8 个百分点以上。AlexNet 是一个深度网络，尽管计算量很大，但由于使用了 GPU，它变得可行。

AlexNet 的特点如下:

*   它是一个包含八层的深度卷积神经网络。

*   输入尺寸为 224×224×3 彩色图像。

*   前五层是卷积层和最大池层的组合，具有以下配置:

*   *卷积层 1* :内核 11×11，滤镜 96，步长 4×4，激活 ReLU

*   *池化层 1* :内核大小为 3×3 的最大池化，步长为 2×2

*   *卷积层 2* :内核 5×5，滤波器 256，步长 1×1，激活 ReLU

*   *池化层 2* :内核大小为 3×3 的最大池化，步长为 2×2

*   *卷积层 3* :内核 3×3，滤波器 384，步长 1×1，激活 ReLU

*   *卷积层 4* :内核 3×3，滤波器 384，步长 1×1，激活 ReLU

*   *卷积层 5* :内核 3×3，滤波器 384，步长 1×1，激活 ReLU

*   池化层 5:内核大小为 3×3 的最大池化，步长为 2×2

*   最后三层是一个完全连接的 MLP。

*   所有卷积层都使用 ReLU 激活函数。

*   输出层使用 softmax 激活。

*   输出层有 1000 个类。

*   网络有 6000 万个参数，65 万个神经元，在一个 GPU 上训练大概需要 3 天。

图 [5-39](#Fig39) 显示了 AlexNet 的图示。

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig39_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig39_HTML.jpg)

图 5-39

具有五个卷积层和三个全连接 MLP 的 AlexNet

#### VGG-16

我们要探索的下一个著名的深度神经网络是 VGG-16，它在 2014 年赢得了 ImageNet 大规模视觉识别挑战(ILSVRC)比赛。VGG 是由牛津视觉几何小组(VGG)的研究人员设计的。他们的出版物在 [`https://arxiv.org/abs/1409.1556`](https://arxiv.org/abs/1409.1556) 有售。

图 [5-40](#Fig40) 显示了 VGG-16 网络。以下是它的突出特点:

![../images/493065_1_En_5_Chapter/493065_1_En_5_Fig40_HTML.jpg](../images/493065_1_En_5_Chapter/493065_1_En_5_Fig40_HTML.jpg)

图 5-40

具有 16 层(13 个卷积层和 3 个密集层)的 VGG-16 架构

*   VGG-16 是由 16 层组成的卷积神经网络。

*   它有 13 个卷积层和 3 个全连接密集层。

*   16 个卷积层具有以下特征:

*   *卷积层 1* :输入尺寸 224×224×3，内核 3×3，过滤器 64，激活 ReLU

*   *卷积层 2* :内核 3×3，滤波器 64，激活 ReLU

*   *池层*:最大池，内核大小 2×2，步数 2×2

*   *卷积层 3* :内核 3×3，滤波器 128，激活 ReLU

*   *卷积层 4* :内核 3×3，滤镜 128，激活 ReLU

*   *池层*:最大池，内核大小 2×2，步数 2×2

*   *卷积层 5* :内核 3×3，滤镜 256，激活 ReLU

*   *卷积层 6* :内核 3×3，滤波器 256，激活 ReLU

*   *卷积层 7* :内核 3×3，滤波器 256，激活 ReLU

*   *池层*:最大池，内核大小 2×2，步数 2×2

*   *卷积层 8* :内核 3×3，滤镜 512，激活 ReLU

*   *卷积层 9* :内核 3×3，滤镜 512，激活 ReLU

*   *卷积层* 10:内核 3×3，滤镜 512，激活 ReLU

*   *池层*:最大池，内核大小 2×2，步数 2×2

*   *卷积层 11* :内核 3×3，滤镜 512，激活 ReLU

*   *卷积层 12* :内核 3×3，滤镜 512，激活 ReLU

*   *卷积层 13* :内核 3×3，滤镜 512，激活 ReLU

*   池层:最大池，内核大小 2×2，步长 2×2

*   *全连通第 14 层(MLP 输入层)*:展平密集层，输入尺寸 25088

*   *全连通隐藏层 15* :输入尺寸为 4096 的密集层

*   1000 个类的全连接输出层。

*   这个网络有 1.38 亿个参数。

这里有一个练习:修改清单 [5-7](#PC19) 并使用 TensorFlow 实现 VGG-16 网络。

### 摘要

在本章中，我们学习了人工神经网络和卷积神经网络的基础知识。我们编写了基于 TensorFlow 的代码来训练我们自己的 ANN 和 CNN 模型，评估结果，并使用保存的模型对图像进行分类。我们还学习了如何调整超参数，以及如何在 TensorBoard 的 HParams 仪表板中可视化分析。此外，我们探索了一些流行的 CNN:LeNet-5，AlexNet 和 VGG-16。

在这一章中，我们解决了分类问题。换句话说，我们的模型被训练来辨别输入图像属于哪一类。在下一章，我们将学习如何检测图像中的物体。****