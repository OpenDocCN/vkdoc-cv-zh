# 十一、MobileNetV3

在本章中，我们将了解 MobileNetV3，它通过降低网络的复杂性，在移动硬件上提供了 EfficientNet 的优化版本。该模型主要基于 EfficientNet 的搜索策略，具有特定于移动设备的参数空间目标。

这是移动模型的当前艺术状态，但在这一点上，我们深深陷入了争论什么硬件在运行的领域，使得 1:1 模型比较变得困难。制造商越来越多地推出定制硬件，每种设备的运行方式都会略有不同。不过，另一方面，可以给 EfficientNet 搜索算法一个任意的起点(例如，知道它将在什么硬件上运行)，然后为该设备生成一个优化的网络。我相信这越来越是未来的发展方向:随着越来越多新的人工智能硬件变得可用，网络将被定制为在特定设备上运行。

首先，让我们看看 swish 和 sigmoid 激活函数的一些特定于移动设备的变体，我们可以用它们来加速对我们的网络的评估。

## 硬嗖嗖和硬乙状结肠

在上一章中，我们讨论了如何使用 swish 和 sigmoid 作为激活函数，使网络能够学习到更准确的结果。不过，在运行时，这些函数在内存方面比我们的 ReLU 激活函数要昂贵得多。MobileNet 作者介绍了我们的 sigmoid 函数的 relu6 变体:

```py
hardSigmoid(x) = relu6(x + 3)/6
hardSwish(x) = x * hardSigmoid(x)

```

以便减少运行网络所需的内存量并简化运行时间。

然而，他们发现他们无法在不牺牲性能的情况下简单地将此应用于所有节点。我们一会儿会回到这个话题。

## 移除一半网络的挤压和激励(SE)块逻辑

同样，EfficientNet 的 SEBlock 逻辑也很强大，但是在移动设备上这是一个昂贵的操作。然而，他们发现他们可以在不牺牲性能的情况下为某些层移除这一点。我们一会儿会再回到这个话题。

## 自定义标题

作者为他们的输出层实现了一个定制的 head 逻辑，我认为这很有趣。本质上，他们使用一对卷积来代替 EfficientNet 中使用的密集输出神经网络层。从技术角度来看，这种方法不如密集方法精确，但在移动设备上实现起来更简单、更快。

## 超参数

最后，作者结合前面的文章，大量使用了高效网络搜索策略。从概念上讲，他们给搜索算法提供了前面提到的构建模块，并运行一组 TPU，让强化学习发挥它的魔力。由此，他们产生了两个不同的网络，MobileNetV3-大型和 MobileNetV3-小型，由于前面的限制，这两个网络略有不同。例如，虽然两种变体都在网络的后面部分使用 SEBlock，但小型变体在其第二层使用 se block，而大型变体则不使用。每层的滤波器数量完全是为了优化性能而学习的。两个网络在最初几层都使用 ReLU，但在中途就切换到 hardSwish。

## 表演

综上所述，该网络在 ImageNet 上具有更高的精度，但在有硬件支持的移动设备上，评估时间不到 10ms。然后，作者还使用不同的起始要求(例如，仅允许 3×3 卷积)运行他们的搜索策略，以产生最小的变体，该变体应合理地适应未来，取决于市场上出现的任何新硬件。

## 密码

让我们来构建 MobileNetV3。这将结合硬件感知网络架构搜索(NAS)和 NetAdapt 算法，以利用这两种方法的优势。这个网络比我们到目前为止看到的网络要复杂得多，但是如果你仔细观察，我想你可以看到它只是我们到目前为止看到的所有技术的组合。需要注意的关键部分是末尾的大量 MBConvBlockStack 参数集合，这些参数生成了细微不同的块，这些块组合在一起可以产生一个既准确又能在移动设备上良好运行的网络。

```py
``
import TensorFlow

public enum ActivationType {
  case hardSwish
  case relu
}

public struct SqueezeExcitationBlock: Layer {
  // https://arxiv.org/abs/1709.01507
  public var averagePool = GlobalAvgPool2D<Float>()
  public var reduceConv: Conv2D<Float>
  public var expandConv: Conv2D<Float>
  @noDerivative public var inputOutputSize: Int

  public init(inputOutputSize: Int, reducedSize: Int) {
    self.inputOutputSize = inputOutputSize
    reduceConv = Conv2D<Float>(
      filterShape: (1, 1, inputOutputSize, reducedSize),
      strides: (1, 1),
      padding: .same)
    expandConv = Conv2D<Float>(
      filterShape: (1, 1, reducedSize, inputOutputSize),
      strides: (1, 1),
      padding: .same)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    let avgPoolReshaped = averagePool(input).reshaped(to: [
      input.shape[0], 1, 1, self.inputOutputSize,
    ])
    return input
      * hardSigmoid(expandConv(relu(reduceConv(avgPoolReshaped))))
  }
}

public struct InitialInvertedResidualBlock: Layer {
  @noDerivative public var addResLayer: Bool
  @noDerivative public var useSELayer: Bool = false
  @noDerivative public var activation: ActivationType = .relu

  public var dConv: DepthwiseConv2D<Float>
  public var batchNormDConv: BatchNorm<Float>
  public var seBlock: SqueezeExcitationBlock
  public var conv2: Conv2D<Float>
  public var batchNormConv2: BatchNorm<Float>

  public init(
    filters: (Int, Int),
    strides: (Int, Int) = (1, 1),
    kernel: (Int, Int) = (3, 3),
    seLayer: Bool = false,
    activation: ActivationType = .relu
  ) {
    self.useSELayer = seLayer
    self.activation = activation
    self.addResLayer = filters.0 == filters.1 && strides == (1, 1)

    let filterMult = filters
    let hiddenDimension = filterMult.0 * 1
    let reducedDimension = hiddenDimension / 4

    dConv = DepthwiseConv2D<Float>(
      filterShape: (3, 3, filterMult.0, 1),
      strides: (1, 1),
      padding: .same)
    seBlock = SqueezeExcitationBlock(
      inputOutputSize: hiddenDimension, reducedSize: reducedDimension)
    conv2 = Conv2D<Float>(
      filterShape: (1, 1, hiddenDimension, filterMult.1),
      strides: (1, 1),
      padding: .same)
    batchNormDConv = BatchNorm(featureCount: filterMult.0)
    batchNormConv2 = BatchNorm(featureCount: filterMult.1)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    var depthwise = batchNormDConv(dConv(input))
    switch self.activation {
    case .hardSwish: depthwise = hardSwish(depthwise)
    case .relu: depthwise = relu(depthwise)
    }

    var squeezeExcite: Tensor<Float>
    if self.useSELayer {
      squeezeExcite = seBlock(depthwise)
    } else {
      squeezeExcite = depthwise
    }

    let piecewiseLinear = batchNormConv2(conv2(squeezeExcite))

    if self.addResLayer {
      return input + piecewiseLinear
    } else {
      return piecewiseLinear
    }
  }
}

public struct InvertedResidualBlock: Layer {
  @noDerivative public var strides: (Int, Int)
  @noDerivative public let zeroPad = ZeroPadding2D<Float>(padding: ((0, 1), (0, 1)))
  @noDerivative public var addResLayer: Bool
  @noDerivative public var activation: ActivationType = .relu
  @noDerivative public var useSELayer: Bool

  public var conv1: Conv2D<Float>
  public var batchNormConv1: BatchNorm<Float>
  public var dConv: DepthwiseConv2D<Float>
  public var batchNormDConv: BatchNorm<Float>
  public var seBlock: SqueezeExcitationBlock
  public var conv2: Conv2D<Float>
  public var batchNormConv2: BatchNorm<Float>

  public init(
    filters: (Int, Int),
    expansionFactor: Float,
    strides: (Int, Int) = (1, 1),
    kernel: (Int, Int) = (3, 3),
    seLayer: Bool = false,
    activation: ActivationType = .relu
  ) {
    self.strides = strides
    self.addResLayer = filters.0 == filters.1 && strides == (1, 1)
    self.useSELayer = seLayer
    self.activation = activation

    let filterMult = filters
    let hiddenDimension = Int(Float(filterMult.0) * expansionFactor)
    let reducedDimension = hiddenDimension / 4

    conv1 = Conv2D<Float>(
      filterShape: (1, 1, filterMult.0, hiddenDimension),
      strides: (1, 1),
      padding: .same)
    dConv = DepthwiseConv2D<Float>(
      filterShape: (kernel.0, kernel.1, hiddenDimension, 1),
      strides: strides,
      padding: strides == (1, 1) ? .same : .valid)
    seBlock = SqueezeExcitationBlock(
      inputOutputSize: hiddenDimension, reducedSize: reducedDimension)
    conv2 = Conv2D<Float>(
      filterShape: (1, 1, hiddenDimension, filterMult.1),
      strides: (1, 1),
      padding: .same)
    batchNormConv1 = BatchNorm(featureCount: hiddenDimension)
    batchNormDConv = BatchNorm(featureCount: hiddenDimension)
    batchNormConv2 = BatchNorm(featureCount: filterMult.1)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    var piecewise = batchNormConv1(conv1(input))
    switch self.activation {
    case .hardSwish: piecewise = hardSwish(piecewise)
    case .relu: piecewise = relu(piecewise)
    }
    var depthwise: Tensor<Float>
    if self.strides == (1, 1) {
      depthwise = batchNormDConv(dConv(piecewise))
    } else {
      depthwise = batchNormDConv(dConv(zeroPad(piecewise)))
    }
    switch self.activation {
    case .hardSwish: depthwise = hardSwish(depthwise)
    case .relu: depthwise = relu(depthwise)
    }
    var squeezeExcite: Tensor<Float>
    if self.useSELayer {
      squeezeExcite = seBlock(depthwise)
    } else {
      squeezeExcite = depthwise
    }

    let piecewiseLinear = batchNormConv2(conv2(squeezeExcite))

    if self.addResLayer {
      return input + piecewiseLinear
    } else {
      return piecewiseLinear
    }
  }
}

public struct MobileNetV3Large: Layer {
  @noDerivative public let zeroPad = ZeroPadding2D<Float>(padding: ((0, 1), (0, 1)))
  public var inputConv: Conv2D<Float>
  public var inputConvBatchNorm: BatchNorm<Float>

  public var invertedResidualBlock1: InitialInvertedResidualBlock
  public var invertedResidualBlock2: InvertedResidualBlock
  public var invertedResidualBlock3: InvertedResidualBlock
  public var invertedResidualBlock4: InvertedResidualBlock
  public var invertedResidualBlock5: InvertedResidualBlock
  public var invertedResidualBlock6: InvertedResidualBlock
  public var invertedResidualBlock7: InvertedResidualBlock
  public var invertedResidualBlock8: InvertedResidualBlock
  public var invertedResidualBlock9: InvertedResidualBlock
  public var invertedResidualBlock10: InvertedResidualBlock
  public var invertedResidualBlock11: InvertedResidualBlock
  public var invertedResidualBlock12: InvertedResidualBlock
  public var invertedResidualBlock13: InvertedResidualBlock
  public var invertedResidualBlock14: InvertedResidualBlock
  public var invertedResidualBlock15: InvertedResidualBlock

  public var outputConv: Conv2D<Float>
  public var outputConvBatchNorm: BatchNorm<Float>

  public var avgPool = GlobalAvgPool2D<Float>()
  public var finalConv: Conv2D<Float>
  public var dropoutLayer: Dropout<Float>
  public var classiferConv: Conv2D<Float>
  public var flatten = Flatten<Float>()

  @noDerivative public var lastConvChannel: Int

  public init(classCount: Int = 1000, dropout: Double = 0.2) {
    inputConv = Conv2D<Float>(
      filterShape: (3, 3, 3, 16),
      strides: (2, 2),
      padding: .same)
    inputConvBatchNorm = BatchNorm(
      featureCount: 16)

    invertedResidualBlock1 = InitialInvertedResidualBlock(
      filters: (16, 16))
    invertedResidualBlock2 = InvertedResidualBlock(
      filters: (16, 24),
      expansionFactor: 4, strides: (2, 2))
    invertedResidualBlock3 = InvertedResidualBlock(
      filters: (24, 24),
      expansionFactor: 3)
    invertedResidualBlock4 = InvertedResidualBlock(
      filters: (24, 40),
      expansionFactor: 3, strides: (2, 2), kernel: (5, 5), seLayer: true)
    invertedResidualBlock5 = InvertedResidualBlock(
      filters: (40, 40),
      expansionFactor: 3, kernel: (5, 5), seLayer: true)
    invertedResidualBlock6 = InvertedResidualBlock(
      filters: (40, 40),
      expansionFactor: 3, kernel: (5, 5), seLayer: true)
    invertedResidualBlock7 = InvertedResidualBlock(
      filters: (40, 80),
      expansionFactor: 6, strides: (2, 2), activation: .hardSwish)
    invertedResidualBlock8 = InvertedResidualBlock(
      filters: (80, 80),
      expansionFactor: 2.5, activation: .hardSwish)
    invertedResidualBlock9 = InvertedResidualBlock(
      filters: (80, 80),
      expansionFactor: 184 / 80.0, activation: .hardSwish)
    invertedResidualBlock10 = InvertedResidualBlock(
      filters: (80, 80),
      expansionFactor: 184 / 80.0, activation: .hardSwish)
    invertedResidualBlock11 = InvertedResidualBlock(
      filters: (80, 112),
      expansionFactor: 6, seLayer: true, activation: .hardSwish)
    invertedResidualBlock12 = InvertedResidualBlock(
      filters: (112, 112),
      expansionFactor: 6, seLayer: true, activation: .hardSwish)
    invertedResidualBlock13 = InvertedResidualBlock(
      filters: (112, 160),
      expansionFactor: 6, strides: (2, 2), kernel: (5, 5), seLayer: true,
      activation: .hardSwish)
    invertedResidualBlock14 = InvertedResidualBlock(
      filters: (160, 160),
      expansionFactor: 6, kernel: (5, 5), seLayer: true, activation: .hardSwish)
    invertedResidualBlock15 = InvertedResidualBlock(
      filters: (160, 160),
      expansionFactor: 6, kernel: (5, 5), seLayer: true, activation: .hardSwish)

    lastConvChannel = 960
    outputConv = Conv2D<Float>(
      filterShape: (
        1, 1, 160, lastConvChannel
      ),
      strides: (1, 1),
      padding: .same)
    outputConvBatchNorm = BatchNorm(featureCount: lastConvChannel)

    let lastPointChannel = 1280
    finalConv = Conv2D<Float>(
      filterShape: (1, 1, lastConvChannel, lastPointChannel),
      strides: (1, 1),
      padding: .same)
    dropoutLayer = Dropout<Float>(probability: dropout)
    classiferConv = Conv2D<Float>(
      filterShape: (1, 1, lastPointChannel, classCount),
      strides: (1, 1),
      padding: .same)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    let initialConv = hardSwish(
      input.sequenced(through: zeroPad, inputConv, inputConvBatchNorm))
    let backbone1 = initialConv.sequenced(
      through: invertedResidualBlock1,
      invertedResidualBlock2, invertedResidualBlock3, invertedResidualBlock4, invertedResidualBlock5)
    let backbone2 = backbone1.sequenced(
      through: invertedResidualBlock6, invertedResidualBlock7,
      invertedResidualBlock8, invertedResidualBlock9, invertedResidualBlock10)
    let backbone3 = backbone2.sequenced(
      through: invertedResidualBlock11,
      invertedResidualBlock12, invertedResidualBlock13, invertedResidualBlock14, invertedResidualBlock15)
    let outputConvResult = hardSwish(outputConvBatchNorm(outputConv(backbone3)))
    let averagePool = avgPool(outputConvResult).reshaped(to: [
      input.shape[0], 1, 1, self.lastConvChannel,
    ])
    let finalConvResult = dropoutLayer(hardSwish(finalConv(averagePool)))
    return flatten(classiferConv(finalConvResult))
  }
}

public struct MobileNetV3Small: Layer {
  @noDerivative public let zeroPad = ZeroPadding2D<Float>(padding: ((0, 1), (0, 1)))
  public var inputConv: Conv2D<Float>
  public var inputConvBatchNorm: BatchNorm<Float>

  public var invertedResidualBlock1: InitialInvertedResidualBlock
  public var invertedResidualBlock2: InvertedResidualBlock
  public var invertedResidualBlock3: InvertedResidualBlock
  public var invertedResidualBlock4: InvertedResidualBlock
  public var invertedResidualBlock5: InvertedResidualBlock
  public var invertedResidualBlock6: InvertedResidualBlock
  public var invertedResidualBlock7: InvertedResidualBlock
  public var invertedResidualBlock8: InvertedResidualBlock
  public var invertedResidualBlock9: InvertedResidualBlock
  public var invertedResidualBlock10: InvertedResidualBlock
  public var invertedResidualBlock11: InvertedResidualBlock

  public var outputConv: Conv2D<Float>
  public var outputConvBatchNorm: BatchNorm<Float>

  public var avgPool = GlobalAvgPool2D<Float>()
  public var finalConv: Conv2D<Float>
  public var dropoutLayer: Dropout<Float>
  public var classiferConv: Conv2D<Float>
  public var flatten = Flatten<Float>()

  @noDerivative public var lastConvChannel: Int

  public init(classCount: Int = 1000, dropout: Double = 0.2) {
    inputConv = Conv2D<Float>(
      filterShape: (3, 3, 3, 16),
      strides: (2, 2),
      padding: .same)
    inputConvBatchNorm = BatchNorm(
      featureCount: 16)

    invertedResidualBlock1 = InitialInvertedResidualBlock(
      filters: (16, 16),
      strides: (2, 2), seLayer: true)
    invertedResidualBlock2 = InvertedResidualBlock(
      filters: (16, 24),
      expansionFactor: 72.0 / 16.0, strides: (2, 2))
    invertedResidualBlock3 = InvertedResidualBlock(
      filters: (24, 24),
      expansionFactor: 88.0 / 24.0)
    invertedResidualBlock4 = InvertedResidualBlock(
      filters: (24, 40),
      expansionFactor: 4, strides: (2, 2), kernel: (5, 5), seLayer: true,
      activation: .hardSwish)
    invertedResidualBlock5 = InvertedResidualBlock(
      filters: (40, 40),
      expansionFactor: 6, kernel: (5, 5), seLayer: true, activation: .hardSwish)
    invertedResidualBlock6 = InvertedResidualBlock(
      filters: (40, 40),
      expansionFactor: 6, kernel: (5, 5), seLayer: true, activation: .hardSwish)
    invertedResidualBlock7 = InvertedResidualBlock(
      filters: (40, 48),
      expansionFactor: 3, kernel: (5, 5), seLayer: true, activation: .hardSwish)
    invertedResidualBlock8 = InvertedResidualBlock(
      filters: (48, 48),
      expansionFactor: 3, kernel: (5, 5), seLayer: true, activation: .hardSwish)
    invertedResidualBlock9 = InvertedResidualBlock(
      filters: (48, 96),
      expansionFactor: 6, strides: (2, 2), kernel: (5, 5), seLayer: true,
      activation: .hardSwish)
    invertedResidualBlock10 = InvertedResidualBlock(
      filters: (96, 96),
      expansionFactor: 6, kernel: (5, 5), seLayer: true, activation: .hardSwish)
    invertedResidualBlock11 = InvertedResidualBlock(
      filters: (96, 96),
      expansionFactor: 6, kernel: (5, 5), seLayer: true, activation: .hardSwish)

    lastConvChannel = 576
    outputConv = Conv2D<Float>(
      filterShape: (
        1, 1, 96, lastConvChannel
      ),
      strides: (1, 1),
      padding: .same)
    outputConvBatchNorm = BatchNorm(featureCount: lastConvChannel)

    let lastPointChannel = 1280
    finalConv = Conv2D<Float>(
      filterShape: (1, 1, lastConvChannel, lastPointChannel),
      strides: (1, 1),
      padding: .same)
    dropoutLayer = Dropout<Float>(probability: dropout)
    classiferConv = Conv2D<Float>(
      filterShape: (1, 1, lastPointChannel, classCount),
      strides: (1, 1),
      padding: .same)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    let initialConv = hardSwish(
      input.sequenced(through: zeroPad, inputConv, inputConvBatchNorm))
    let backbone1 = initialConv.sequenced(
      through: invertedResidualBlock1,
      invertedResidualBlock2, invertedResidualBlock3, invertedResidualBlock4, invertedResidualBlock5)
    let backbone2 = backbone1.sequenced(
      through: invertedResidualBlock6, invertedResidualBlock7,
      invertedResidualBlock8, invertedResidualBlock9, invertedResidualBlock10, invertedResidualBlock11)
    let outputConvResult = hardSwish(outputConvBatchNorm(outputConv(backbone2)))
    let averagePool = avgPool(outputConvResult).reshaped(to: [
      input.shape[0], 1, 1, lastConvChannel,
    ])
    let finalConvResult = dropoutLayer(hardSwish(finalConv(averagePool)))
    return flatten(classiferConv(finalConvResult))
  }
}

```

### 结果

该网络将被训练为比 EfficientNet 稍差，但是可以在移动设备上快速评估。此外，生成的网络很小，因此可以很容易地通过网络发送到边缘设备。

```py
Starting training...
[Epoch 1] Accuracy: 50/500 (0.1) Loss: 3.3504734
[Epoch 2] Accuracy: 253/500 (0.506) Loss: 1.4156498
[Epoch 3] Accuracy: 335/500 (0.67) Loss: 1.0543922
[Epoch 4] Accuracy: 326/500 (0.652) Loss: 1.1357045
[Epoch 5] Accuracy: 353/500 (0.706) Loss: 0.9812555
[Epoch 6] Accuracy: 350/500 (0.7) Loss: 0.9210515
[Epoch 7] Accuracy: 380/500 (0.76) Loss: 0.7407557
[Epoch 8] Accuracy: 347/500 (0.694) Loss: 1.038017
[Epoch 9] Accuracy: 343/500 (0.686) Loss: 1.0409927

[Epoch 10] Accuracy: 377/500 (0.754) Loss: 0.8882818
[Epoch 11] Accuracy: 381/500 (0.762) Loss: 0.9374979
[Epoch 12] Accuracy: 383/500 (0.766) Loss: 0.8867029
[Epoch 13] Accuracy: 365/500 (0.73) Loss: 1.3112245
[Epoch 14] Accuracy: 377/500 (0.754) Loss: 0.9881239
[Epoch 15] Accuracy: 386/500 (0.772) Loss: 0.99048007
[Epoch 16] Accuracy: 406/500 (0.812) Loss: 0.78758305
[Epoch 17] Accuracy: 402/500 (0.804) Loss: 0.8263649
[Epoch 18] Accuracy: 407/500 (0.814) Loss: 0.8147187
[Epoch 19] Accuracy: 401/500 (0.802) Loss: 0.8540674
[Epoch 20] Accuracy: 387/500 (0.774) Loss: 0.90144944
[Epoch 21] Accuracy: 404/500 (0.808) Loss: 1.0089223
[Epoch 22] Accuracy: 396/500 (0.792) Loss: 0.97762024
[Epoch 23] Accuracy: 399/500 (0.798) Loss: 0.9001269
[Epoch 24] Accuracy: 389/500 (0.778) Loss: 1.1596041
[Epoch 25] Accuracy: 384/500 (0.768) Loss: 1.235701
[Epoch 26] Accuracy: 396/500 (0.792) Loss: 1.0384445
[Epoch 27] Accuracy: 405/500 (0.81) Loss: 0.9806802
[Epoch 28] Accuracy: 405/500 (0.81) Loss: 0.9442753

[Epoch 29] Accuracy: 411/500 (0.822) Loss: 0.85053337
[Epoch 30] Accuracy: 422/500 (0.844) Loss: 0.8129424

```

#### EfficientNet-边缘

同样，我们可以使用 EfficientNet 搜索策略为移动设备构建网络；我们可以用它来为更小的设备构建网络。谷歌已经生产了一系列小型 ASIC 设备(Coral 是品牌名称)，称为 EdgeTPU，可以插入你的计算机，让我们在自己的硬件上运行 tensorflow lite 模型。从概念上讲，这些设备的内存空间和计算能力极其有限，但它们是 AI 硬件，就像我们的显卡一样。通过为 EfficientNet 搜索算法提供设备限制，他们能够发现一组最佳网络，在计算能力极其有限的设备上运行。

## 概述

在过去的几章中，我们已经从小网络发展到大网络，现在我们又回到小网络。这些研究领域都越来越紧密，相互关联。现在让我们来看看如何将它应用到你自己的工作中。