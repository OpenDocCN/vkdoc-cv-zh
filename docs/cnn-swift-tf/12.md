# 12.诡计多端

在这一章中，我们将研究如何通过结合多种不同的方法来修改我们最初的 ResNet 50 网络，以达到与 EfficientNet 几乎一样准确的结果。

你已经走到这一步了。我们已经从使用神经网络来执行图像识别的最基础发展到了该领域的最新水平。现在请允许我对我的方法提出一些限制条件。首先，我已经在这个领域开辟了一条非常直接的道路，目标是让新人的早期阶段尽可能简单。在这个过程中，我跳过了许多历史、重要的里程碑和大量的研究。有许多我没有提到的不同的论文和方法包含了有趣的想法，你应该看看。简而言之，进步永远不会像我在这里试图展示的那样是线性的。通常有许多随机的方法，错误的开始导致死胡同，并且尝试了许多不同的东西，其中只有一小部分真正起作用。进步通常是丑陋而乏味的。

## 诡计多端

让我们来看一个有时被称为锦囊妙计式方法的例子。一般来说，有人会想出一个新奇的想法，然后发表在报纸上。我们已经看到了十几个这样的例子。然后，各种各样的其他研究人员和团体将试图把它与尽可能多的其他不同方法结合在一起，试图找到一种产生新结果的神奇组合。在高层次上，这可能是 NASNet 的学术版本。经常发生的是，人们发现有其他方法可以得到相同的结果，可以说，最初的研究人员最终陷入了局部极值。

>在卷积神经网络中混合组合技术的性能改进

```py
> https://arxiv.org/abs/2001.06268

```

让我们来看看最近的一篇论文，“在卷积神经网络中复合汇编技术的性能改进”，作为这方面的一个例子。Lee 等人在前几章中采用了我们相同的 ResNet 50 方法，并发现如何修改它，以更低的成本产生几乎与 EfficientNet 一样好的结果。

他们对我们之前看到的基本网络进行了如下调整:

*   用 3×3 步幅 2+3×3+3×3 卷积方法替换了 ResNet 50 的 7×7 磁头

*   从 ResNet 50 模块中的初始 1x1 卷积中移除 2x2 步幅，并将其添加到 3x3 卷积中

*   添加了 Averagepool2d 步骤作为跳过连接卷积层的一部分

*   增加了一个频道关注(CA)操作员

*   选择性内核(SK)块

*   大小网块跳过连接

为了做到这一点，他们使用了以下图像识别纸:

>使用卷积神经网络进行图像分类的技巧包

```py
> https://arxiv.org/abs/1812.01187

```

>选择性核心网络

```py
> https://arxiv.org/abs/1903.06586

```

>大-小网络:用于视觉和语音识别的有效多尺度特征表示

```py
> https://arxiv.org/abs/1807.03848

```

>再次使卷积网络保持平移不变

```py
> https://arxiv.org/abs/1904.11486

```

此外，他们使用以下数据扩充/训练/标准化技术:

>通过惩罚置信输出分布来调整神经网络

```py
> https://arxiv.org/abs/1701.06548

```

>自动增强:从数据中学习增强策略

```py
> https://arxiv.org/abs/1805.09501

```

>混淆:超越经验风险最小化

```py
> https://arxiv.org/abs/1710.09412

```

>提取神经网络中的知识

```py
> https://arxiv.org/abs/1503.02531

```

> DropBlock:卷积网络的正则化方法

```py
> https://arxiv.org/abs/1810.12890

```

### 从中可以学到什么

对我来说，这就是为什么我不会对研究小组在解决问题上投入越来越多的计算能力感到不安。即使他们的方法可以总结为蛮力，在证明更大规模的方法有效时，他们为个体研究人员打开了大门，使他们能够用更简单的硬件复制他们的结果。

我的经验是事情通常是这样的:

*   许多研究人员试图找到小的新方法，因为他们没有大规模的机器。

*   有人发现了能产生持续改进的东西(例如，人们可以复制他们的结果)。

*   大型研究团队会投入大量计算资源来解决这个问题。几个月后，他们发表了尝试测量的结果。

缩放通常如下所示:

*   原研究员:适马 0.5 改进。

*   10 倍集群:适马 0.85 改进。

*   100 倍集群:适马 0.95 改进。

*   世界上所有的计算能力:适马 0.985 改进。

*   六个月后:有人发现如何用有限的计算资源复制大型集群的工作，然后循环往复。

*   与此同时，许多不知名的小研究人员正在发表被完全忽视的新发现。

*   有人发表了一篇博客文章，然后像病毒一样传播开来，我们又回到了起点。

## 阅读报纸

要在这个领域取得成功，你需要的关键技能不是最前沿的网络理论或最快的计算机，这两者都很可能在一年内过时。取而代之的是一项永恒的技能，那就是独立阅读论文并跟上进度的能力。当你在论文中遇到你不理解的东西时，你需要能够查阅论文的参考文献，并找出他们的想法是从哪里来的。如果你追溯到足够远的地方，这些参考文献往往会集中在几个关键概念上。学会这些，无论你想做什么，你都会有一个坚实的基础。

### 保持在曲线后面

数量惊人的论文问世，轰动一时，然后销声匿迹。我发现试图跟上最新的发展很有可能会偏离正题。相反，我的建议是落后趋势几个月。让其他人阅读最新和最伟大的作品，然后等待他们实际证明事情是可行的。在 GitHub 上寻找展示新事物如何工作的代码演示，或者在 Jupyter 笔记本上寻找解释正在发生什么的代码演示。对我来说，这就是为什么你也应该选择其他框架(例如 pytorch ),因为这样你就可以更容易地从不断测试新想法的更广泛的机器学习研究人员社区中吸收知识。

找几个研究人员在 Twitter 上关注，然后看看他们在读什么，谈论什么。让他们为你过滤。也许从博弈论的角度来看，如果每个人都这样做，进步将会停滞不前，但除非你是这些领域的领先研究者，否则陷入死胡同的可能性很高。

举一个不同的例子，我们可以考虑学习、理解和运行这些不同测试数据集的模型所需的工作，如下所示:

*   一分钟

*   CIFAR: 1 小时

*   Imagenette: 1 天

*   ImageNet: 1 个月

这些数字是我瞎编的，不要想太多。那么，对我来说，你应该在小网络上比在大网络上多花一个数量级的钱。在你跳到 CIFAR 之前，你应该做十几次不同的 MNIST；在跳转到 Imagenette 之前，你要做十几次 CIFAR 诸如此类。对于运行一次 ImageNet 所需的计算，你可以在一台基本的计算机上以一千种不同的方式运行 MNIST，但是找到这样做的人是极其罕见的，尽管所需的资源应该是任何人都可以获得的。

对我来说，很难与拥有大型集群的高端研究团队竞争，这些集群拥有最新、最棒的硬件，能够进行大规模的大规模实验。但是我们能超越他们的地方很简单，那就是在一个特定的问题上比其他人做得更深入。大型研究小组的成功也是他们的弱点，因为他们不断寻找新的方法来产生可发表的结果。如果这对你来说不重要，那么你可以比他们花更多的时间在杂草上。通过扩展，你可以发现他们在匆忙获得结果时错过了什么。

### 我是如何阅读报纸的

通常，我会阅读摘要，并希望对论文内容有一个高层次的理解。我很高兴地承认，我经常阅读摘要和前几段，感觉我不知道到底发生了什么。有时候论文涉及的范围太广，以至于无法用几句话来概括(或者可能不是非常清楚)，所以我认为作者和我都有责任。我通常直接跳到图表上，希望这些图表能让我直观地了解这篇论文到底想做什么。如果失败了，我将宣读结论。如果所有这些都失败了，那么我会坐下来，试着略读这篇论文，试图通过这种方式获得高层次的理解。我的基本过程是试图获得高层次的理解，然后进行连续的重读，直到我真正理解了正在发生的事情。

如果我觉得文件很重要，我喜欢把它们打印出来，然后看着它们。在空白处做笔记也是我的一种做法。如果你经常在旅途中，能够在你的笔记本电脑上携带数千份数字形式的作品是很好的，但我已经慢慢积累了一批我认为手边放着很重要的作品。

最后，慢慢来！深度远比广度更有价值。我发现，找到几篇真正有趣的论文并花时间彻底理解它们，比试图涉足一堆随机领域要好得多。

## 概述

我们已经分解了这个领域的一篇论文，试图通过结合来自学术界的半打其他技术来构建一个高效的网络级性能。我们已经讨论了如何开始自己阅读论文。