# 8\. 移动网络 v1

有一些有趣的尝试，让更小的模型运行在设备后 SqueezeNet。我们需要的是一个专门为移动设备设计的模型。谷歌的一组研究人员开发的东西被称为 MobileNet，这是一个重要的网络家族，我们将花几章时间来了解它。在高层次上，我们将使用深度方向可分卷积来产生一个比 SqueezeNet 更精确的网络，它在移动电话硬件上运行良好。

## MobileNet （v1）

> MobileNets:用于移动视觉应用的高效卷积神经网络

```
> https://arxiv.org/abs/1704.04861

```

专为在移动硬件上运行而设计的模型，更好地利用了参数+数据空间。

### 空间可分卷积

让我们再来看看我们的 Sobel 滤波器，在我们介绍卷积的那一章。在那里，我们把它看作是两个 3×3 的矩阵运算。但是如果我们在数学上聪明，我们可以把它简化为一个[3x1]和[1x3]的乘法。

这给了我们同样的结果，但有额外的属性，它可以更便宜地计算。我们的[3x 3][3x 3]组合最终需要 9 次运算，而我们的[3x 1][1x 3]只需要 6 次运算，减少了 33%!然而，并不是所有的内核都可以这样分解。

### 深度方向回旋

我们可以利用图像数据中的一个关键属性:颜色。我们有三个通道——红色、绿色、蓝色——每次评估我们的神经网络时，我们都在运行相同的过滤操作集。

我们可以为输入图像的每个区域创建单独的卷积滤波器组，通过颜色通道组合在一起。在学术设置中，通道也被称为深度，因此这些被称为深度方向卷积。你需要知道的另一种方法是增加滤波器输出的数量，称为通道乘法器。

### 逐点卷积

这只是谜题的一半；我们仍然需要将我们的渠道数据重新组合在一起。在关于 SqueezeNet 的上一章中，我们讨论了如何在应用 3×3 卷积之前，将 1×1 卷积放入堆栈中，以显著减少数据量。从概念上讲，这被称为逐点卷积，因为所有通道输入数据都要通过它。通过使用这些逐点卷积，我们可以将减少的数据空间映射回我们期望的最终过滤器大小。然后，我们只需要增加逐点操作符的数量，以匹配我们想要的输出过滤器的数量。

从概念上讲，我们获取输入图像并运行多组深度方向卷积，然后使用一堆小的点方向卷积将它们组合回我们想要的输出形状。滤波器的这种组合称为深度方向可分离卷积，是该网络性能的关键。我们已经获得了 SqueezeNet 压缩方法的大部分好处，但破坏性比 SqueezeNet 小。此外，我们现在使用更便宜的运算，因为深度可分卷积可以在移动硬件中加速。

## ReLU 6

到目前为止，我们已经为我们的模型使用了一个 ReLU 激活函数，如下所示:

```
relu(x) = max(features, 0)

```

当构建我们知道将要量化的模型时，限制 ReLU 层的输出并通过扩展迫使网络从一开始就使用较小的数字是有价值的。因此，我们简单地为我们的 ReLU 激活引入一个上限函数，如下所示:

```
relu6(x) = min(max(features, 0), 6)

```

现在，我们可以简化我们的输出逻辑，以利用这一减少的空间。

### 使用这种方法减少 MAC 的示例

>代表性深度神经网络架构的基准分析

```
> https://arxiv.org/abs/1810.00736

```

这篇文章的第 3 页有一张漂亮的图表，直观地展示了这些网络之间的差异。从概念上讲，我们有一个比 SqueezeNet 稍大的网络，但我们有一个与 ResNet 18(早期的 ResNet 34 的较小版本)相当的顶级精度。如果你想知道我们接下来要去哪里，看看 VGG16 与 MobileNet v2 的对比。

## 密码

这个网络比我们的 SqueezeNet 方法使用了更多类型的层，但产生了明显更好的结果，因为它们在计算上更便宜。这是我们将会反复看到的事情。

```
```
import TensorFlow

public struct ConvBlock: Layer {
  public var zeroPad = ZeroPadding2D<Float>(padding: ((0, 1), (0, 1)))
  public var conv: Conv2D<Float>
  public var batchNorm: BatchNorm<Float>

  public init(filterCount: Int, strides: (Int, Int)) {
    conv = Conv2D<Float>(
      filterShape: (3, 3, 3, filterCount),
      strides: strides,
      padding: .valid)
    batchNorm = BatchNorm<Float>(featureCount: filterCount)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    let convolved = input.sequenced(through: zeroPad, conv, batchNorm)
    return relu6(convolved)
  }
}

public struct DepthwiseConvBlock: Layer {
  @noDerivative let strides: (Int, Int)
  @noDerivative public let zeroPad = ZeroPadding2D<Float>(padding: ((0, 1), (0, 1)))

  public var dConv: DepthwiseConv2D<Float>
  public var batchNorm1: BatchNorm<Float>
  public var conv: Conv2D<Float>
  public var batchNorm2: BatchNorm<Float>

  public init(
    filterCount: Int, pointwiseFilterCount: Int,
    strides: (Int, Int)
  ) {
    self.strides = strides
    dConv = DepthwiseConv2D<Float>(
      filterShape: (3, 3, filterCount, 1),
      strides: strides,
      padding: strides == (1, 1) ? .same : .valid)
    batchNorm1 = BatchNorm<Float>(
      featureCount: filterCount)
    conv = Conv2D<Float>(
      filterShape: (
        1, 1, filterCount,
        pointwiseFilterCount
      ),
      strides: (1, 1),
      padding: .same)
    batchNorm2 = BatchNorm<Float>(featureCount: pointwiseFilterCount)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    var convolved1: Tensor<Float>
    if self.strides == (1, 1) {
      convolved1 = input.sequenced(through: dConv, batchNorm1)
    } else {
      convolved1 = input.sequenced(through: zeroPad, dConv, batchNorm1)
    }
    let convolved2 = relu6(convolved1)
    let convolved3 = relu6(convolved2.sequenced(through: conv, batchNorm2))
    return convolved3
  }
}

public struct MobileNetV1: Layer {
  @noDerivative let classCount: Int
  @noDerivative let scaledFilterShape: Int

  public var convBlock1: ConvBlock
  public var dConvBlock1: DepthwiseConvBlock
  public var dConvBlock2: DepthwiseConvBlock
  public var dConvBlock3: DepthwiseConvBlock
  public var dConvBlock4: DepthwiseConvBlock
  public var dConvBlock5: DepthwiseConvBlock
  public var dConvBlock6: DepthwiseConvBlock
  public var dConvBlock7: DepthwiseConvBlock
  public var dConvBlock8: DepthwiseConvBlock
  public var dConvBlock9: DepthwiseConvBlock
  public var dConvBlock10: DepthwiseConvBlock
  public var dConvBlock11: DepthwiseConvBlock
  public var dConvBlock12: DepthwiseConvBlock
  public var dConvBlock13: DepthwiseConvBlock
  public var avgPool = GlobalAvgPool2D<Float>()
  public var dropoutLayer: Dropout<Float>
  public var outputConv: Conv2D<Float>

  public init(
    classCount: Int = 10,
    dropout: Double = 0.001
  ) {
    self.classCount = classCount
    scaledFilterShape = Int(1024.0 * 1.0)

    convBlock1 = ConvBlock(filterCount: 32, strides: (2, 2))
    dConvBlock1 = DepthwiseConvBlock(
      filterCount: 32,
      pointwiseFilterCount: 64,
      strides: (1, 1))
    dConvBlock2 = DepthwiseConvBlock(
      filterCount: 64,
      pointwiseFilterCount: 128,
      strides: (2, 2))
    dConvBlock3 = DepthwiseConvBlock(
      filterCount: 128,
      pointwiseFilterCount: 128,
      strides: (1, 1))
    dConvBlock4 = DepthwiseConvBlock(
      filterCount: 128,
      pointwiseFilterCount: 256,
      strides: (2, 2))
    dConvBlock5 = DepthwiseConvBlock(
      filterCount: 256,
      pointwiseFilterCount: 256,
      strides: (1, 1))
    dConvBlock6 = DepthwiseConvBlock(
      filterCount: 256,
      pointwiseFilterCount: 512,
      strides: (2, 2))
    dConvBlock7 = DepthwiseConvBlock(
      filterCount: 512,
      pointwiseFilterCount: 512,
      strides: (1, 1))
    dConvBlock8 = DepthwiseConvBlock(
      filterCount: 512,
      pointwiseFilterCount: 512,
      strides: (1, 1))
    dConvBlock9 = DepthwiseConvBlock(
      filterCount: 512,
      pointwiseFilterCount: 512,
      strides: (1, 1))
    dConvBlock10 = DepthwiseConvBlock(
      filterCount: 512,
      pointwiseFilterCount: 512,
      strides: (1, 1))
    dConvBlock11 = DepthwiseConvBlock(
      filterCount: 512,
      pointwiseFilterCount: 512,
      strides: (1, 1))
    dConvBlock12 = DepthwiseConvBlock(
      filterCount: 512,
      pointwiseFilterCount: 1024,
      strides: (2, 2))
    dConvBlock13 = DepthwiseConvBlock(
      filterCount: 1024,
      pointwiseFilterCount: 1024,
      strides: (1, 1))

    dropoutLayer = Dropout<Float>(probability: dropout)
    outputConv = Conv2D<Float>(
      filterShape: (1, 1, scaledFilterShape, classCount),
      strides: (1, 1),
      padding: .same)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    let convolved = input.sequenced(
      through: convBlock1, dConvBlock1,
      dConvBlock2, dConvBlock3, dConvBlock4)
    let convolved2 = convolved.sequenced(
      through: dConvBlock5, dConvBlock6,
      dConvBlock7, dConvBlock8, dConvBlock9)
    let convolved3 = convolved2.sequenced(
      through: dConvBlock10, dConvBlock11, dConvBlock12, dConvBlock13, avgPool
    ).reshaped(to: [
      input.shape[0], 1, 1, scaledFilterShape,
    ])
    let convolved4 = convolved3.sequenced(through: dropoutLayer, outputConv)
    return convolved4.reshaped(to: [input.shape[0], classCount])
  }
}
```

```

### 结果

我们的结果与之前的 Resnet 50 网络相当，但这个网络总体上更小，并且可以在运行时更快地进行评估，因此对于移动设备来说是一个坚实的改进。

```
Starting training...
[Epoch 1 ] Accuracy: 50/500  (0.1)   Loss: 2.5804458
[Epoch 2 ] Accuracy: 262/500 (0.524) Loss: 1.5034955
[Epoch 3 ] Accuracy: 224/500 (0.448) Loss: 1.928577
[Epoch 4 ] Accuracy: 286/500 (0.572) Loss: 1.4074985
[Epoch 5 ] Accuracy: 306/500 (0.612) Loss: 1.3206513
[Epoch 6 ] Accuracy: 334/500 (0.668) Loss: 1.0112444
[Epoch 7 ] Accuracy: 362/500 (0.724) Loss: 0.8360394
[Epoch 8 ] Accuracy: 343/500 (0.686) Loss: 1.0489439
[Epoch 9 ] Accuracy: 317/500 (0.634) Loss: 1.6159635
[Epoch 10] Accuracy: 338/500 (0.676) Loss: 1.0420185
[Epoch 11] Accuracy: 354/500 (0.708) Loss: 1.0034739
[Epoch 12] Accuracy: 358/500 (0.716) Loss: 0.9746185
[Epoch 13] Accuracy: 344/500 (0.688) Loss: 1.152486
[Epoch 14] Accuracy: 365/500 (0.73)  Loss: 0.96197647
[Epoch 15] Accuracy: 353/500 (0.706) Loss: 1.2438473
[Epoch 16] Accuracy: 367/500 (0.734) Loss: 1.044013
[Epoch 17] Accuracy: 365/500 (0.73)  Loss: 1.1098087
[Epoch 18] Accuracy: 352/500 (0.704) Loss: 1.3609929
[Epoch 19] Accuracy: 376/500 (0.752) Loss: 1.2861694
[Epoch 20] Accuracy: 376/500 (0.752) Loss: 1.0280938
[Epoch 21] Accuracy: 369/500 (0.738) Loss: 1.1655327
[Epoch 22] Accuracy: 369/500 (0.738) Loss: 1.1702954
[Epoch 23] Accuracy: 363/500 (0.726) Loss: 1.151112
[Epoch 24] Accuracy: 378/500 (0.756) Loss: 0.94088197

[Epoch 25] Accuracy: 386/500 (0.772) Loss: 1.03443
[Epoch 26] Accuracy: 379/500 (0.758) Loss: 1.1582794
[Epoch 27] Accuracy: 384/500 (0.768) Loss: 1.1210178
[Epoch 28] Accuracy: 377/500 (0.754) Loss: 1.136668
[Epoch 29] Accuracy: 382/500 (0.764) Loss: 1.2300915
[Epoch 30] Accuracy: 381/500 (0.762) Loss: 1.0231776

```

## 概述

我们已经研究了 MobileNet，这是一个来自 2017 年的重要计算机视觉网络，它大量使用深度方向可分离卷积，以便以显著降低的尺寸和计算预算产生与 ResNet 18(我们的 ResNet 34 网络的较小版本)相当的结果。我们可以用现在的硬件在手机上以接近实时的速度(例如，50 毫秒/预测速度)运行这个程序。接下来，让我们看看如何稍微调整我们的 MobileNet 网络，以产生更好的结果。