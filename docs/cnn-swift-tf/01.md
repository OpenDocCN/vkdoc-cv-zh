# 一、MNIST：1D 神经网络

在本章中，我们将研究一个名为 MNIST 的简单图像识别数据集，并构建一个基本的一维神经网络，通常称为多层感知器，以对我们的数字进行分类，并对黑白图像进行分类。

## 数据集概述

MNIST(修正的国家标准和技术研究所)是 1999 年建立的数据集，是计算机视觉问题的一个极其重要的试验台。你会在这个领域的学术论文中随处看到它，它被认为是相当于 hello world 的计算机视觉。它是数字 0-9 的手绘数字的预处理灰度图像的集合。每幅图像宽 28×28 像素，总共 784 像素。对于每个像素，都有相应的 8 位灰度值，即从 0(白色)到 255(全黑)的数字。

首先，我们甚至不会把它当作实际的图像数据。我们将展开它——我们将从最上面一行开始，一次抽出每一行，直到我们得到一长串数字。我们可以想象将这个概念扩展到 28×28 像素，以产生一长行输入值，这是一个 784 像素长和 1 像素宽的向量，每个向量都有一个从 0 到 255 的对应值。

数据集已经过清理，因此没有很多非数字噪声(例如，灰白色背景)。这将使我们的工作更简单。如果您下载实际的数据集，您通常会以逗号分隔文件的形式获得它，每行对应一个条目。我们可以通过一次一个地反向赋值来将它转换成图像。实际数据集是 60000 个手绘**训练**位数，对应**标签**(实际数)，10000 个* *测试* *位数，对应* *标签* *。数据集本身通常以 python pickle(一种存储字典的简单方法)文件的形式分发(您不需要知道这一点，只是以防您在网上遇到这种情况)。

因此，我们的目标是学习如何根据我们从**训练**数据集中学习到的**模型* *正确猜测* *测试* *数据集中的数字。这被称为**监督学习* *任务，因为我们的目标是模仿另一个人(或模型)所做的。我们将简单地选取单个行，并尝试使用一种简单的称为**多层感知器* *的神经网络来猜测相应的数字。这通常是**MLP**的简称。

## 数据集处理程序

我们可以使用 Swift for Tensorflow 项目的一部分“swift-models”中的数据集加载器，以简化前面示例的处理。为了使以下代码生效，您将需要使用以下 swift package manager import 来自动将数据集添加到您的代码中。

基本:如果你是 swift 编程新手，只是想开始，只需使用在我们为 Tensorflow 设置 swift 的章节中使用的 swift-models checkout，并将以下代码(MLP 演示)放入 LeNet-MNIST 示例中的“main.swift”文件，然后运行“swift run LeNet-MNIST”。

高级:如果您已经是 swift 程序员，以下是我们将使用的基本 swift 模型导入文件:

```py
```
/// swift-tools-version:5.3
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
  name: "ConvolutionalNeuralNetworksWithSwiftForTensorFlow",
  platforms: [
    .macOS(.v10_13),
  ],
  dependencies: [
    .package(
      name: "swift-models", url: "https://github.com/tensorflow/swift-models.git", .branch("master")
    ),
  ],
  targets: [
    .target(
      name: "MNIST-1D", dependencies: [.product(name: "Datasets", package: "swift-models")],
      path: "MNIST-1D"),
  ]
)
```py

```

希望前面的代码不会太混乱。导入这个代码库会让我们的生活轻松很多。现在，让我们建立我们的第一个神经网络！

## 代码:多层感知器+ MNIST

让我们看一个非常简单的演示。将这段代码放入带有适当导入的“main.swift”文件中，我们将运行它:

```py
```

/// 1
import Datasets
import TensorFlow

// 2
struct MLP: Layer {
  var flatten = Flatten<Float>()
  var inputLayer = Dense<Float>(inputSize: 784, outputSize: 512, activation: relu)
  var hiddenLayer = Den se<Float>(inputSize: 512, outputSize: 512, activation: relu)
  var outputLayer = Dense<Float>(inputSize: 512, outputSize: 10)

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    return input.sequenced(through: flatten, inputLayer, hiddenLayer, outputLayer)
  }
}

// 3
let batchSize = 128
let epochCount = 12
var model = MLP()
let optimizer = SGD(for: model, learningRate: 0.1)
let dataset = MNIST(batchSize: batchSize)

print("Starting training...")

for (epoch, epochBatches) in dataset.training.prefix(epochCount).enumerated() {
  // 4
  Context.local.learningPhase = .training
  for batch in epochBatches {
    let (images, labels) = (batch.data, batch.label)
    let (_, gradients) = valueWithGradient(at: model) { model -> Tensor<Float> in
      let logits = model(images)
      return softmaxCrossEntropy(logits: logits, labels: labels)
    }
    optimizer.update(&model, along: gradients)
  }

  // 5
  Context.local.learningPhase = .inference
  var testLossSum: Float = 0
  var testBatchCount = 0
  var correctGuessCount = 0
  var totalGuessCount = 0
  for batch in dataset.validation {
    let (images, labels) = (batch.data, batch.label)
    let logits = model(images)
    testLossSum += softmaxCrossEntropy(logits: logits, labels: labels).scalarized()
    testBatchCount += 1

    let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels
    correctGuessCount += Int(Tensor<Int32>(correctPredictions).sum().scalarized())
    totalGuessCount = totalGuessCount + batch.data.shape[0]
  }

  let accuracy = Float(correctGuessCount) / Float(totalGuessCount)
  print(
    """
    [Epoch \(epoch + 1)] \
    Accuracy: \(correctGuessCount)/\(totalGuessCount) (\(accuracy)) \
    Loss: \(testLossSum / Float(testBatchCount))
    """
  )
}
```py

```

## 结果

当您运行前面的代码时，您应该会得到如下所示的输出:

```py
```
Loading resource: train-images-idx3-ubyte Loading resource: train-labels-idx1-ubyte Loading resource: t10k-images-idx3-ubyte Loading resource: t10k-labels-idx1-ubyte
Starting training…
[Epoch 1] Accuracy: 9364/10000 (0.9364) Loss: 0.21411717
[Epoch 2] Accuracy: 9547/10000 (0.9547) Loss: 0.15427242
[Epoch 3] Accuracy: 9630/10000 (0.963) Loss: 0.12323072
[Epoch 4] Accuracy: 9645/10000 (0.9645) Loss: 0.11413358
[Epoch 5] Accuracy: 9700/10000 (0.97) Loss: 0.094898805
[Epoch 6] Accuracy: 9747/10000 (0.9747) Loss: 0.0849531
[Epoch 7] Accuracy: 9757/10000 (0.9757) Loss: 0.076825164
[Epoch 8] Accuracy: 9735/10000 (0.9735) Loss: 0.082270846
[Epoch 9] Accuracy: 9782/10000 (0.97) Loss: 0.07173009
[Epoch 10] Accuracy: 9782/10000 (0.97) Loss: 0.06860765
[Epoch 11] Accuracy: 9779/10000 (0.9779) Loss: 0.06677916
[Epoch 12] Accuracy: 9794/10000 (0.9794) Loss: 0.063436724

```py

恭喜你，你已经完成了机器学习！这个演示只有几行，但是实际上在幕后发生了很多事情。我们来分析一下这是怎么回事。

## 演示细分(高级别)

我们将使用注释中的编号(例如，//1，//2 等)逐段查看前面的所有代码。).我们将首先进行一遍尝试并解释在高层次上正在发生的事情，然后进行第二遍，在那里我们解释本质细节。

## 进口(1)

我们的前几行非常简单；我们正在导入 swift-models MNIST 数据集处理器，然后是 TensorFlow 库。

## 模型分解(2)

接下来，我们建立实际的神经网络，一个 MLP 模型:

```
```py
/// 2
struct MLP: Layer {
  var flatten = Flatten<Float>()
  var inputLayer = Dense<Float>(inputSize: 784, outputSize: 512, activation: relu)
  var hiddenLayer = Dense<Float>(inputSize: 512, outputSize: 512, activation: relu)
  var outputLayer = Dense<Float>(inputSize: 512, outputSize: 10)

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    return input.sequenced(through: flatten, inputLayer, hiddenLayer, outputLayer)
  }
}
```

```py

这个数据结构里有什么？我们的第一行只是定义了一个名为 MLP 的新结构，它子类化了**Layer**，这是 swift 中 tensorflow 的一个类型。为了定义这个类，S4tf 实施了一个**协议**定义，我们实现了函数**forward**(以前的**callAsFunction**)，它接受**输入**并将其映射到**输出* *。我们的中线实际上定义了我们感知机的层次:

```
```py
    var flatten = Flatten<Float>()
    var inputLayer = Dense<Float>(inputSize: 784, outputSize: 512, activation: relu)
    var hiddenLayer = Dense<Float>(inputSize: 512, outputSize: 512, activation: relu)
    var outputLayer = Dense<Float>(inputSize: 512, outputSize: 10)
```

```py

我们有四个内部层:

1.  扁平化操作:这只是接受输入，并将其简化为一行输入数字(一个向量)。

    我们的数据集在内部给我们一张 28x28 像素的图片，这只是将它转换成一行 784 像素长的数字。

    接下来，我们有三个**密集的**层，这是一种特殊类型的神经网络，称为* *全连接* *层。第一个从我们的初始输入(例如，展平的 784x1 向量)到 512 个节点，如下所示。

2.  密集层:784(前面的输入)到 512 个节点。

3.  另一个密集层:512 个节点再到 512 个节点。

4.  一个输出层:512 个节点到 10 个节点(位数，0–9)。

    最后是一个转发函数，这就是我们的神经网络逻辑神奇之处。我们实际上采取的输入，运行它通过展平，密度 1，密度 2 和输出层产生我们的结果。

所以我们的

```
return input.sequenced(through: flatten, inputLayer,
hiddenLayer, outputLayer)

```py

然后是实际接收输入并通过这四层进行映射的调用。接下来我们将查看实际的培训循环，以了解所有这些是如何实际发生的，但 swift 对于 tensorflow 的魔力很大一部分在于这几行。我们稍后会更详细地讨论这里发生了什么，但从概念上讲，这个功能只不过是按顺序应用前面的四层。

## 全局变量(3)

这些行只是设置一些我们将要使用的不同工具:

```
```py
let batchSize = 128
let epochCount = 12
var model = MLP()
let optimizer = SGD(for: model, learningRate: 0.1)
let dataset = MNIST(batchSize: batchSize)
```

```py

前两行设置了几个全局变量:我们的 batchSize(我们每次要查看多少个 MNIST 示例)和 epochCount(我们要对数据集进行的遍历次数)。

下一行初始化我们的模型，这是我们之前讨论过的。

第四行初始化我们的优化器，稍后我们会详细讨论。

最后一行设置了我们的数据集处理程序。

下一行通过循环我们的数据开始了我们的实际训练过程:

```
```py
for (epoch, epochBatches) in dataset.training.prefix(epochCount).enumerated()  {
```

```py

现在我们可以进入实际的训练循环了！

## 训练循环:更新(4)

这是我们训练循环的实际核心。从概念上讲，我们将拍摄一组图片或**批**并将每张图片显示给第一组输入密集节点，这将**激发* *并转到下一组隐藏的密集节点，这将* *激发* *并转到最终的密集节点输出集。然后，我们将获取网络最后一层的所有输出，选择最大的一个，并查看它。如果这个节点和我们给它的原始输入是同一个数，那么我们会给网络一个**奖励* *，告诉它增加对结果的信心。如果这个答案是错误的，那么我们会给网络一个**负奖励* *并告诉它降低对结果的信心。通过使用数千个样本重复这一过程，我们的网络可以学习准确预测它从未见过的输入。

```
```py
  Context.local.learningPhase = .training
  for batch in epochBatches {
    let (images, labels) = (batch.data, batch.label)
    let (_, gradients) = valueWithGradient(at: model) { model -> Tensor<Float> in
      let logits = model(images)
      return softmaxCrossEntropy(logits: logits, labels: labels)
    }
    optimizer.update(&model, along: gradients)
  }

```

这在引擎盖下是如何工作的？一点点微积分和我们所有的数据混合在一起。对于每个训练示例，我们获取原始像素值(图像数据)，然后获取相应的标签(图片的实际数量)。然后，我们通过计算模型对 X 的预测值来确定**模型**的* *梯度* *，然后使用一个名为 softmaxCrossEntropy 的函数来查看我们的预测值与实际值 y 的比较情况。从概念上讲，softmax 只是获取一组输入，然后将它们的结果归一化为一个百分比。这在数学上可能有点复杂，因此转换数字以使用自然对数 e，然后除以指数之和，具有在任意输入中保持一致和易于在计算机上评估的有用的双重属性。然后，我们更新**模型* *的方向，使其与应该出现的方向略有不同(如果正确，则更倾向于正确的方向，如果不正确，则远离)。我们的学习率决定了我们每次应该走多远(例如，因为我们的学习率是 0.1，所以我们每次只走网络认为正确的方向的 10%)。在调用所有这些的 for 循环中，我们将对我们的所有数据重复这个过程(一遍)多轮，或**个时期* *。

## 训练循环:准确性(5)

接下来，我们在我们的测试数据上运行我们的模型，并计算它在它尚未看到的图像上正确的频率(但我们知道正确的答案)。那么，精确度是什么意思，我们如何计算它？我们的代码如下所示:

```py
```
  Context.local.learningPhase = .inference
  var testLossSum: Float = 0
  var testBatchCount = 0
  var correctGuessCount = 0
  var totalGuessCount = 0
  for batch in dataset.validation {
    let (images, labels) = (batch.data, batch.label)
    let logits = model(images)
    testLossSum += softmaxCrossEntropy(logits: logits, labels: labels).scalarized()
    testBatchCount += 1

    let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels
    correctGuessCount += Int(Tensor<Int32>(correctPredictions).sum().scalarized())
    totalGuessCount = totalGuessCount + batch.data.shape[0]
  }

  let accuracy = Float(correctGuessCount) / Float(totalGuessCount)
  print(
    """
    [Epoch \(epoch + 1)] \
    Accuracy: \(correctGuessCount)/\(totalGuessCount) (\(accuracy)) \
    Loss: \(testLossSum / Float(testBatchCount))
    """
  )
```py

```

在与我们的训练数据集类似的过程中，我们简单地获取测试输入图像，通过我们的模型运行它们，然后将我们的结果与我们知道的正确答案进行比较。然后，我们计算正确答案的数量除以图像的总数，得出我们的准确率。我们最后的几行只是打印出每次通过数据集的各种数字，或**纪元* *，因此我们可以看到我们的损失是否在减少(例如，网络每次通过都变得更加准确)。

## 演示细分(低级)

好了，我们已经在高层次上完成了 MNIST 的例子。现在让我们来看一下我们正在调用的一些函数，并更深入地探索我们简单的训练循环。

## 完全连接的神经网络层

完全连接的层构成了我们网络的主干，因此值得花些时间来了解它们。在较高层次上，输入数据集中的每组结点都映射到输出数据集中。然后，网络的每条边都有一个由我们的训练函数更新的权重。然后每个节点的数学就是字面上的[权重]*[输入]+[偏差]，输出节点的值就是这个数学函数的结果。**Weight**是我们将在这个节点的输入上放置多少值，然后**bias**是一个常量值，无论发生什么情况都分配给这个节点。这两者的价值将在我们的培训中学习。我们用矩阵数学来表示我们的变量，这就是为什么每个值都在括号里。

对于单个节点来说，前面的数学很容易理解，但神经网络的真正魔力来自许多这些节点的共同作用。大致来说，每个神经元学习输入的一个部分或**特征，然后通过与其他神经元合作，它们共同学习产生我们想要的结果所需的一组权重。所有这些工作的第二个要素是我们将多个层结合在一起。这些节点不是独立地学习它们的值，而是从其他也在更新的节点学习。这意味着，通过结合一起工作来确定何时触发的想法，神经元一起工作来找到最有效的方式来表示输入数据。

请注意，我们在这里非常宽松地使用了 learn 这个词。前面的数学都是正确的，但是人们经常认为这个过程比实际存在的要聪明得多。我认为思考这个问题的最佳方式是简单地将您的输入数据视为半相关样本的集合(例如，分布)，然后神经网络是将该分布缩减为极小表示的一种方式。我们将继续探索理解这一关键概念的不同方式。

ReLU 是一个足够简单的函数，可以用数学来解释:relu(x) = max(0，x)。这意味着我们返回原始值，然后对于所有小于零的值，我们只返回一个零。这里还有其他的选择(我们将在下一章讨论)，特别是 sigmoid 函数，但是由于 ReLU 产生了很好的结果并且很容易评估(并且通过扩展很快)，它已经成为了你在实践中会发现的事实上的标准激活函数。

## 优化器如何工作

为了继续前面的想法，我们的目标是试图找到一组神经元，它们将一起激发来表示我们的数据。因此，在高层次上，我们将向我们的网络展示我们的数据，然后计算我们的模型与我们的理论结果有多远，然后尝试在下一次将我们的网络稍微移近更正确。这个过程就是我们的优化器所做的。如果我们的网络猜测正确，并且正朝着正确的方向前进，那么我们告诉它继续前进。如果我们的网络猜错了，并且正朝着错误的方向前进，那么我们告诉它继续朝着相反的方向前进。

表示这一点的最简单的方法是考虑试图找到一条曲线的最小值，比如 y = x^2.我们可以随便在曲线上取任意一点，在附近的另一点(可以说一步之遥)计算结果。那么两种可能性中的任何一种都会发生:要么我们离基地越来越远(例如，向错误的方向移动)，要么我们离基地越来越近。然后，对于我们的下一步，我们可以继续朝着同一个方向前进，或者改变我们的路线。不管怎样，我们最终会接近曲线的底部。

继续前面的想法，我们的方法有几个问题。第一种是当我们的步长过大时。离底部越远，这将收敛得越快，但当我们接近底部时，我们最终将处于从一边跳到另一边的状态。另一方面，选择的步长太小，需要很长时间才能达到最小值，但这通常不是太大的问题(如果达到了，就达到了)。下一个技巧是添加所谓的动量(或二阶梯度)。基本思想是，我们不完全改变每一步的速度，而是保持先前的运动(例如，我们每一步只增加 10%的方向变化)。

## 优化器+神经网络

前面的想法就是所谓的凸优化。然而，在处理神经网络时，事情就有点棘手了。首先，根据定义，我们正在为每个神经元更新一个优化函数，因此这个问题爆发为在超维空间中处理许多不同的函数。对计算机来说，这只不过是一个非常大的数学问题，但对人类来说，不再有一个好的方法来可视化正在发生的事情。这是一个很大的数学开放领域，叫做非凸优化。

第二个问题更简单:对于我们的数学问题，我们很容易计算出我们是否在朝着正确的方向前进，因为我们知道正确的答案是什么。神经网络中的一个非常大的问题(特别是对于更高级的领域)是为我们的问题找到正确的目标函数。在本书中，我们将主要使用 softmax 交叉熵损失。对于图像识别的问题，这很容易通过将我们的答案与已知结果进行比较来表示(例如，我们只是对事情进行正确与否的分级)。但是，在神经网络的更高级应用中，构建自定义损失函数是一个有趣的问题，你应该知道。

## Swift for Tensorflow

前面的文本涵盖了神经网络部分。现在，让我们看看 swift for tensorflow 的用武之地。从数学的角度来看，提到的方法有望简单到足以理解。将它应用于我们的神经网络问题，以扩展到更大问题的方式，问题更加复杂。最大的问题是，对于现实世界的网络来说，在内存中跟踪我们所有的梯度使得更新它们变得更加简单和快速。第二个是当手工构建这些模型时，很容易引入一些微妙的错误，这些错误会在以后造成问题。用于 tensorflow 的 Swift 使用 Swift 的类型系统来要求层协议，正如我们前面看到的。基本的想法很简单，我们要确保每个模型都执行这个协议。然后，我们可以向模型中添加新的部分，只要他们比理论上扩展这个协议，所述部分的任何任意组合都可以工作。实施这一层协议迫使我们，程序员，保持我们的函数链正确，并且通过扩展允许编译器以任何它想要的方式来模拟我们的梯度。那么，通过扩展，编译器可以为我们手头的任何硬件设备输出代码。这就是我们将 swift 用于 tensorflow 的原因:获得我们网络的编译时检查，以及使用平台特定优化在许多不同硬件后端上运行我们模型的能力。

## 支线任务

为了理解发生了什么，您可以对代码进行一些简单的调整:

*   尝试使密集层变小或变大(例如，将 inputLayer、hiddenLayer 和 outputLayer 中的 512 更改为 128 或 1024)，然后再次运行以查看这如何影响结果。

*   尝试将历元数增加到 30，并将学习速率降低到 0.001，看看较小的步长如何仍能收敛到相同的结果。

## 概述

我们已经了解了如何与一个名为 MNIST 的简单数据集进行交互，该数据集由从 0 到 9 的灰度手绘数字组成，共有 10 个类别。我们已经构建了一个简单的一维神经网络(称为**多层感知器* *)，使用 swift for tensorflow 对 MNIST 数字进行分类。我们已经了解了如何使用一种称为**随机梯度下降* *的统计技术，在每次看到新图像时更新我们的神经网络，以产生越来越好的结果。我们建立了一个基本但功能性的训练循环，多次遍历数据集，或**个时期* *，从最初的随机状态(本质上是猜测)训练我们的神经网络，最终能够识别 90%以上的数字。

从概念上讲，这是本书最难的一章。从字面上看，我们未来要做的一切只是采取同样的基本方法，并不断改进。在前进之前，花些时间把提到的所有事情都记下来。接下来，我们将向我们构建的神经网络添加一些卷积，以产生我们的第一个卷积神经网络。