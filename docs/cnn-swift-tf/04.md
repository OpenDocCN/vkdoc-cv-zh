# 4.VGG 网络

在本章中，我们将通过制作一个更大版本的 CIFAR 网络，从 2014 年开始建设 VGG，这是一个最先进的网络。

## 背景:ImageNet

MNIST 和 CIFAR 是受欢迎的，在学术研究中经常被引用的数据集，作为新想法的试验台，但在过去几年中，人们越来越多地达到了在它们之上建立网络的实际限制。我们的下一个数据集是 ImageNet，这是一个流行的真实世界数据集，用于构建和训练图像识别和对象检测网络。ImageNet 有一千个类别，所以我们在本书的其余部分将使用的网络将能够支持更大的分类问题。数据集本身是从互联网上搜集的大约 130 万张图片。在数据方面，训练数据集约为 147GB，另外还有 7GB 的测试和验证文件。如果你去 ImageNet 网站(如 [`http://www.image-net.org`](http://www.image-net.org) )你可以浏览一些类别，它们的名字像“n01440764”如果您将这些数字与 synnet 文件进行比较，您可以找出每个类别对应的内容。

### 获取图像

这曾经是一件稍微复杂的事情，但是最近 swift-models 存储库为 ImageNet 数据集添加了一个很好的数据加载器，您可以在您的系统上使用它。但是，请注意，您将需要几百千兆字节的空闲磁盘空间来处理文件(提取、转换等)。).话虽如此，对于我们的目的来说，ImageNet 有点大，因此我们将使用一个子集，以免达到我们的计算机和 swift for tensorflow 的极限。

### Imagenette 数据集

Imagenette 是 fast.ai 的杰瑞米·霍华德 ImageNet 的子集，旨在使测试计算机视觉网络更容易。具体是以下十大类:tench、英式 springer、卡带播放器、链锯、教堂、法国号、垃圾车、气泵、高尔夫球、降落伞。

还有第二个更难的版本 Imagenette，另一个子集的十个类别，称为 Imagewoof，这是具体的以下十个犬种类别:澳大利亚梗，边境梗，萨摩耶，小猎犬，西施犬，英国猎狐犬，罗得西亚脊背犬，野狗，金毛猎犬，老英国牧羊犬。

我们可以从 swift-models 存储库中加载这两个数据集，并在您的培训脚本中交换它们。使用 swift-models loader 的好处在于，它可以自动下载、提取实际 ImageNet 图像(具有半随机大小)并将其批量调整为可预测的输入大小(例如，224 x 224 像素)。

### 日期增加

一般来说，图像识别/深度神经网络中一个非常重要的主题是**数据增强* *，我们在本书中基本上会跳过它，因为我想避免让这个领域的新手感到事情复杂。但是，在继续之前，让我们在这里简单地讨论一下。

我们可以想象增加我们的神经网络的规模，直到我们有一个“完美”的神经网络，对于我们展示的每一幅图像，它都给我们正确的结果。关键的概念是，我们使用的优化函数试图最小化我们给它的数据集的损失。所以，我们对这个“完美”网络的优化函数已经到了零(它从不出错)，正如我们所希望的那样。听起来很棒，让我们写一篇论文并收集我们的奖品吧！

但是等等！在我们这样做之前，我们可能会尝试，比如说，水平翻转我们的猫图片，然后将这个新图片交给我们的神经网络。会发生什么？基本上，我们正在向我们的神经网络展示一幅前所未见的画面，所以结果充其量是半随机的。结果可能是，我们翻转的猫的“最接近”输入图像(在神经网络的搜索空间中)是一张狗的图片，因此当我们的网络看到这张新图片时，它会说“狗”。

然后，数据扩充(以及一般的训练深度神经网络)的基本思想是确保我们不会**过度拟合**(例如，过于接近我们的训练数据集)以至于我们无法* *概括* *(例如，对我们从未见过的数据正确地做出新的预测)。有几个基本方法:

1.  收集更多数据！你不会在学术竞赛/目的中看到这种情况，但许多现实世界的机器学习涉及到获取或构建更大的数据集，以确保我们的网络不是真的在猜测新的条件，而是已经“看到”了一个相当类似的例子。同样，另一个常见的问题是只给我们的网络提供灰色猫的图片，然后当它看到橙色猫时，它不知道该怎么办。如果都是同一只猫，那么有很多例子对我们没有太大帮助！这个问题的另一个常见版本是获得与我们想要最终分类的不同的训练示例，例如，从互联网上训练照片，然后尝试将它们应用到现实世界的相机输入。只要有可能，就使用最终要测试的相同数据。同样，只要你能，收集更多的数据！

2.  数据扩充:我们可以使用计算机对我们的数据进行各种修改，以增加我们总体的样本数量。一些常见的例子:
    *   我们可以将图像从左向右翻转。

    *   改变我们的亮度(伽玛)。

    *   旋转我们的图像。

    *   随机裁剪(切掉图片的边缘)。

    *   随机缩放(使我们的图片变大，然后切掉现在更大的边缘)。

通常，这些方法也会结合起来，以确保神经网络也能获得尽可能多的训练数据变量。重要的一点是，这些方法经常变得特定于领域。或者说，我们可以翻转猫/狗的图片，但不能翻转字母表中的字母！

1.  给我们的网络增加噪音:另一个极其重要的方面是给我们的运营增加噪音，以确保我们的网络不会过于依赖特定的输入/图像。这是一项非常有价值的技术，通过使我们的网络对噪声具有鲁棒性来提高现实世界的性能。有一个重要的相关研究领域叫做对抗性输入，它试图通过引入细微的噪声来欺骗分类器，从而欺骗网络。

这里有一些关于这个主题的有趣论文，你可以看看:

>退出:防止神经网络过度拟合的简单方法

```
> www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf

```

这是一篇重要的论文，你应该知道。在训练我们的模型时，通过随机修剪(删除)密集节点，得到的网络概括得更好。另一个有趣的效果是，这也加快了网络速度。

>混淆:超越经验风险最小化

```
> https://arxiv.org/abs/1710.09412

```

大致来说，我们正在训练我们的网络识别图像，并在它们得到正确答案时给予奖励。该论文随机组合两个输入图像(例如，50%狗和 50%猫->新图片)，并奖励网络猜测相应的答案(例如，50%狗和 50%猫标签)。这个简单的调整显著提高了网络的泛化能力。

>改进了带剪切块的卷积神经网络的正则化

```
> https://arxiv.org/abs/1708.04552

```

这个想法类似于 mixup，但是我们是通过剪切和粘贴图像来创建我们的目标图像的，并且有类似的改进效果。一般来说，有时在更高层次上解决这个问题是很重要的，确保我们不要试图让我们的网络太深，以至于最终总是试图追求“完美”的解决方案，而是学习足够的知识，以便能够在非测试环境中做得很好。这是一个微妙的领域，人们经常陷入追逐“完美”的参数集，但他们的网络在处理新数据时表现不佳。这是一个我们可以花很多时间的领域。我们将在本书的后面重新讨论这个问题。

## 利用光彩造型修护发膏

现在，让我们进入第一个真正用于图像识别的最先进的卷积神经网络。VGG 代表视觉几何小组，这是英国牛津大学的一个计算机视觉/数学相关研究人员小组。

>“用于大规模图像识别的非常深的卷积网络”

```
> https://arxiv.org/abs/1409.1556

```

他们制作了一组网络(以他们的小组命名)，在 2014 年的 ILSVRC 竞赛中排名第二，仅次于 GoogLeNet。

然而，不要让这吓到你，因为他们的方法在技术上并不比我们目前所看到的 MNIST 和 CIFAR 网络更复杂。他们通过堆叠我们在过去几章中看到的相同的卷积组来建立他们的大型神经网络。他们的网络与我们之前构建的网络完全相同:两组 3x3 卷积、一个最大池、另外两组 3x3 卷积和一个最大池。接下来，他们继续堆叠层，并添加三组 3x3 卷积、一个最大池、三组 3x3 卷积、一个最大池、三组 3x3 卷积和一个最大池。最后，对于输出层，他们使用两个由 4096 个完全连接的节点组成的大型层(可以说，使他们的网络能够了解更多信息)，最后有一个由一千个节点组成的输出层来映射到每个 ImageNet 类别。

这个网络之所以叫 VGG16，是因为它有(输入)[2 + 2 + 3 + 3 + 3] + 2(全连接神经网络)+ 1(输出)层。出于我们的目的，我们将只在最后使用 10 个输出节点(例如，为什么我们的 classCount init 参数是 10)，以处理我们较小的 Imagenette 数据集，但其他方面都是相同的。

## 密码

首先，让我们看看我们的训练循环，它应该看起来非常熟悉我们的 CIFAR 和 MNIST 训练循环。唯一真正的区别是，现在我们正在处理一个更大的数据集。我们的下一个网络对训练有点挑剔，所以我们使用具有较小学习速率(更新步长值)的 SGD 来确保它正确训练，不会“跳跃”太多。

```
```
import Datasets
import TensorFlow

struct VGG16: Layer {
  var conv1a = Conv2D<Float>(filterShape: (3, 3, 3, 64), padding: .same, activation: relu)
  var conv1b = Conv2D<Float>(filterShape: (3, 3, 64, 64), padding: .same, activation: relu)
  var pool1 = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))

  var conv2a = Conv2D<Float>(filterShape: (3, 3, 64, 128), padding: .same, activation: relu)
  var conv2b = Conv2D<Float>(filterShape: (3, 3, 128, 128), padding: .same, activation: relu)
  var pool2 = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))

  var conv3a = Conv2D<Float>(filterShape: (3, 3, 128, 256), padding: .same, activation: relu)
  var conv3b = Conv2D<Float>(filterShape: (3, 3, 256, 256), padding: .same, activation: relu)
  var conv3c = Conv2D<Float>(filterShape: (3, 3, 256, 256), padding: .same, activation: relu)
  var pool3 = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))

  var conv4a = Conv2D<Float>(filterShape: (3, 3, 256, 512), padding: .same, activation: relu)
  var conv4b = Conv2D<Float>(filterShape: (3, 3, 512, 512), padding: .same, activation: relu)
  var conv4c = Conv2D<Float>(filterShape: (3, 3, 512, 512), padding: .same, activation: relu)
  var pool4 = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))

  var conv5a = Conv2D<Float>(filterShape: (3, 3, 512, 512), padding: .same, activation: relu)
  var conv5b = Conv2D<Float>(filterShape: (3, 3, 512, 512), padding: .same, activation: relu)
  var conv5c = Conv2D<Float>(filterShape: (3, 3, 512, 512), padding: .same, activation: relu)
  var pool5 = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))

  var flatten = Flatten<Float>()
  var inputLayer = Dense<Float>(inputSize: 512 * 7 * 7, outputSize: 4096, activation: relu)
  var hiddenLayer = Dense<Float>(inputSize: 4096, outputSize: 4096, activation: relu)
  var outputLayer = Dense<Float>(inputSize: 4096, outputSize: 10)

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    let conv1 = input.sequenced(through: conv1a, conv1b, pool1)
    let conv2 = conv1.sequenced(through: conv2a, conv2b, pool2)
    let conv3 = conv2.sequenced(through: conv3a, conv3b, conv3c, pool3)
    let conv4 = conv3.sequenced(through: conv4a, conv4b, conv4c, pool4)
    let conv5 = conv4.sequenced(through: conv5a, conv5b, conv5c, pool5)
    return conv5.sequenced(through: flatten, inputLayer, hiddenLayer, outputLayer)
  }
}

let batchSize = 32
let epochCount = 10

let dataset = Imagenette(batchSize: batchSize, inputSize: .resized320, outputSize: 224)
var model = VGG16()
let optimizer = SGD(for: model, learningRate: 0.002, momentum: 0.9)

print("Starting training...")

for (epoch, epochBatches) in dataset.training.prefix(epochCount).enumerated() {
  Context.local.learningPhase = .training
  for batch in epochBatches {
    let (images, labels) = (batch.data, batch.label)
    let (_, gradients) = valueWithGradient(at: model) { model -> Tensor<Float> in
      let logits = model(images)
      return softmaxCrossEntropy(logits: logits, labels: labels)
    }
    optimizer.update(&model, along: gradients)
  }

  Context.local.learningPhase = .inference
  var testLossSum: Float = 0
  var testBatchCount = 0
  var correctGuessCount = 0
  var totalGuessCount = 0
  for batch in dataset.validation {
    let (images, labels) = (batch.data, batch.label)
    let logits = model(images)
    testLossSum += softmaxCrossEntropy(logits: logits, labels: labels).scalarized()
    testBatchCount += 1

    let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels
    correctGuessCount += Int(Tensor<Int32>(correctPredictions).sum().scalarized())
    totalGuessCount = totalGuessCount + batch.label.shape[0]
  }

  let accuracy = Float(correctGuessCount) / Float(totalGuessCount)
  print(
    """
    [Epoch \(epoch+1)] \
    Accuracy: \(correctGuessCount)/\(totalGuessCount) (\(accuracy)) \
    Loss: \(testLossSum / Float(testBatchCount))
    """
  )
}
```

```

### 结果

在 Imagenette 数据集上运行此网络应该会产生如下结果:

```
```
[Epoch 1 ] Accuracy: 125/500 (0.25)  Loss: 2.290163
[Epoch 2 ] Accuracy: 170/500 (0.34)  Loss: 1.8886051
[Epoch 3 ] Accuracy: 205/500 (0.41)  Loss: 1.6971107
[Epoch 4 ] Accuracy: 243/500 (0.486) Loss: 1.5611153
[Epoch 5 ] Accuracy: 257/500 (0.514) Loss: 1.43015
[Epoch 6 ] Accuracy: 290/500 (0.58)  Loss: 1.2774785
[Epoch 7 ] Accuracy: 67/500  (0.534) Loss: 1.3170111
[Epoch 8 ] Accuracy: 309/500 (0.618) Loss: 1.1680012
[Epoch 9 ] Accuracy: 299/500 (0.598) Loss: 1.403522
[Epoch 10] Accuracy: 303/500 (0.606) Loss: 1.40440996
```

```

### 内存使用

使用 VGG16，您可能会达到系统的内存限制。请记住，您可能需要将批处理大小更改为 16(甚至更少)，以便将数据集干净地放入 GPU 的内存中。一个很好的做法是启动一个作业，然后使用 tmux 打开一个新的 shell 会话，并运行“nvidia-smi -l 5 ”,观察设备在作业开始时如何填充内存。

在我们深入探讨之前，让我们来看看您在某个时间点通常会遇到的另一个重要问题，这肯定是 VGG 的问题，即针对 tensorflow 的 swift 内存不足。将您的批处理大小设置为 128，运行您的代码，并等待一会儿:

```
```
Fatal error: OOM when allocating tensor with shape[128,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc: file /
home/skoonce/swift/swift-source/tensorflow-swift-apis/Sources/ TensorFlow/Bindings/EagerExecution.swift, line 300 Current stack trace:
0 libswiftCore.so 0x00007fcb746f6c40
swift_reportError + 5
0
1 libswiftCore.so 0x00007fcb74767590
_swift_stdlib_reportF atalErrorInFile + 115
2 libswiftCore.so 0x00007fcb7445c53e
<unavailable> + 14554

22
3 libswiftCore.so 0x00007fcb7445c147
<unavailable> + 14544

33 libswiftCore.so 0x00007fcb745fc310 valueWithPullback<A, B>(at:in:) + 106
34 libswiftTensorFlow.so 0x00007fcb74bb9e20 valueWithGradient<A, B>(at:in:) + 1073
35 VGG-Imagewoof 0x000055a5370311ed
<unavailable> + 46453
57
36 libc.so.6 0x00007fcb5d6d6ab0
libc_start_main + 231
37 VGG-Imagewoof 0x000055a536bf90ba
<unavailable> + 2213$0
Illegal instruction (core dumped)
```

```

我们在这里使用的 Imagenette 数据集使用了大约 16GB 的主内存。如果你有一个 8GB 内存的 GPU，你可能需要在接下来的几章中减少你的批处理大小，以避免可怕的 OOM(见前面的文本)。将其切成两半会使您更容易根据需要处理更大的数据集，但是对于一些较大的网络，您可能需要使用更小的批处理大小。

我鼓励您尝试不同的批量大小，并对每个批量运行 nvidia-smi，以了解这些概念之间的关系。在我看来，这是一项需要掌握的重要技能，因为它将使您能够针对具有更多/更少内存的设备来扩展和缩减您的工作负载。特别是 tensorflow 的 Swift 目前有点“globby ”,因为它似乎以数千兆字节的增量抓取东西，所以使用 s4tf 学习这一点不会像使用其他机器学习框架那样容易，但知道如何为您的设备调整工作负载是一项宝贵的技能，您在该领域(以及其他软件包)还需要一段时间。

## 模型重构

在某个时候，我们将会触及我们通过简单地复制和粘贴更多层来产生越来越大的神经网络所能完成的极限。现在是一个很好的时机来看看我们如何通过使用稍微复杂一点的编程方法来扩展我们的方法。首先，让我们做一些重构，并了解如何将多个层结合在一起，以减少重复代码的数量。

### 带子块的 VGG16

这是怎么回事？基本上，我们正在构建一些更小的块，这样我们就可以减少主网络中重复代码的数量。由于我们所有的 VGG 网络块看起来都一样(N ^ 3x 3 层+一个最大池)，我们可以通过编程来定义它们。

```
```
struct VGGBlock2: Layer {
  var conv1a: Conv2D<Float>
  var conv1b: Conv2D<Float>
  var pool1 = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))

  init(featureCounts: (Int, Int)) {
    conv1a = Conv2D(filterShape: (3, 3, featureCounts.0, featureCounts.1), padding: .same, activation: relu)
    conv1b = Conv2D(filterShape: (3, 3, featureCounts.1, featureCounts.1), padding: .same, activation: relu)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    return input.sequenced(through: conv1a, conv1b, pool1)
  }
}

struct VGGBlock3: Layer {
  var conva: Conv2D<Float>
  var convb: Conv2D<Float>
  var convc: Conv2D<Float>
  var pool = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))

  init(featureCounts: (Int, Int)) {
    conva = Conv2D(filterShape: (3, 3, featureCounts.0, featureCounts.1), padding: .same, activation: relu)
    convb = Conv2D(filterShape: (3, 3, featureCounts.1, featureCounts.1), padding: .same, activation: relu)
    convc = Conv2D(filterShape: (3, 3, featureCounts.1, featureCounts.1), padding: .same, activation: relu)
  }

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    return input.sequenced(through: conva, convb, convc, pool)
  }
}

struct VGG16: Layer {
  var layer1 = VGGBlock2(featureCounts: (3, 64))
  var layer2 = VGGBlock2(featureCounts: (64, 128))
  var layer3 = VGGBlock3(featureCounts: (128, 256))
  var layer4 = VGGBlock3(featureCounts: (256, 512))
  var layer5 = VGGBlock3(featureCounts: (512, 512))

  var flatten = Flatten<Float>()
  var inputLayer = Dense<Float>(inputSize: 512 * 7 * 7, outputSize: 4096, activation: relu)
  var hiddenLayer = Dense<Float>(inputSize: 4096, outputSize: 4096, activation: relu)
  var outputLayer = Dense<Float>(inputSize: 4096, outputSize: 10)

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    let backbone = input.sequenced(through: layer1, layer2, layer3, layer4, layer5)
    return backbone.sequenced(through: flatten, inputLayer, hiddenLayer, outputLayer)
  }
}
```

```

### 支线任务

从现代标准来看，AlexNet 的结构有些非正统，所以我在本书中有意跳过了它，但由于历史原因，它是一篇值得一读的重要论文。

>使用深度卷积神经网络的 ImageNet 分类

```
> https://papers.nips.cc/paper/4824-imagenet-classification- with-deep-convolutional-neural-networks.pdf

```

inception v1(Google net 现在更为人所知的名称)比 VGG 有更好的表现，但它是一个更复杂的模型。这篇论文在历史上很重要，但我建议你先掌握剩余网络。

>通过卷积更深入

```
> https://arxiv.org/abs/1409.4842

```

## 概述

今天，VGG 不像我们马上要看的其他网络那样受欢迎，但它肯定仍在使用，尽管已经有五年的历史了。这种网络仍然可以在图像处理环境中看到，例如风格转换和作为对象检测网络的基础。你还会经常在特定领域的图像识别问题中看到经过重新训练的 VGG 网络，比如人脸识别。祝贺你成功来到这里。你已经成功地复制了你的第一篇学术论文！接下来，让我们看看如何稍微修改我们的网络，以产生更好的结果。