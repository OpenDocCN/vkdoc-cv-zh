# 13.重访 MNIST

二十世纪有许多有趣的发明，但我认为电脑是最重要的一项。每年都有越来越多的计算周期被推向市场，每年都有对计算的兴趣和需求增加。我们可能已经达到了登纳德标度的极限，但还有几十年有趣的改进要做。

## 后续步骤

以下是我对不久的将来的看法:

*   更多内核

*   更多内存

*   更多带宽

*   更多定制硬件

*   更通用的硬件

核心通常很简单。我们已经达到了硅发展速度的极限，但我们可以继续在设备中制造额外的晶体管。那么最简单的方法就是简单地增加芯片上单个处理器的数量。AMD 最近针对处理器的锐龙小芯片方法表明，这可以持续很长时间。

RAM:如果你愿意的话，现在你可以为云服务器提供万亿字节的内存。在这方面还有很长的路要走。这方面的真正障碍不是内存大小，而是我们的下一个趋势。

带宽:PCI 4 已经上市，人们已经在研究 PCI 5 和 PCI 6。大多数现代系统的真正限制不再是内核，而是它们之间的协调和同步。我们已经达到了原始时钟速度的极限，所以现在关键的技巧是保持内核得到指令和数据。如果 Threadripper 上的每个内核实际上一个周期处理一点数据，那么我们的处理速度会突然超过我们的内存。

定制硬件:苹果的 ARM 处理器、英伟达的 GPU 和谷歌的 TPUv1，以及最近使用 TSMC 晶圆厂的 Cerberas 等新公司正在推动该行业的许多事情。他们正在将巨大的规模经济推向市场，并使人们有可能以低廉的价格租用晶圆厂空间，这反过来又使人们有可能以比以往任何时候都低得多的价格建造定制硅。你可以在软件中制作芯片原型，将设计发送出去，不久之后就可以通过邮件获得结果。这使得全新一代的硬件能够进入市场，我认为我们现在只是看到了可能性的开端。

通用硬件:对我来说，这是能够自己制造芯片的超级有趣的另一面。由于专利问题和交叉许可知识产权的需要，这些年来计算领域的许多进展都停滞不前。有一些开源芯片设计(RISC-V 就是一个很好的例子),您可以使用它们来免费构建一个现代的 64 位处理器。像 LLVM 这样的工具意味着，如果你能为你的架构建立一个导出模块，那么突然间你就能把整个软件生态系统带到你的新设备上。

## 棘手问题

希望所有给定的想法都不会引起你的争议。现在，我相信如果我们看看这些想法，我们可以看到一些清晰的趋势正在形成。

多核编程并不是一个新概念，但实际使用它才是。二十多年来，它已经在台式电脑上随处可见。话虽如此，实际上很少有软件真正使用了 CPU 上所有可用的能力，大多数程序员仍然停留在单线程编程模型中。大多数现代并行化是通过一次运行大量作业(例如，在一台服务器上托管十几个虚拟机，或者在一个队列中运行 10，000 个作业)，而不是通过实际将单个作业适当拆分。

RAM 尤其是深度学习的一个重要限制因素，但这实际上是因为下一个问题，带宽。GPU 用于深度学习的真正力量不是 GPU 本身，而是内部内存/通信总线。速度越来越高的 RAM 是目前生态系统中最昂贵的组件之一，但每一次迭代都允许更多的数据通过 GPU 处理器运行，因此这一块将继续发展。我认为这项技术最终会回到 CPU 的领域，让它们做出更大的贡献。

带宽:GPU - > RAM 内存可以合理地很好地解决上述问题，但是每当我们想要尝试协调多个 GPU 的工作时，我们就又回到了触及 PCI 总线带宽限制的起点。Nvidia 很好地意识到了这一弱点，并竭尽全力为他们的 DGX 系列计算机实现了自定义的 GPU 内部网络堆栈(NCCL)。Habana Labs 的 Gaudi 通过在每个 ASIC 上粘贴一个 100 千兆以太网交换机，简单地取代了所有这些定制的硅和复杂性，以保证每个节点之间的 1tb 通信带宽。Nvidia 最近对交换机硬件制造商 Mellanox 的收购，对我来说也指向了这一未来。EGX A100 将 200Gbps Infiniband 放在每个 GPU 上，因此 PCI 总线不再是一个限制因素，多个卡可以拥有自己的专用背板来相互通信。然后，可以随意实施各种网络拓扑，而不必依赖于定制的通信协议，这意味着这种方法将很容易随着 200 和 400GbE 的上线而扩展。未来使用 800GbE 和 1.6TbE 将这一数字再翻一番应该也是可行的。

自定义运算:除了基本的 MAC 运算(这是当前大多数人工智能硬件的目标)，仍然不确定什么样的数学运算在实践中最有用。一方面，你可以说 INT1、INT4、INT8 和 FP16 数学形式的技术方法是使现有操作更小并增加单次处理的数据量的自然延伸。另一方面，你可以在谷歌的 TPU 和英特尔即将推出的加速器中使用 BFloat16 这种务实的方法，通过降低处理缓冲区溢出的复杂性，简化了将 FP32 工作流移植到新设备的过程。Nvidia 的 Ampere 路线图显示，他们通过添加更大版本的 BFloat 方法(例如，支持 INT1、INT4、INT8、FP16、BFloat16、TFloat32、FP32、TFloat64、FP64)来支持基本上所有可能的操作，并将实际实现事情的责任放在编码器上。该平台令人兴奋的地方在于，通过对终端用户可用的操作进行标准化，不再有任何借口不使用定制的 precision 硬件。

通用硬件:对我来说，最有趣的静悄悄的革命是 ARM 芯片组，以及亚马逊最近将该平台用于下一代服务器硬件。通过从回路中去除专有硅，可以实现更高的规模效率。这将需要几年的时间来完全发挥出来，但这是我们将在不久的将来。ARM 和 RISC-V 将跟随前沿平台，悄悄地吸收它们带给市场的任何新创新。与此同时，专利硅技术将不得不与成本更低的商品化创新进行斗争。

## TPU 案例研究

所有这些技术都很酷，但从根本上说，为了编写优化的软件，程序员必须提前计划他们的数据和内存访问。就像我之前说过的，我们已经达到了单一数据风格编程的极限，并且越来越需要学习如何拥抱特定于数据流的方法。让我们看看谷歌的 TPU，作为在实践中解决上述问题的一个例子:

1.  内核:TPU 使用相当简单的 ASIC 逻辑，并将多个内核放在一个处理包中。然后，他们将许多这样的处理器连接在一起，形成一个环形拓扑结构，形成一个单一的 TPU 单元。

2.  对于 RAM，Google 只是在每个单元上投入几百 GB 的 RAM 来简化本地内存访问。

3.  带宽:这实际上是 TPU 系统的秘密能力之一。每个 TPU 都安装在一个定制的网络背板上，允许以极快的速度进行 pod 内部通信。多组 TPU 被放在一起，它们共享相同的网络背板，以优化通信。

4.  自定义操作:BFloat16 简化了向 TPU 的移植逻辑，但从长远来看，他们正在考虑添加更多的自定义类型。TPUv1 实际上是 INT8，作为一个历史旁白。

5.  这也在雷达之外，但每个 TPU 单元都有一个内部处理器，在内部处理许多更复杂的逻辑，以便 TPU 芯片可以专注于原始数学。积极研究的一个领域是，寻找方法在运行中进行预处理，以便芯片本身能够保持供给。

## 张量流 1 + Pytorch

对我来说，从为 TPU 编写软件的角度来看，第一代 tensorflow 的许多设计决策和限制都是有意义的。对于像 TPU 这样的定制 ASIC 设备，您必须有一个预定义的图形，并且不能在运行中执行任意代码。如果您可以按需访问成千上万个 TPU 内核，那么关键的技巧是将您的代码分解成可以在每个内核上运行的单元，而不是简化整体逻辑。我认为 CUDA 支持是一种事后的想法，但该框架的成功是因为这是人们在现实世界中最有可能使用的实际硬件。谷歌花了很多周期优化 TPU 代码，却发现类似的优化在 CUDA 设备上不起作用，反之亦然。他们试图弥合差距，但越来越多地触及试图让不同的世界一起工作的极限。对于他们的内部工作，他们可以轻松地花钱雇人编写定制的 C++内核来优化运行在大型集群上的软件，但对于谷歌总部以外的人来说，这显然是不切实际的。

Pytorch 在过去几年中作为 Tensorflow 的替代产品迅速流行起来。这很大一部分是因为它允许人们使用内存中的(例如，非静态)图形，这使得调试更加简单(例如，我们可以附加一个调试器并在适当的位置查看网络变量，而不是必须添加日志语句并重复运行)。Tensorflow 2 完全支持这种模式，热切执行是未来的首选方法。同样，用于 Tensorflow 的 Keras Python 包装器已被提升为 Tensorflow 生态系统的成熟部分(例如，它现在是标准库的一部分)。

关于优化，Pytorch 只是走了一条更简单的路线，尽可能快地从高级代码转向 CUDA。这明显更容易优化，因此 Pytorch 团队的优化工作简单得多。然而，它们现在与 CUDA 紧密相连，并且延伸开来，与 Nvidia 能够推向市场的任何硬件紧密相连。他们一直在尝试在 Pytorch 和 CUDA 层之间添加编译器技术，但尽管这是问题所在，我不认为这是解决问题的正确地方。

## 进入功能编程

那么，对我来说，强迫程序员使用函数式范例是每个人的最终归宿。为了让编译器做出关于如何优化代码的正确决策，他们必须尽可能多地访问关于正在执行的操作的信息。试图生成一个中间代码块，然后对其进行分析以进行优化，可以产生短期的加速效果，但从长期来看，这是徒劳的。几十年的编译器理论告诉我们，无论元编译器有多聪明，它都无法与程序员竞争知道真正需要做什么。

或者，用一个虚构的例子来说，编译器有数千种方法来尝试和优化这个循环:

```
```
var i = 0
for n in 1...100000
{
    i = i + n
}
    print (i)
```

```

然而，一个人可以看到我们可以将其简化为

```
```f(n) = n * (n + 1) / 2```

```

运用数学。对我来说，我们使用函数式编程的原因不是它本身更容易，而是通过迫使程序员以更严格的风格编码，我们使编译器更容易为我们做出关于如何实际执行事情的决定。我们现在正在牺牲一点时间，让我们以后的生活更简单。举例来说，我曾经写过很多 C 语言代码，但是我花在调试内存问题上的时间和我试图添加新特性的时间一样多。在这方面，Swift 招致了运行时损失，但另一方面，我有两倍的时间来实现新特性。当您学会信任编译器来捕捉/防止某些类别的错误时，下一个级别的函数式编程就来了，因此您可以专注于问题的核心逻辑，而不是细节。

无论你如何编写核心深度学习逻辑本身，每个人都必须弄清楚如何实际安排他们的工作。为了做到这一点，最好的方法是强迫最终用户使用与他们正在操作的实际数据相匹配的数据原语，并考虑到它将在其上运行的硬件。然后，编译器可以找出将给定数据转换成实际操作的最佳方式。即使您手工实现，这也是编写定制代码失败的地方，因为每次我们的终端硬件发生变化，我们都必须编写新的内核。

## Swift + TPU 演示

时间会证明 Swift for Tensorflow 是否是更广泛的机器学习生态系统的前进方向。对谷歌本身来说，我相信这越来越成为他们未来做事的方式。让我们回到我们的第一个机器学习演示，一个应用于 MNIST 数据集的卷积神经网络，并使用 TPU 再次进行。为了将这个演示转换为在 TPU 上运行，从历史上来说，我们需要直接使用 C++或者通过一个高级 API(例如 Keras)来使用 c++，这个 API 对我们隐藏了粗糙的边缘。

要做到这一点，您需要按照 Google Cloud 一章中的说明设置一个远程服务器。你不需要 GPU 或 CUDA，因为你将使用 TPU。之后，您将需要在与您的服务器相同的区域中创建一个 TPU 实例，以便它们可以一起通信。首先确定您将在哪里创建 TPU (v3-8 是您所需要的全部)，然后向后工作到您的主机服务器的区域。启动并运行系统后，为云系统设置以下 shell 参数:

```
export XLA_USE_XRT=1
export  XRT_TPU_CONFIG="tpu_worker;0;<TPU_DEVICE_IP>:8470"
export  XRT_WORKERS='localservice:0;grpc://
localhost:40934'
export  XRT_DEVICE_MAP="TPU:0;/job:localservice/replica:0/ task:0/device:TPU:0"

```

现在，我们可以运行我们简单的 MNIST CNN 演示，使用 XLA 在 TPU 上运行我们的 swift 代码:

```
```
import Datasets
import TensorFlow

struct CNN: Layer {
  var conv1a = Conv2D<Float>(filterShape: (3, 3, 1, 32), padding: .same, activation: relu)
  var conv1b = Conv2D<Float>(filterShape: (3, 3, 32, 32), padding: .same, activation: relu)
  var pool1 = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))

  var flatten = Flatten<Float>()
  var inputLayer = Dense<Float>(inputSize: 14 * 14 * 32, outputSize: 512, activation: relu)
  var hiddenLayer = Dense<Float>(inputSize: 512, outputSize: 512, activation: relu)
  var outputLayer = Dense<Float>(inputSize: 512, outputSize: 10)

  @differentiable
  public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
    let convolutionLayer = input.sequenced(through: conv1a, conv1b, pool1)
    return convolutionLayer.sequenced(through: flatten, inputLayer, hiddenLayer, outputLayer)
  }
}

let batchSize = 128
let epochCount = 12
var model = CNN()
var optimizer = SGD(for: model, learningRate: 0.1)
let dataset = MNIST(batchSize: batchSize)

let device = Device.defaultXLA
model.move(to: device)
optimizer = SGD(copying: optimizer, to: device)

print("Starting training...")

for (epoch, epochBatches) in dataset.training.prefix(epochCount).enumerated() {
  Context.local.learningPhase = .training
  for batch in epochBatches {
    let (images, labels) = (batch.data, batch.label)
    let deviceImages = Tensor(copying: images, to: device)
    let deviceLabels = Tensor(copying: labels, to: device)
    let (_, gradients) = valueWithGradient(at: model) { model -> Tensor<Float> in
      let logits = model(deviceImages)
      return softmaxCrossEntropy(logits: logits, labels: deviceLabels)
    }
    optimizer.update(&model, along: gradients)
    LazyTensorBarrier()
  }

  Context.local.learningPhase = .inference
  var testLossSum: Float = 0
  var testBatchCount = 0
  var correctGuessCount = 0
  var totalGuessCount = 0

  for batch in dataset.validation {
    let (images, labels) = (batch.data, batch.label)
    let deviceImages = Tensor(copying: images, to: device)
    let deviceLabels = Tensor(copying: labels, to: device)
    let logits = model(deviceImages)
    testLossSum += softmaxCrossEntropy(logits: logits, labels: deviceLabels).scalarized()
    testBatchCount += 1

    let correctPredictions = logits.argmax(squeezingAxis: 1) .== deviceLabels
    correctGuessCount += Int(Tensor<Int32>(correctPredictions).sum().scalarized())
    totalGuessCount = totalGuessCount + batch.data.shape[0]
    LazyTensorBarrier()
  }

  let accuracy = Float(correctGuessCount) / Float(totalGuessCount)
  print(
    """
    [Epoch \(epoch + 1)] \
    Accuracy: \(correctGuessCount)/\(totalGuessCount) (\(accuracy)) \
    Loss: \(testLossSum / Float(testBatchCount))
    """
  )
}
```

```

## 结果

您应该会看到与我们第二章类似的结果:

```
Starting training...
[Epoch 1] Accuracy: 9645/10000 (0.9645) Loss: 0.11085216
[Epoch 2] Accuracy: 9745/10000 (0.9745) Loss: 0.078900985
[Epoch 3] Accuracy: 9795/10000 (0.9795) Loss: 0.057063542
[Epoch 4] Accuracy: 9826/10000 (0.9826) Loss: 0.05429901
[Epoch 5] Accuracy: 9857/10000 (0.9857) Loss: 0.042912092
[Epoch 6] Accuracy: 9861/10000 (0.9861) Loss: 0.043906994
[Epoch 7] Accuracy: 9871/10000 (0.9871) Loss: 0.041553106
[Epoch 8] Accuracy: 9840/10000 (0.984) Loss: 0.050182436
[Epoch 9] Accuracy: 9867/10000 (0.9867) Loss: 0.044656143
[Epoch 10] Accuracy: 9872/10000 (0.9872) Loss: 0.040160652
[Epoch 11] Accuracy: 9876/10000 (0.9876) Loss: 0.041967977
[Epoch 12] Accuracy: 9878/10000 (0.9878) Loss: 0.041590735

```

## 概述

利用您对 Swift for Tensorflow 的了解，您已经在 TPU(或者根据需要在 CPU 或 GPU 上)上运行了一个定制内核。时间会告诉我们还会支持什么样的后端，但对我来说，这是拥抱这种方法的真正力量，即编写一次代码并在任何地方运行的能力。