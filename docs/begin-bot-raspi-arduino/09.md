# 九、OpenCV 简介

自从第一章介绍树莓派以来，我们已经走过了很长的路。至此，您已经了解了 Pi 和 Arduino。你已经学会了如何对两块电路板进行编程。你和传感器和马达一起工作过。你已经制造了你的机器人，并给它编了程序，让它四处漫游并跟随一条线。

然而，老实说，你并没有真正需要树莓派的功能。事实上，这有点阻碍。你用机器人做的所有事情——漫游和循线，没有 Pi 你也可以用 Arduino 做得很好。现在是时候展示圆周率的真正力量，并了解为什么你想在你的机器人使用它。

在这一章中，我们将做一些你不能单独用 Arduino 做的事情。我们将连接一个简单的网络摄像头，并开始使用通常所说的计算机视觉。

## 计算机视觉

计算机视觉是允许计算机分析图像并提取有用信息的算法的集合。它应用广泛，并迅速成为日常生活的一部分。如果你有一部智能手机，你很可能至少有一个应用使用计算机视觉。大多数新的中高端相机都内置了面部检测功能。脸书使用计算机视觉进行面部检测。计算机视觉被运输公司用来跟踪他们仓库里的包裹。当然，它被用于机器人导航、物体探测、物体回避和许多其他行为。

这一切都始于一幅图像。计算机分析图像以识别线条、角落和大范围的颜色。这个过程被称为特征提取，它是几乎所有计算机视觉算法的第一步。一旦这些特征被提取出来，计算机就可以将这些信息用于许多不同的任务。

面部识别是通过将特征与包含面部特征数据的 XML 文件进行比较来完成的。这些 XML 文件被称为级联。它们可用于许多不同类型的对象，而不仅仅是面部。同样的技术也可以用于物体识别。您只需向应用提供您感兴趣的对象的特性信息。

计算机视觉也包含视频。运动跟踪是计算机视觉的一个常见应用。为了检测运动，计算机比较来自静止摄像机的单个帧。如果没有运动，特征将不会在帧之间改变。因此，如果计算机识别出帧之间的差异，最有可能的是运动。基于计算机视觉的运动跟踪比红外传感器更可靠，如第八章中讨论的 PIR 传感器。

计算机视觉的一个令人兴奋的最新应用是增强现实。从视频流中提取的特征可用于识别表面上的独特图案。因为计算机知道图案，所以它可以很容易地计算出表面的角度。然后在图案上叠加 3D 模型。这个 3D 模型可以是物理的东西，比如建筑物，也可以是具有二维文本的平面对象。建筑师用这种技术向客户展示一座建筑在地平线上的样子。博物馆用它来提供更多关于展览或艺术家的信息。

所有这些都是现代环境下计算机视觉的例子。但是应用的列表太大了，无法在这里深入讨论，而且还在不断增加。

### 开放计算机视觉

就在几年前，计算机视觉对业余爱好者来说还不太容易。它需要大量繁重的数学运算，甚至更繁重的处理。计算机视觉项目通常使用笔记本电脑完成，这限制了它的应用。

OpenCV 已经存在一段时间了。1999 年，英特尔研究院建立了一个促进计算机视觉发展的开放标准。2012 年，它被非营利组织 OpenCV 基金会接管。你可以在他们的网站上下载最新版本。然而，要让它在 Raspberry Pi 上运行还需要一点额外的努力。我们很快就会谈到这一点。

OpenCV 是用 C++原生编写的；但是，它可以在 C、Java 和 Python 中使用。我们对 Python 实现感兴趣。因为我们的电机控制器库与 Python 3 不兼容，所以我们需要安装 OpenCV for Python 2.7。

#### 安装 OpenCV

我们将在树莓派 上安装 OpenCV。你要确保你的树莓皮插在充电器上，而不是电池组上，给自己足够的时间来安装。我们将从源代码编译 OpenCV，这意味着我们将从互联网上下载源代码，并直接在 Pi 上构建它。需要注意的是，尽管这个过程并不困难，但确实需要很长时间，并且需要输入许多 Linux 命令。我通常在晚上开始这个过程，让最终的构建运行一整夜。

1.  登录您的树莓派。
2.  在 Pi 上打开一个终端窗口。
3.  确保 Raspberry Pi 已更新。

    ```py
    sudo apt-get update
    sudo apt-get upgrade
    sudo rpi-update
    sudo reboot

    ```

4.  这些命令安装了构建 OpenCV 的先决条件。

    ```py
    sudo apt-get install build-essential git cmake pkg-config
    sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev
    sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev
    sudo apt-get install libxvidcore-dev libx264-dev
    sudo apt-get install libgtk2.0-dev
    sudo apt-get install libatlas-base-dev gfortran

    ```

5.  下载 OpenCV 源代码和 OpenCV 贡献的文件。贡献的文件包含了许多 OpenCV 主发行版中没有的功能。

    ```py
    cd ~
    git clone https://github.com/Itseez/opencv.git
    cd opencv
    git checkout 3.1.0
    cd ~
    git clone https://github.com/Itseez/opencv_contrib.git
    cd opencv_contrib
    git checkout 3.1.0

    ```

6.  安装 Python 开发库和 pip。

    ```py
    sudo apt-get install python2.7-dev
    wget https://bootstrap.pypa.io/get-pip.py
    sudo python get-pip.py

    ```

7.  确保安装了 NumPy。

    ```py
    pip install numpy

    ```

8.  准备编译的源代码。

    ```py
    cd ~/opencv
    mkdir build
    cd build
    cmake -D CMAKE_BUILD_TYPE=RELEASE \
        -D CMAKE_INSTALL_PREFIX=/usr/local \
        -D INSTALL_C_EXAMPLES=OFF \
        -D INSTALL_PYTHON_EXAMPLES=ON \
        -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \
        -D BUILD_EXAMPLES=ON ..

    ```

9.  现在让我们编译源代码。这部分需要一段时间。有些人试图利用树莓派 的 ARM CPU 中的所有四个核心。然而，我发现这很容易出错，而且它从来没有为我工作过。我的建议是咬紧牙关:让 Pi 决定要使用的内核数量，并让它运行。如果您想大胆尝试，您可以通过在下面的行

    ```py
    make

    ```

    中添加`–j4`开关来强制 Pi 使用四个内核
10.  如果您尝试了–j4 开关，但失败了，大约在第四个小时，请输入以下行:

    ```py
    make clean
    make

    ```

11.  编译好源代码后，现在就可以安装它了。

    ```py
    sudo make install
    sudo ldconfig

    ```

12.  通过打开 Python 命令行来测试安装。

    ```py
    python

    ```

13.  导入 OpenCV。

    ```py
    >>>import cv2

    ```

现在，您的 Raspberry Pi 上应该已经安装了 OpenCV 的运行版本。如果导入命令不起作用，您需要确定它没有安装的原因。互联网是您排除故障的指南。

### 选择摄像机

在我们真正让 OpenCV 在我们的机器人上工作之前，我们需要安装一个摄像头。Raspberry Pi 有几个选项:Pi 摄像头或 USB 网络摄像头。

Pi 摄像机直接连接到专门为其设计的端口。一旦连接上，您需要进入 raspi-config 并启用它。Pi 摄像头的优势在于它比 USB 摄像头快一点，因为它直接连接到电路板。它不通过 USB 串行总线。这让它有了一点优势。

大多数 Pi 摄像机都配有一根短的 6 英寸带状电缆。由于树莓派在我们机器人上的位置，这是不够的。可以订购更长的电缆。Adafruit 有几个选择。但是，对于这个项目，我们将使用一个简单的网络摄像机。

USB 摄像头在任何电子产品零售商处都很容易买到。网上也有很多选择。对于这个基本的应用，我们不需要任何特别健壮的东西。任何能提供像样图像的相机都可以。拥有高分辨率也不是问题。由于我们是在 Raspberry Pi 的有限资源下运行相机，较低的分辨率实际上会有助于性能。记住，OpenCV 逐像素分析每一帧。图像中的像素越多，需要处理的事情就越多。

对于我的机器人，我选择了直播！Cam Sync HD by Creative(见图 9-1 )，这是一款基本的高清网络摄像头，内置麦克风，通过标准 USB 2 端口操作。这个项目我们不需要麦克风，但将来可能会需要。它可以捕捉 720p 高清视频，这对我们的机器人来说可能有点多，但如果性能受到影响，我总是在软件中降低分辨率。

![A457480_1_En_9_Fig1_HTML.jpg](img/A457480_1_En_9_Fig1_HTML.jpg)

图 9-1

Creative Live! Cam Sync HD

### 安装摄像机

大多数网络摄像头都安装在显示器的顶部。他们通常有一个折叠夹，为监视器上的摄像机提供支撑。不幸的是，这些夹子通常是作为相机机身的一部分模制而成的，如果不损坏相机就无法拆除。这当然适用于生活！凸轮同步。所以，再一次，一点点创造力开始发挥作用。

我用来安装传感器的支架以 45 度角从机器人的前面脱落。为了让自己轻松一点，我选择不在相机支架上钻孔。相反，我使用我的可靠的安装胶带和一对安装支架。当我爬上去的时候，我想让它爬得相当高，并且稍微向下。这个想法是给它最好的视角，以及任何直接在它前面的物体。我还希望镜头尽可能靠近中心轴，以使软件方面的事情更简单。图 9-2 显示安装摄像头后的机器人。

![A457480_1_En_9_Fig2_HTML.jpg](img/A457480_1_En_9_Fig2_HTML.jpg)

图 9-2

Camera mounted on robot

## OpenCV 基础知识

OpenCV 有许多功能。它拥有 500 多个库和数千种功能。这是一个非常大的主题——太大了，一章无法涵盖。我将讨论在您的机器人上执行一些简单任务所需的基础知识。

我说简单的任务。这些任务非常简单，因为 OpenCV 抽象了后台发生的大量数学运算。当我考虑几年前机器人爱好的状态时，我发现即使是最基本的东西也能轻易获得，这真是令人惊讶。

目标是在本章结束时建造一个能够识别球并向球移动的机器人。我介绍的函数将帮助我们实现这个目标。我强烈建议花时间浏览 OpenCV 网站上的一些教程( [`https://opencv.org`](https://opencv.org) )。

要在 Python 代码中使用 OpenCV，您需要导入它。而且，当您这样做时，您可能还需要导入 NumPy 库。NumPy 增加了许多数学和数字处理功能，使得使用 OpenCV 更加容易。所有与图像相关的代码都应该以此开头:

```py
import cv2
import numpy as np

```

在本章的代码讨论中，我假设这已经完成了。以`cv2`为前缀的函数是 OpenCV 函数。如果以`np`为前缀，则为 NumPy 函数。如果你想扩展你在这本书里读到的内容，这一点很重要。OpenCV 和 NumPy 是两个独立的库，但是 OpenCV 经常使用 NumPy。

### 使用图像

在本节中，您将学习如何从文件中打开图像，以及如何从摄像机中捕捉实时视频。然后，我们将看看如何处理和分析这些图像，从中获取有用的信息。具体来说，我们将研究如何识别特定颜色的球，并跟踪它在帧中的位置。

但是首先，我们有一个先有鸡还是先有蛋的问题。我们需要在所有的练习中看到我们图像处理的结果。为此，我们需要从如何显示图像开始。这是我们会广泛使用的东西，而且非常容易使用。但我想确保在你学习如何捕捉图像之前，我先覆盖它。

#### 显示图像

在 OpenCV 中显示图像实际上非常容易。`imshow()`功能提供了这一功能。该函数适用于静态图像和视频图像，并且两者之间的实现不会改变。`imshow()`功能打开一个新窗口显示图像。当您调用它时，您必须为窗口以及您想要显示的图像或框架提供一个名称。

这是关于 OpenCV 如何处理视频的重要一点。因为 OpenCV 将视频视为一系列独立的帧，所以几乎所有用于修改或分析图像的功能都适用于视频。这显然包括`imshow()`。

如果我们想显示一个加载到`img`变量中的图像，它看起来会像这样:

```py
cv2.imshow('img', img)

```

在这个例子中，第一个参数是窗口的名称。它出现在窗口的标题栏中。第二个参数是保存图像的变量。显示视频的格式完全相同。我通常使用变量 cap 进行视频捕获，因此代码如下所示:

```py
cv2.imshow('cap', cap)

```

如您所见，代码是相同的。同样，这是因为 OpenCV 将视频视为一系列独立的帧。实际上，视频捕获依靠一个循环来连续捕获下一帧。所以从本质上来说，显示一个文件中的静止图像和一个相机中的单独帧是完全一样的。

还有一个元素要显示。为了实际显示图像，`imshow()`函数要求也调用`waitKey()`。`waitKey()`功能等待键盘按键按下指定的毫秒数。许多人用这个来捕捉一个退出键。除非我需要按键，否则我通常将它传递为零。

```py
cv2.waitKey(0)

```

我们在本章中广泛使用了`imshow()`和`waitKey()`。

### 捕捉图像

使用 OpenCV 所需的图像有几个来源，所有这些来源都是两个因素的变体:文件或相机，以及静止或视频。在大多数情况下，我们只关心来自摄像机的视频，因为我们使用 OpenCV 进行导航。但是所有的方法都有优点。

打开一个静止图像文件是学习新技术的一个很好的方法，尤其是当您正在处理计算机视觉的特定方面时。例如，如果您正在学习如何使用滤镜来识别某种颜色的球，那么使用由三个不同颜色的球组成的静止图像(除此之外别无其他)可以让您专注于特定的目标，而不必担心用于捕捉实时视频流的底层框架。哦，这是一个伏笔，如果你没有意识到这一点。

从用相机捕捉静止图像中学到的技术可以应用到真实环境中。它允许您通过使用包含真实世界元素的图像来改进或微调代码。

显然，捕捉现场视频是我们在机器人中使用的目标。实时视频流是我们用来识别我们的目标对象，然后导航到它。随着您的计算机视觉经验的增长，您可能会将运动检测或其他方法添加到您的清单中。由于机器人上的摄像头的目的是实时采集环境信息，因此需要实时视频。

文件中的视频对于学习过程也非常有用。您可能希望从机器人捕捉实时视频，并将其保存到文件中供以后分析。比方说，你正在利用一天中你能找到的任何空闲时间从事你的机器人项目。你可以随身携带你的笔记本电脑，但是携带一个机器人却是另一回事。因此，如果你从你的机器人那里录制视频，你可以在没有机器人陪伴的情况下进行你的计算机视觉算法。

请记住，Python 和 OpenCV 的一大优点是它们是抽象的，并且在很大程度上是平台独立的。所以，你在 Windows 机器上写的代码移植到你的 Raspberry Pi 上。

出差，并希望在酒店休息一段时间？去家人那里度假，偶尔需要离开一下？在午餐时间或课间溜进一点机器人程序？将录制的视频与 Python 和 OpenCV 的本地实例一起使用，并使用您的检测算法。当你回家后，你可以把代码转移到你的机器人上，并进行现场测试。

在本节中，我们使用前三种技术。我向您展示了如何保存和打开视频文件，但在大多数情况下，我们将使用静止图像来学习检测算法，使用实时视频来学习跟踪。

#### 打开图像文件

OpenCV 使得处理图像和文件变得非常容易，尤其是考虑到后台发生的事情使得这些操作成为可能。打开图像文件也不例外。我们使用`imread()`函数从本地存储器打开图像文件。`imread()`函数有两个参数:文件名和颜色类型标志。打开文件显然需要文件名。颜色类型标志决定了是以彩色还是灰度打开图像。

让我们打开并显示一个图像。我将使用三个彩色球的图像，这也是本章后面学习如何检测颜色的图像。如果您已经安装了 Python 和 OpenCV，这个练习可以在 Pi 或您的计算机上完成。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`open_image.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread('color_balls_small.jpg')
    cv2.imshow('image',img)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python open_image.py`并按回车键。

一个窗口打开，在白色背景上显示三个彩球的图像(参见图 9-3 )。按任意键关闭它。

![A457480_1_En_9_Fig3_HTML.jpg](img/A457480_1_En_9_Fig3_HTML.jpg)

图 9-3

Three colored balls

由于 IDLE 与基于 Linux 的机器上的 GUI 系统的交互方式，如果您直接从 IDLE 运行代码，图像窗口将不会正常关闭。然而，通过从终端运行代码，我们没有这个问题。

#### 捕捉视频

用相机捕捉视频与打开文件略有不同。使用视频还有几个步骤。一个变化是我们必须使用一个循环来获得多个帧；否则，OpenCV 只会捕获单个帧，这不是我们想要的。通常使用开放的`while`回路。这会捕捉视频，直到我们主动停止它。

为了使测试更容易，我将球直接放在摄像机的前面(见图 9-4 )。现在，我们只想捕捉图像。

为了从摄像机中捕捉视频，我们将创建一个`videoCapture()`对象，然后在一个循环中使用`read()`方法来捕捉帧。`read()`方法返回两个对象:一个返回值和一个图像帧。返回值只是一个验证读取成功或失败的整数。如果读取成功，则值为 1；否则，读取失败，并返回 0。为了防止导致代码出错的错误，可以测试读取是否成功。

![A457480_1_En_9_Fig4_HTML.jpg](img/A457480_1_En_9_Fig4_HTML.jpg)

图 9-4

Ball positioned in front of the robot for testing

我们关心图像帧。如果读取成功，则返回一个图像。如果不是，则返回一个空对象来代替它。由于空对象不能访问 OpenCV 方法，所以当您试图修改或操作图像时，您的代码将会崩溃。这就是为什么测试读取操作是否成功是个好主意。

##### 查看摄像机

在下一个练习中，我们将打开之前安装的摄像机来观看视频。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为 view_camera.py。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np

    cap = cv2.VideoCapture(0)

    while(True):
        ret,frame = cap.read()

        cv2.imshow('video', frame)
        if cv2.waitKey(1) & 0xff == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存脚本的工作文件夹。
7.  类型`sudo python view_camera.py`。

这将打开一个窗口，显示您的摄像机看到的内容。如果您使用远程桌面会话来处理 Pi，您可能会看到以下警告消息:Xlib:extension RANR missing on display:10。此消息意味着系统正在寻找 vncserver 中不包含的功能。可以忽略。

如果您担心视频图像的刷新率，请记住，当我们通过远程桌面会话运行几个窗口时，我们对 Raspberry Pi 的要求非常高。如果您连接显示器和键盘来访问 Pi，它会运行得更快。如果您在没有可视化的情况下运行视频捕获，它会运行得更快。

##### 录制视频

录制视频是查看摄像机的延伸。要录制，您必须声明您将使用的视频编解码器，然后设置将传入视频写入 SD 卡的`VideoWriter`对象。

OpenCV 使用 FOURCC 码来指定编解码器。FOURCC 是视频编解码器的四字符代码。你可以在 [`www.fourcc.org`](http://www.fourcc.org) 找到更多关于 FOURCC 的信息。

当创建`VideoWriter`对象时，我们需要提供一些信息。首先，我们必须提供文件名来保存视频。接下来，我们提供编解码器，然后是帧速率和分辨率。一旦创建了`VideoWriter`对象，我们只需使用`VideoWriter`对象的`write()`方法将每个从写到文件中。

让我们录制一些机器人的视频。我们将使用 XVID 编解码器写入一个名为`test_video.avi`的文件。我们将使用上一个练习中的视频捕获代码，而不是从头开始。

1.  在空闲 IDE 中打开 view_camera.py 文件。
2.  选择文件➤另存为并将文件另存为`record_camera.py`。
3.  更新代码。在下面，新行以粗体显示:

    ```py
    import cv2
    import numpy as np

    cap = cv2.VideoCapture(0)
    fourcc = cv2.VideoWriter_fourcc(*'XVID')
    vidWrite = cv2.VideoWriter('test_video.avi', \
                    fourcc, 20, (640,480))

    while(True):
        ret,frame = cap.read()

        vidWrite.write(frame)

        cv2.imshow('video', frame)
        if cv2.waitKey(1) & 0xff == ord('q'):
            break

    cap.release()

    vidWrite.release()

    cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存脚本的工作文件夹。
7.  类型`sudo python record_camera.py`。
8.  让视频运行几秒钟，然后按 Q 结束程序并关闭窗口。

现在，您的工作目录中应该有一个视频文件。接下来，我们将看看从文件中读取视频。

代码中有几项需要注意。当我们创建`VideoWriter`对象时，我们以元组的形式提供了视频分辨率。这是整个 OpenCV 中非常常见的做法。此外，我们必须释放`VideoWriter`对象。这关闭了文件的写入。

##### 从文件中读取视频

从文件中回放视频与从摄像机中观看视频完全相同。唯一的区别是，我们提供要播放的文件的名称，而不是向视频设备提供索引。我们将使用`ret`变量来测试视频文件的结尾；否则，当没有更多的视频播放时，我们会得到一个错误。

在本练习中，我们将简单地回放我们在上一个练习中录制的视频。代码应该看起来非常熟悉。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`view_video.py`。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np

    cap = cv2.VideoCapture('test_video.avi')

    while(True):
        ret,frame = cap.read()

        if ret:
            cv2.imshow('video', frame)
        if cv2.waitKey(1) & 0xff == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存脚本的工作文件夹。
7.  类型`sudo python view_video.py`。

一个新窗口打开。它显示了我们在上一个练习中记录的视频文件。当到达文件结尾时，视频停止。按 Q 结束程序并关闭窗口。

### 图像转换

现在你知道了更多关于如何获取图像的知识，让我们来看看我们可以用它们做些什么。我们将看一些非常基本的操作。选择这些操作是因为它们将帮助我们达到跟踪球的目标。OpenCV 非常强大，它拥有比我在这里介绍的更多的功能。

#### 轻弹

很多时候，摄像机在项目中的摆放并不理想。经常，我不得不把相机倒过来装，或者因为这样或那样的原因，我需要翻转图像。

幸运的是，OpenCV 用`flip()`方法使这变得非常简单。`flip()`方法有三个参数:要翻转的图像、指示如何翻转的代码和翻转图像的目的地。最后一个参数仅在您想要将翻转的图像指定给另一个变量时使用，但是您可以在适当的位置翻转图像。

通过提供 flipCode，可以水平和/或垂直翻转图像。flipCode 为正、负或零。零水平翻转图像，正值垂直翻转图像，负值在两个轴上翻转图像。通常情况下，您会在两个轴上翻转图像，以有效地将其旋转 180 度。

让我们使用之前使用的三个球的图像来说明翻转帧。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`flip_image.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread('color_balls_small.jpg')
    h_img = cv2.flip(img, 0)
    v_img = cv2.flip(img, 1)
    b_img = cv2.flip(img, -1)

    cv2.imshow('image', img)
    cv2.imshow('horizontal', h_img)
    cv2.imshow('vertical', v_img)
    cv2.imshow('both', b_img)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python flip_image.py`并按回车键。

打开四个窗口，每个窗口都有不同版本的图像文件。按任意键退出。

#### 调整大小

您可以调整图像的大小。这有助于减少处理图像所需的资源。图像越大，需要的内存和 CPU 资源就越多。为了调整图像的大小，我们使用了`resize()`方法。这些参数是要缩放的图像、作为元组的所需维度以及插值。

插值是一种数学方法，用于确定如何处理像素的删除或添加。请记住，在处理图像时，您实际上是在处理一个多维数组，该数组包含组成图像的每个点或像素的信息。缩小图像时，您正在删除像素。当你放大一幅图像时，你是在增加像素。插值是发生这种情况的方法。

有三种插值选项。INTER_AREA 最适合用于归约。INTER_CUBIC 和 INTER_LINEAR 都适合放大图像，INTER_LINEAR 是两者中速度较快的。如果没有提供插值，OpenCV 使用 INTER_LINEAR 作为缩小和放大的缺省值。

三个球的图像目前为 800×533 像素。虽然它不是大号的，但我们会把它做小一点。让我们把两个轴的大小都设为当前大小的一半。为此，我们将使用 INTER_AREA 插值。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`resize_image.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread('color_balls_small.jpg')
    x,y = img.shape[:2]
    resImg = cv2.resize(img, (y/2, x/2), interpolation = cv2.INTER_AREA)

    cv2.imshow('image', img)
    cv2.imshow('resized', resImg)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python resize_image.py`并按回车键。

应该开了两扇窗。第一个有原始图像。第二个显示缩小的图像。按任意键关闭窗口。

### 使用颜色

颜色显然是处理图像的一个非常重要的部分。因此，它是 OpenCV 中非常重要的一部分。颜色可以做很多事情。我们将重点关注完成我们的最终目标所需的几个关键要素，即识别和用机器人追球。

#### 色彩空间

处理颜色的一个关键要素是颜色空间，它描述了 OpenCV 如何表达颜色。在 OpenCV 中，颜色由一系列数字表示。色彩空间决定了这些数字的含义。

OpenCV 的默认颜色空间是 BGR。这意味着每种颜色都由 0 到 255 之间的三个整数来描述，它们依次对应于三个颜色通道——蓝色、绿色和红色。表示为(255，0，0)的颜色在蓝色通道中具有最大值，绿色和红色都为零。这代表纯蓝色。鉴于此，(0，255，0)为绿色，(0，0，255)为红色。值(0，0，0)代表黑色，没有任何颜色，而(255，255，255)代表白色。

如果你过去曾与图形打交道，BGR 与你可能习惯的截然相反。大多数数字图形都是用 RGB 来描述的，即红色、绿色和蓝色。所以，这可能需要一点时间来适应。

有许多颜色空间。我们关心的是 BGR、RGB、HSV 和灰度。我们已经讨论了默认的颜色空间，BGR，和常见的 RGB 颜色空间。HSV 是色调、饱和度和值。色调代表 0 到 180 范围内的颜色。饱和度表示颜色从 0 到 255 有多白。值是从 0 到 255 的颜色与黑色的距离。如果饱和度和值都为 0，则颜色为灰色。饱和度和值 255 是色调的最亮版本。

色调有点复杂。它在 0 到 180 的范围内，0 和 180 都是红色。这就是记住色轮的重要性。如果 0 和 180 在红色空间中间的轮盘顶部相遇，当你围绕轮盘顺时针移动时，色相= 30 是黄色，色相= 60 是绿色，色相= 90 是蓝绿色，色相= 120 是蓝色，色相= 150 是紫色，色相= 180 将我们带回红色。

你最常遇到的是灰度。灰度就是它听起来的样子:图像的黑白版本。特征检测算法使用它来创建遮罩。我们用它来过滤物体。

要将图像转换到不同的色彩空间，可以使用`cvtColor`方法。它需要两个参数:图像和颜色空间常数。颜色空间常量内置于 OpenCV 中。分别是 COLOR_BGR2RGB、COLOR_BGR2HSV、COLOR_BGR2GRAY。你看到那里的模式了吗？如果您想从 RGB 颜色空间转换到 HSV 颜色空间，常量应该是 COLOR_RGB2HSV。

让我们把三个彩球的图像转换成灰度图像。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`gray_image.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread('color_balls_small.jpg')
    grayImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    cv2.imshow('img', img)
    cv2.imshow('gray', grayImg)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python gray_image.py`并按回车键。

这将打开两个窗口:一个显示原始彩色图像，另一个显示灰度图像。单击任意键退出程序并关闭窗口。

#### 颜色过滤器

对颜色进行过滤需要的代码非常少，但同时，这可能会有点令人沮丧，因为您通常不是在寻找特定的颜色，而是一个颜色范围。颜色很少是纯粹的，只有一种价值。这就是为什么我们希望能够在色彩空间之间转换。当然，我们可以在 BGR 寻找红色。但是要做到这一点，我们需要这三个值的具体范围。在所有色彩空间都是如此的情况下，通常在 HSV 空间中更容易调整到您需要的范围。

用于过滤特定颜色的策略相当简单，但是需要几个步骤，并且需要记住一些事情。

首先，我们将在 HSV 颜色空间中复制图像。然后，我们应用我们的过滤范围，使其成为自己的图像。为此，我们使用`inRange()`方法。它有三个参数:我们要应用滤镜的图像、下限值范围和上限值范围。`inRange`方法扫描提供的图像中的所有像素，以确定它们是否在指定的范围内。如果是，则返回 true 或 1；否则，它返回 0。这留给我们的是一个黑白图像，我们可以用它作为一个面具。

接下来，我们使用`bitwise_and()`方法应用蒙版。该方法获取两幅图像，并返回像素匹配的区域。因为这不是我们想要的，我们需要耍点小花招。出于我们的目的，`bitwise_and`需要三个参数:图像 1、图像 2 和一个遮罩。因为我们想要返回遮罩显示的所有内容，所以图像 1 和图像 2 都使用我们的原始图像。然后，我们通过指定遮罩参数来应用遮罩。因为我们忽略了几个可选参数，所以我们需要显式地指定掩码参数，就像这样:`mask = mask_image`。结果是图像只显示了我们要过滤的颜色。

演示这一点的最简单的方法是通过它走一走。一旦你知道发生了什么，代码其实很简单。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`blue_filter.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread("color_balls_small.jpg")
    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

    lower_blue = np.array([80,120,120])
    upper_blue = np.array([130,255,255])

    blueMask = cv2.inRange(imgHSV,lower_blue,upper_blue)

    res = cv2.bitwise_and(img, img, mask=blueMask)

    cv2.imshow('img', img)
    cv2.imshow('mask', blueMask)
    cv2.imshow('blue', res)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python blue_filter.py`并按回车键。

打开三个窗口，显示我们图像的不同版本。首先是常规图像。第二张是黑白图像，充当我们的面具。第三个是最终的蒙版图像。仅显示蒙版白色区域下的像素。

让我们花点时间浏览一下代码，弄清楚我们在做什么以及为什么做。

我们像处理所有脚本一样开始，先导入 OpenCV 和 NumPy，然后加载图像。

```py
import cv2
import numpy as np
img = cv2.imread("color_balls_small.jpg")

```

接下来，我们复制图像并将其转换到 HSV 颜色空间。

```py
imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

```

一旦进入 HSV 颜色空间，就更容易过滤蓝色球。正如我所讨论的，我们知道纯蓝色的色调值为 120。因为我们过滤的对象不太可能是纯蓝色的，所以我们需要给它一个颜色范围。在这种情况下，我们要寻找从 80(介于绿色和蓝色之间)到 130 的所有内容。我们还想过滤不是接近白色或接近黑色的颜色，所以我们使用值 120 和 255 作为饱和度和值范围。为了确保过滤器范围是 OpenCV 能够理解的格式，我们将它们创建为 NumPy 数组。

```py
lower_blue = np.array([80,120,120])
upper_blue = np.array([130,255,255])

```

有了指定的滤镜范围，我们可以使用它们和`inRange()`方法来确定图像的 HSV 版本中的像素是否在我们正在寻找的蓝色范围内。这将创建遮罩图像以排除所有非蓝色像素。

```py
blueMask = cv2.inRange(imgHSV,lower_blue,upper_blue)

```

接下来，我们使用`bitwise_and()`来应用我们的面具。因为我们希望返回蒙版中图像的所有像素，所以我们将原始图像作为图像 1 和图像 2 传递。这将图像与其自身进行比较，并返回整个图像，因为图像中的每个像素都与其自身匹配。

```py
res = cv2.bitwise_and(img, img, mask=blueMask)

```

最后，我们显示原始图像、遮罩和过滤后的图像。然后，在我们关闭窗口并退出程序之前，我们等待一个键被按下。

```py
cv2.imshow('img', img)
cv2.imshow('mask', blueMask)
cv2.imshow('blue', res)

cv2.waitKey(0)

```

正如你所看到的，一旦你知道它是如何工作的，过滤颜色是非常容易的。当你过滤红色时，事情变得有点复杂。红色出现在色调光谱的低端和高端，因此您必须创建两个滤镜并组合生成的遮罩。这可以很容易地用 OpenCV 的`add()`方法来完成，它看起来像这样:

```py
combinedMask = cv2.add(redMask1, redMask2)

```

最后，你得到的图像只有你想要的像素。对人眼来说，它很容易被识别为相关组。对于计算机来说，情况并非如此。本来，计算机不能识别黑色像素和蓝色像素之间的差异。这就是斑点检测发挥作用的地方。

### 斑点和斑点检测

斑点是相似像素的集合。它们可以是任何东西，从单调的圆形到 jpeg 图像。对计算机来说，像素就是像素，它无法区分球的图像和平面的图像。这就是计算机视觉如此具有挑战性的原因。我们已经开发了许多不同的技术来尝试推断关于图像的信息；每一种都在速度和准确性方面有所取舍。

大多数技术使用称为特征提取的过程，特征提取是一组算法的总称，这些算法对图像中的突出特征进行分类，如线条、边缘、大范围颜色等。一旦提取了这些特征，就可以对它们进行分析或与其他特征进行比较，以确定图像。这就是面部检测和运动检测等功能的工作原理。

我们将使用一种更简单的方法来跟踪一个对象。我们将使用上一节中的颜色过滤技术来识别大面积的颜色，而不是提取细节特征并进行分析。然后，我们将使用内置函数来收集关于像素组的信息。这种更简单的技术被称为斑点检测。

#### 找到一个斑点

OpenCV 使得斑点检测变得相当容易，尤其是在我们过滤掉所有我们不想要的东西之后。一旦图像被过滤，我们可以使用掩模进行干净斑点检测。OpenCV 的`SimpleBlobDetector`类识别斑点的位置和大小。

这个类并不像你想象的那么简单。它内置了许多需要启用或禁用的参数。如果启用，您需要确保这些值适用于您的应用。

设置参数的方法是`SimpleBlobDetector_Params()`。创建检测器的方法是`SimpleBlobDetector_create()`。您将参数传递给 create 方法，以确保一切设置正确。

一旦设置好参数并正确创建了检测器，就可以使用`detect()`方法来识别关键点。在简单斑点检测器的情况下，关键点表示任何检测到的斑点的中心和大小。

最后，我们使用`drawKeyPoints()`方法在斑点周围画一个圆。默认情况下，这会在斑点的中心绘制一个小圆。然而，可以传递一个标志，使得圆的大小相对于斑点的大小。

让我们看一个例子。我们将使用之前练习中的过滤器代码，并添加斑点检测。在本练习中，我们过滤图像中的蓝色球。然后我们用蒙版找到球的中心，围绕它画一个圆。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`simple_blob_detect.py`。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np

    img = cv2.imread("color_balls_small.jpg")
    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

    # setup parameters
    params = cv2.SimpleBlobDetector_Params()

    params.filterByColor = False
    params.filterByArea = False
    params.filterByInertia = False
    params.filterByConvexity = False
    params.filterByCircularity = False

    # create blob detector
    det = cv2.SimpleBlobDetector_create(params)

    lower_blue = np.array([80,120,120])
    upper_blue = np.array([130,255,255])

    blueMask = cv2.inRange(imgHSV,lower_blue,upper_blue)

    res = cv2.bitwise_and(img, img, mask=blueMask)

    # get keypoints
    keypnts = det.detect(blueMask)

    # draw keypoints
    cv2.drawKeypoints(img, keypnts, img, (0,0,255),
           cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    cv2.imshow('img', img)
    cv2.imshow('mask', blueMask)
    cv2.imshow('blue', res)

    # print the coordinates and size of keypoints to terminal
    for k in keypnts:
        print k.pt[0]
        print k.pt[1]
        print k.size

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python simple_blob_detect.py`并按回车键。

这将打开图像的三个版本。但是，原始图像现在有一个红色的圆圈围绕着蓝色的球。在终端窗口中，我们打印了球的中心坐标和大小。在本章的后面，当我们开始跟踪球时，会用到球的中心。

##### 参数

`SimpleBlobDetector`类需要几个参数才能正常工作。强烈建议通过将相应的参数设置为 True 或 False 来显式启用或禁用所有过滤器选项。如果启用了过滤器，您还需要为其设置参数。默认参数被配置为提取黑色圆形斑点。

在上一个练习中，我们简单地禁用了所有过滤器。由于我们正在处理一个球的过滤图像，并且我们在图像中只有一个斑点，我们不需要添加其他过滤器。虽然从技术上讲，您可以单独使用 SimpleBlobDetector 的参数，而不屏蔽所有其他的参数，但是在拨入所有参数以获得我们想要的结果时，这可能会更具挑战性。此外，我们使用的方法允许您更深入地了解 OpenCV 在后台做什么。

理解 SimpleBlobDetector 的工作方式对于更好地理解如何使用过滤器是很重要的。有几个参数可用于微调结果。

首先发生的是通过应用阈值将图像转换成几个二值图像。`minThreshold`和`maxThreshold`决定整体范围，而`thresholdStep`决定阈值之间的距离。

然后使用`findContours()`对每个二进制图像进行轮廓处理。这允许系统计算每个斑点的中心。已知中心后，使用`minDistanceBetweenBlobs`参数将几个斑点组合成一组。

组的中心作为关键点返回，组的总直径也是如此。计算每个滤波器的参数并应用滤波器。

##### 过滤器

下面列出了过滤器及其相应的参数。

###### filterByColor

这将过滤每个二进制图像的相对强度。它测量斑点中心的强度值，并将其与参数`blobColor`进行比较。如果它们不匹配，则该斑点不合格。强度从 0 到 255 测量；0 为暗，255 为亮。

###### 过滤区域

当单个斑点被分组时，计算它们的总面积。该过滤器寻找`minArea`和`maxArea`之间的斑点。

###### 过滤循环

圆度通过公式

![$$ \frac{4^{\ast }{\pi}^{\ast } Area}{perimeter^{\ast } perimeter} $$](img/A457480_1_En_9_Chapter_Equa.gif)

计算

这将返回一个介于 0 和 1 之间的比率，该比率将与`minCircularity`和`maxCircularity`进行比较。如果该值在这些参数之间，则结果中包含该斑点。

###### 过滤惯性

惯性是对斑点被拉长程度的估计。它是一个介于 0 和 1 之间的比率。如果值在`minInertiaRatio`和`maxInertiaRatio`之间，则在关键点结果中返回斑点。

###### filterby 凸性

凸度是一个介于 0 和 1 之间的比值。它测量斑点中凸曲线和凹曲线之间的比率。凸度参数为`minConvexity`和`maxConvexity`。

#### 斑点跟踪

我们在上一节中看到，斑点中心的 x 和 y 坐标作为关键点的一部分返回，用于跟踪斑点。要跟踪 blob，您需要使用来自机器人摄像机的实时视频流，然后定义跟踪对您的项目意味着什么。最简单的跟踪形式是简单地用斑点移动生成的圆。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`blob_tracker.py`。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np

    cap = cv2.VideoCapture(0)

    # setup detector and parameters
    params = cv2.SimpleBlobDetector_Params()

    params.filterByColor = False
    params.filterByArea = True
    params.minArea = 20000
    params.maxArea = 30000
    params.filterByInertia = False
    params.filterByConvexity = False
    params.filterByCircularity = True
    params.minCircularity = 0.5
    params.maxCircularity = 1

    det = cv2.SimpleBlobDetector_create(params)

    # define blue
    lower_blue = np.array([80,60,20])
    upper_blue = np.array([130,255,255])

    while True:
        ret, frame = cap.read()

        imgHSV = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

        blueMask = cv2.inRange(imgHSV,lower_blue,upper_blue)
        blur= cv2.blur(blueMask, (10,10))

        res = cv2.bitwise_and(frame, frame, mask=blueMask)

        # get and draw keypoint
        keypnts = det.detect(blur)

        cv2.drawKeypoints(frame, keypnts, frame, (0,0,255),
                          cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

        cv2.imshow('frame', frame)
        cv2.imshow('mask', blur)

        for k in keypnts:
            print k.size

        if cv2.waitKey(1) & 0xff == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`sudo python blob_tracker.py`并按回车键。

两个窗口打开:一个显示用于过滤颜色的遮罩，另一个显示视频流。应该在斑点周围画一个圆。

我启用了`filterByArea`和`filterByCircularity`来确保我只得到球。您可能需要调整检测器的参数来微调过滤器。

## 追球机器人

你现在知道如何用安装在机器人上的网络摄像头跟踪一个斑点了。在第八章中，你学习了一种叫做 PID 控制器的算法。当我们将 PID 控制器与我们的球跟踪程序结合起来时会发生什么？

接下来，让我们给这个小机器人编程，让它去追逐它一直在追踪的那个蓝色的球。为此，您将使用您刚刚学到的关于斑点跟踪的知识以及您在第八章中学到的知识。

PID 控制器期望以偏离期望结果的形式输入。因此，我们需要从定义期望的结果开始。在这种情况下，目标只是将球保持在框架的中间。因此，我们的误差值将是中心的方差，这也意味着我们需要定义帧的中心。一旦我们定义了中心，偏差就是从中心的 x 位置减去球的 x 位置。我们也将从中心的 y 位置减去球的 y 位置。

现在我们可以使用两个 PID 控制器来保持球在框架的中心。第一个控制器操纵机器人。当球在 x 轴上运动时，偏差要么是负的，要么是正的。如果是积极的，转向左边。如果是负数，转向右边。同样，我们可以用 y 轴来控制机器人的速度。正 y 方差驱动机器人前进，而负 y 方差驱动机器人后退。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`ball_chaser.py`。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np
    import time

    from Adafruit_MotorHAT import Adafruit_MotorHAT as amhat
    from Adafruit_MotorHAT import Adafruit_DCMotor as adamo

    # create motor objects
    motHAT = amhat(addr=0x60)
    mot1 = motHAT.getMotor(1)
    mot2 = motHAT.getMotor(2)
    mot3 = motHAT.getMotor(3)
    mot4 = motHAT.getMotor(4)

    motors = [mot1, mot2, mot3, mot4]

    # motor multipliers
    motorMultiplier = [1.0, 1.0, 1.0, 1.0, 1.0]

    # motor speeds
    motorSpeed = [0,0,0,0]

    # speeds
    speedDef = 100
    leftSpeed = speedDef
    rightSpeed = speedDef
    diff= 0
    maxDiff = 50
    turnTime = 0.5

    # create camera object
    cap = cv2.VideoCapture(0)
    time.sleep(1)

    # PID
    kp = 1.0
    ki = 1.0
    kd = 1.0
    ballX = 0.0
    ballY = 0.0

    x = {'axis':'X',
         'lastTime':int(round(time.time()*1000)),
         'lastError':0.0,
         'error':0.0,
         'duration':0.0,
         'sumError':0.0,
         'dError':0.0,
         'PID':0.0}
    y = {'axis':'Y',
         'lastTime':int(round(time.time()*1000)),
         'lastError':0.0,
         'error':0.0,
         'duration':0.0,
         'sumError':0.0,
         'dError':0.0,
         'PID':0.0}

    # setup detector
    params = cv2.SimpleBlobDetector_Params()

    # define detector parameters
    params.filterByColor = False
    params.filterByArea = True
    params.minArea = 15000
    params.maxArea = 40000
    params.filterByInertia = False
    params.filterByConvexity = False
    params.filterByCircularity = True
    params.minCircularity = 0.5
    params.maxCircularity = 1

    # create blob detector object
    det = cv2.SimpleBlobDetector_create(params)

    # define blue
    lower_blue = np.array([80,60,20])
    upper_blue = np.array([130,255,255])

    def driveMotors(leftChnl = speedDef, rightChnl = speedDef,
                    duration = defTime):
        # determine the speed of each motor by multiplying
        # the channel by the motors multiplier
        motorSpeed[0] = leftChnl * motorMultiplier[0]
        motorSpeed[1] = leftChnl * motorMultiplier[1]
        motorSpeed[2] = rightChnl * motorMultiplier[2]
        motorSpeed[3] = rightChnl * motorMultiplier[3]

        # set each motor speed. Since the speed can be a
        # negative number, we take the absolute value
        for x in range(4):
            motors[x].setSpeed(abs(int(motorSpeed[x])))

        # run the motors. if the channel is negative, run
        # reverse. else run forward
        if(leftChnl < 0):
            motors[0].run(amhat.BACKWARD)
            motors[1].run(amhat.BACKWARD)
        else:
            motors[0].run(amhat.FORWARD)
            motors[1].run(amhat.FORWARD)

        if (rightChnl > 0):
            motors[2].run(amhat.BACKWARD)
            motors[3].run(amhat.BACKWARD)
        else:
            motors[2].run(amhat.FORWARD)
            motors[3].run(amhat.FORWARD)

    def PID(axis):
        lastTime = axis['lastTime']
        lastError = axis['lastError']

        # get the current time
        now = int(round(time.time()*1000))
        duration = now-lastTime

        # calculate the error
        axis['sumError'] += axis['error'] * duration
        axis['dError'] = (axis['error'] - lastError)/duration

        # prevent runaway values
        if axis['sumError'] > 1:axis['sumError'] = 1
        if axis['sumError'] < -1: axis['sumError'] = -1

        # calculate PID
        axis['PID'] = kp * axis['error'] + ki * axis['sumError'] + kd * axis['dError']

        # update variables
        axis['lastError'] = axis['error']
        axis['lastTime'] = now

        # return the output value
        return axis

    def killMotors():
        mot1.run(amhat.RELEASE)
        mot2.run(amhat.RELEASE)
        mot3.run(amhat.RELEASE)
        mot4.run(amhat.RELEASE)

    # main program
    try:
        while True:
            # capture video frame
            ret, frame = cap.read()

            # calculate center of frame
            height, width, chan = np.shape(frame)
            xMid = width/2 * 1.0
            yMid = height/2 * 1.0

            # filter image for blue ball
            imgHSV = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

            blueMask = cv2.inRange(imgHSV, lower_blue, upper_blue)
            blur = cv2.blur(blueMask, (10,10))

            res = cv2.bitwise_and(frame,frame,mask=blur)

            # get keypoints
            keypoints = det.detect(blur)
            try:
                ballX = int(keypoints[0].pt[0])
                ballY = int(keypoints[0].pt[1])
            except:
                pass

            # draw keypoints
            cv2.drawKeypoints(frame, keypoints, frame, (0,0,255),
                              cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

            # calculate error and get PID ratio
            xVariance = (ballX - xMid) / xMid
            yVariance = (yMid - ballY) / yMid

            x['error'] = xVariance/xMid
            y['error'] = yVariance/yMid

            x = PID(x)
            y = PID(y)

            # calculate left and right speeds
            leftSpeed = (speedDef * y['PID']) + (maxDiff * x['PID'])
            rightSpeed = (speedDef * y['PID']) - (maxDiff * x['PID'])

            # another safety check for runaway values
            if leftSpeed > (speedDef + maxDiff): leftSpeed = (speedDef + maxDiff)
            if leftSpeed < -(speedDef + maxDiff): leftSpeed = -(speedDef + maxDiff)
            if rightSpeed > (speedDef + maxDiff): rightSpeed = (speedDef + maxDiff)
            if rightSpeed < -(speedDef + maxDiff): rightSpeed = -(speedDef + maxDiff)

            # drive motors
            driveMotors(leftSpeed, rightSpeed, driveTime)

            # show frame
    ##        cv2.imshow('frame', frame)
    ##        cv2.waitKey(1)

    except KeyboardInterrupt:
        killMotors()
        cap.release()
        cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`sudo python ball_chaser.py`并按回车键。

几秒钟后，你的机器人应该开始向前移动。如果框架内有一个蓝色的球，它应该转向它。机器人试图将球保持在框架的中心。

这段代码中的一些内容与我们过去的做法有些不同。最值得注意的是，我们将 x 和 y 轴的值放入字典中。我们这样做是为了在将值传递给 PID 控制器时将它们保持在一起，这是所做的另一项更改。PID 函数已更新为接受单个参数。然而，它所期望的参数是一个字典。它被赋给函数中的轴变量。然后所有的变量引用都被更新以使用字典。结果在 axis 字典中更新，然后分配给主程序中的适当字典。

我还确保消除任何会影响主循环或相机刷新率的延迟。因为整个程序是在单个进程中运行的，所以它没有我们将进程分解成不同线程时那么快。因此，机器人可能会错过球并跑偏。

## 摘要

在这一章中，我们开始利用 Raspberry Pi 提供的一些令人兴奋的功能。与单独使用微控制器相比，计算机视觉允许我们执行更复杂的任务。

为了准备使用视觉系统，我们在机器人上安装了一个基本的网络摄像头。这需要特别考虑，因为这些网络摄像头不是为安装而设计的。当然，你的解决方案可能与我的不同，所以你可以在安装相机时发挥一些创造力。之后，我们准备安装 OpenCV。

OpenCV 是一个开源社区开发的计算机视觉平台，它使许多视觉功能变得非常简单。在 Raspberry Pi 上安装软件需要相当长的时间，主要是因为我们必须从源代码编译它，尽管它的功能令人印象深刻，但 Raspberry Pi 没有笔记本电脑或 PC 的处理能力，所以编译代码需要一段时间。但是一旦编译和安装，我们就能做一些有趣的事情。

我们用静止图像做了一些练习。这让我们可以学习 OpenCV 的一些基础知识，而不需要处理视频。一旦我们学会了一些基础知识，我们就学会了从摄像机中提取现场视频，并运用我们在静态图像中学到的知识。使用我们在这一章学到的颜色过滤和斑点追踪技术，我们给了我们的机器人看见和跟随一个球的能力。