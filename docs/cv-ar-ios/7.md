# 7.使用 ARKit 和 OpenCV 的增强现实

ARKit 一直是苹果最受欢迎的 iOS 11 功能之一。每天都有更多创新的增强现实应用被开发出来。在这个项目中，我们将专注于使用苹果的 ARKit 建立您的第一个增强现实项目，并在 Unity 中实现它。

## 为什么用 OpenCV 而不用 Core ML？

### 支持多种平台

虽然 Core ML 可能是仅支持 iOS 11 的 iPhone 应用程序的最佳选择，但无论何时您想要向后兼容或想要支持多个平台，您都必须为每个平台构建一个自定义方法或使用 OpenCV。

机器学习并不适合所有问题。尽管苹果公司为物体和场景检测等项目提供了几个免费的开源模型，但计算机视觉的领域远不止机器学习，OpenCV 是访问这些资源的最便捷方式。

### ARKit 和 OpenCV 集成

通过委托调用，ARSession 允许直接访问当前帧数据。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figa_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figa_HTML.jpg)

当一个新的框架变得可见时，这个函数被调用。我们可以从这里通过一个框架得到原始数据的 CVPixelBufferRef。虽然框架中的数据是以平面 YCbCr 格式给出的，但我们只需要灰度版本。我们错过了转换阶段，只捕捉 Y 帧，因为我们知道它是 YUV 格式。

我们是这样做的:

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figb_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figb_HTML.jpg)

我们只需要 Y 帧，它被放在第一位，如前所述。

为此，我们使用平面索引为 0 的![../images/496594_1_En_7_Chapter/496594_1_En_7_Figc_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figc_HTML.jpg)技术，而不是标准的![../images/496594_1_En_7_Chapter/496594_1_En_7_Figd_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figd_HTML.jpg)方法。

最终，由于我们获得的帧是在横向模式下，我们所要做的就是在移动它到任何 OpenCV magic 之前旋转它。

逻辑正在被重用。

如果你正在制作一个也支持 iOS 10 的增强现实应用，你可以通过改变 AVCaptureVideoDataOutput 的格式来适应 ARKit，从而使用前面的方法

![../images/496594_1_En_7_Chapter/496594_1_En_7_Fige_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Fige_HTML.jpg)

并使用 SampleBufferRef 获得相同的 CVPixelBufferRef。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figf_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figf_HTML.jpg)

### 综上

虽然 iOS 11(以及更高版本)的安装基数一直在增加，但一个典型的增强现实应用程序无法单独集成苹果最新更新中包含的工具。然而，通过将我们从 ARKit 获得的帧数据与 OpenCV 集成，我们将能够构建看似令人信服的交互，因为它们使用 OpenCV 的底层实现和 ARKit 的便利方法和 API 真正增加了真实性。交互可以是从身体跟踪到深度分析的任何事情(图 [7-1](#Fig1) )。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Fig1_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Fig1_HTML.jpg)

图 7-1

在 iOS 中使用 ARKit 进行身体跟踪

### 先决条件

我们需要一台 Mac 电脑来开发我们的程序，并需要一台 iOS 设备来运行它们，因为我们正在为苹果生态系统而构建。

### 五金器具

硬件方面需要一台兼容 macOS Catalina 的 macOS PC。身体监测程序也需要强大的苹果 A12 仿生 CPU 才能正常工作。以下 Mac 电脑和 iOS 设备有资格参加此促销活动:

![../images/496594_1_En_7_Chapter/496594_1_En_7_Fig2_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Fig2_HTML.jpg)

图 7-2

符合此促销条件的 Mac 电脑和 iOS 设备

### 软件

您需要在 Mac 上安装以下应用程序来运行演示:

*   Unity3D 2019.1.5f1 带 iOS 构建目标

*   MacOS Catalina 10.15 (beta 版)

*   Xcode 11 (beta 版)

你的设备应该安装 iOS 13(测试版)或 iPadOS 13(测试版)。

如您所见，在撰写本文时，大部分程序仍处于测试阶段。请记住，设备可能会变得缓慢或不可靠，因此要采取额外的预防措施，以避免丢失重要数据。

### 逐步跟踪身体

让我们从 ARKit 魔法开始吧。在电脑上启动 Unity3D 2019.1，开始一个新项目(图 [7-3](#Fig3) )。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Fig3_HTML.png](../images/496594_1_En_7_Chapter/496594_1_En_7_Fig3_HTML.png)

图 7-3

Unity3D 2019

### 步骤 1:设置主要场景

Unity3D 将从创建一个空白场景开始。在添加任何可视元素或编写任何代码之前，我们必须首先导入必要的依赖项。ARKit 工具包包括骨骼跟踪功能。因此，ARKit 和 ARFoundation 依赖包必须被导入。使用 AR 会话和 AR 会话源对象创建一个新场景。这些对象控制着 iOS 摄像头，同时还提供了过多的 ARKit 功能。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Fig4_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Fig4_HTML.jpg)

图 7-4

使用应用程序

此外，创建一个新的 C#脚本，并将其附加到一个名为人体跟踪(HumanBodyTracking.cs)的空游戏对象。

应该是这样的

![../images/496594_1_En_7_Chapter/496594_1_En_7_Fig5_HTML.png](../images/496594_1_En_7_Chapter/496594_1_En_7_Fig5_HTML.png)

图 7-5

应用程序的前端

### 步骤 2:建立骨架

我们现在可以开始实现交互性了，视觉部分已经就位。在 HumanBodyTracking.cs 脚本中添加对 ARHumanBodyManager 类的引用。分析摄像机数据以检测人体的主要脚本是 ARHumanBodyManager。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figg_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figg_HTML.jpg)

我们将使用一些简单的 Unity3D 球体来表示关节。每个球体代表一种不同类型的关节。要逐帧修改关节数据，添加一个 C# Dictionary 类。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figh_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figh_HTML.jpg)

最后，添加对框架用户界面的引用。关节将使用球体对象，骨骼将使用线条对象。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figi_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figi_HTML.jpg)

### 第三步:找到被跟踪的尸体

这是指南中最重要的部分！多亏了 ARKit，身体追踪从未如此简单或有效。您只需使用 ARHumanBodyManager 对象订阅“humanBodiesChanged”事件。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figj_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figj_HTML.jpg)

奇迹发生在事件处理程序中。事件参数包括关于被跟踪的主体的信息。这是你得到尸体的方法:

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figk_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figk_HTML.jpg)

这不是小菜一碟吗？现在，让我们把它们放在一起，展示我们在之前的迭代中构建的 Unity 用户界面的框架。

请注意，截至本文撰写时，ARKit 只允许跟踪一具尸体。

### 第四步:展示骨架

接下来的代码行更新相机空间中的关节位置。球体和线条覆盖在来自 iOS 设备的摄像头馈送的顶部。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figl_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figl_HTML.jpg)

苹果支持 92 种不同类型的关节(指数)。但是这些类型的关节很少被跟踪！它们相邻关节的位置被用来推断它们中的大多数。为了方便起见，我选择了 14 种接头类型，以便我可以与 Kinect 摄像头进行公平的比较。

这就是适当关节的连接方式和人体骨骼的形成方式:

![../images/496594_1_En_7_Chapter/496594_1_En_7_Fign_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Fign_HTML.jpg)

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figm_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figm_HTML.jpg)

关节在 3D 空间的位置和旋转由 ARKit 提供！这就是如何在 2D 屏幕空间中更新球体的比例、位置和旋转:

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figo_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figo_HTML.jpg)

就是这样！让我们在真实的 iOS 设备上构建并测试我们的应用程序！

### 步骤 5:创建和部署

最终，我们必须在真实的设备上构建和测试这个项目。我们不能在 macOS 上测试我们的代码，因为 ARKit 只在 iOS 和 iPadOS 上可用。在 Unity 中选择文件➤构建设置。选择 iOS 构建目标后，单击构建按钮。您必须为生成的项目选择一个保存位置。耐心等待，直到 Unity 的构建过程完成。Unity()将创建一个 Xcode 项目。xcodeproj)。使用 Xcode 11 beta，打开项目。如果您使用旧版本的 Xcode，您会得到一个错误，并且您的项目不能正常运行。给出你的 iOS 开发凭证，连接你的 iOS 13 设备，然后在项目启动后点击运行按钮。项目将以这种方式部署到设备上。

完成后，将摄像机对准一个人，就可以看到 3D 覆盖图出现在被跟踪的身体上！

### ARKit 3 中的人脸追踪

![../images/496594_1_En_7_Chapter/496594_1_En_7_Fig6_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Fig6_HTML.jpg)

图 7-6

ARKit 3 中的人脸追踪

### 用户级别:基本

自从增强现实被引入行业以来，它已经为我们提供了为移动技术用户开发交互式和令人兴奋的机会的能力。AR 可以以多种方式节省我们的时间，包括检查我们理想的沙发在公寓中的外观，确定选择的油漆颜色是否适合室内，甚至立即试穿新的化妆品。

人脸追踪是 iPhone X 和更新版本上 ARKit 3 的一个强大功能。我们可以快速创建应用程序，用合适的组件试穿珠宝、口红或眼镜。在这个例子中，我们将专注于最后一个选择:创建一个 AR 应用程序，通过简单的框架颜色转换来试戴眼镜。

### ARKit 3 的设置和规格

由于 iPhone X 中引入的 A11 仿生芯片是 ARKit 3 进行 AR 面部跟踪所需要的，我们将需要这个系统或一个更新的系统来使我们的例子工作。我们的项目需要 Unity 2019.1 或更高版本来利用 ARKit 3 的所有功能，如多面追踪。我们需要在构建项目后安装所需的 ARKit 包。由于 ARKit for Unity 自 2019 年 7 月起已包含在 ARFoundation 捆绑包中，我们需要从窗口菜单打开软件包管理器并安装以下软件包:

1.  avfoundation

2.  AR 子系统

3.  所有必需的人脸跟踪脚本的 AR 人脸跟踪

4.  用于跟踪姿势驱动程序脚本的 XR 传统输入辅助对象

将项目迁移到 iOS 平台，并在安装后设置播放器配置，这是一个很好的想法。在播放器设置窗口中将目标最低 iOS 版本设置为 11.0，因为 ARKit 3.0 需要最低 11.0 的 SDK 版本。

在同一个窗口中，从 Architecture 下拉列表中选择 ARM64，因为 ARKit 仅支持 64 位设备。您还需要指定相机使用描述，这是当应用程序请求批准使用系统相机时将出现的脚本。你可以把文字改成任何你想要的，比如“ARKit 3 人脸检测。”

### 增强现实中的会话对象

一旦项目被正确设置，我们就可以开始创建必需的 AR 会话对象。最初，我们在场景中创建一个 ARSession 游戏对象，并向其添加两个脚本:一个 AR 会话脚本和一个 AR 输入管理器脚本。

然后，使用 AR 会话起源脚本和 AR 界面管理器，我们创建 AR 会话起源对象。由于 ARKit 很少使用设备的前置摄像头——它只用于面部跟踪/识别场景——面部管理器脚本至关重要。在 AR 会话源中有一个用于摄像机脚本的空字段。

让我们将主摄像机对象放在 AR 会话源对象中，然后在 AR 会话源脚本中引用它。将需要三个脚本:

*   带跟踪的姿势驱动程序(在跟踪的姿势驱动程序中，将姿势源更改为“彩色摄影机”,并启用“使用相对变换”)

*   AR 相机历史

*   AR 相机管理器

你有没有发现在 AR 会话起源脚本的 AR 面部管理器中有一个面部预设的空字段？这就是我们将用来想象我们的基于 AR 人脸的眼镜的预设。

### 预制 AR 眼镜

我们需要制作一个新的游戏对象，我们将其命名为 GlassesPrefab。我们需要做的第一件事就是给它附加两个组件。第一个是 ARFace 变量，它将为我们提供已经识别的人脸点数据。第二个是 ARGlassesController，这是一个脚本，负责根据 ARFace 更新的数据定位眼镜。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Fig7_HTML.png](../images/496594_1_En_7_Chapter/496594_1_En_7_Fig7_HTML.png)

图 7-7

ARFace 应用程序

之后，我们将需要在预设中构建一个游戏对象，它将附加到我们的眼镜网格上，我们将使用 ARGlassesController 来操纵它。预设的最终形式应该类似于上图所示。被操纵的网格必须作为一个子对象保存在我们的预设中。ARFaceManager 控制预设的位置，ARFace 顶点是与检测到的面相关的特征的局部点。

### ARGlassesController 类

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figp_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figp_HTML.jpg)

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figq_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figq_HTML.jpg)

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figr_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figr_HTML.jpg)

### ARKit 3 中的 ARFace 数据和人脸检测

程序很简单:当 ARFaceManager 检测到一张脸时，它会发送新的 ARFace 数据。由于我们的控制器链接到 ARFace 的更新案例，每当检测到一组新的人脸点坐标时，脚本就会更新眼镜的位置。

顶点阵列索引通常应用于人脸上的相同特征点；唯一根据一个人的面部特征而变化的是整体位置和面部网格。我们将眼镜对准顶点指数 16，结果如下。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figt_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figt_HTML.jpg)

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figs_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figs_HTML.jpg)

### 颜色选择器

我们喜欢在一个简单的 UI 中使用连接到按钮的 ColorController 脚本来调整帧的颜色，所以我们将 ARGlassesController 设置为一个 singleton，以便 ColorController 可以在场景中任何时候访问它。

![../images/496594_1_En_7_Chapter/496594_1_En_7_Figu_HTML.jpg](../images/496594_1_En_7_Chapter/496594_1_En_7_Figu_HTML.jpg)

SetFrameColor 函数链接到场景中每个颜色按钮的 Click 事件，允许我们在单击按钮时将帧颜色更改为控制器中指定的颜色。

## 摘要

上述说明适用于增强现实人脸跟踪应用程序。这只是增强现实和 ARKit 中许多可能应用的一个例子。ARKit 面部跟踪 API 可用于构建更复杂的应用程序，不仅适用于时装和化妆品，还适用于工业和企业应用程序，如员工身份识别和用户认证。今天的苹果为我们提供了复杂和多功能的工具，可以支持成千上万的应用程序。鉴于未来公司将更多地投资于增强现实硬件和软件，我们很高兴看到这项技术在不久的将来如何发展，以及苹果在不久的将来将在硬件和软件方面提供什么。