# 二、人工神经网络

机器学习(ML)问题可以分为三类:有监督的、无监督的和强化的。在监督学习中，人类专家在受限环境中进行一些实验，并注意到它们的结果。监督学习算法探索从实验中收集的数据，以将输入映射到输出。例如，一个受限的环境可能有一个机器人想要从一个小房间的一边走到另一边。房间里有一些障碍物可能会使机器人摔倒。主管提供如何到达墙壁而不摔倒的指导。这是通过以例子的形式给机器人知识来帮助它学习如何通过障碍来实现的。机器人利用这些知识来增加通过障碍而不摔倒的概率。在这种情况下，机器人的知识完全依赖于人类。

在强化学习中，人类给机器人一个衡量标准来评估它的表现。机器人必须最大化这个度量来达到它的目标。它不知道何时向右移动。基于度量，机器人将尝试移动不同的位置并计算度量。如果机器人掉在一个给定的位置，那么下次它必须避开它。这样，机器人就会找到使它到达目标而不摔倒的路。

与监督和半监督学习相比，无监督学习既不会给出实验结果，也不会给出度量。根本没有人类来引导它。这很有挑战性。

人工神经网络(简称 ANN)是一种能解决所有这些问题的算法。本书只讨论使用 ANN 的监督学习。ANN 是一个受生物启发的 ML 模型，模仿人类大脑的运作。这是谈论深度学习(DL)时要涵盖的最重要的话题之一。理解只有几层和几个神经元的简单人工神经网络的操作，就更容易理解复杂模型是如何工作的。

在这一章中，将介绍学习 CNN 如何工作的先决条件。它从探索初级水平的人工神经网络开始。从知道它是线性模型的集合开始，你会发现它根本不是一个奇怪的概念；事实上，你已经知道了。本章讨论了一些与人工神经网络相关的概念，如学习率、反向传播和过拟合。本章将帮助你理解为什么我们需要人工神经网络中的学习率，以及它对训练是否有用。对单层感知器使用一个非常简单的 Python 代码，学习率值将被改变以捕捉它的想法，并注意改变学习率如何影响结果。它还讨论了反向传播算法如何用于更新人工神经网络的权重。本章还解释了过度拟合，这是对未知样本预测不佳的原因之一。一种基于回归的正则化技术通过简单的步骤来说明如何避免过拟合。人工神经网络有一个特殊的图表，使解释其结果更容易。本章绘制了数学表示及其图形，并探讨了令初学者感到困难的一点，即如何确定最佳的神经元数量和隐藏层。最后，给出了一个用人工神经网络进行 Python 分类的例子。

## 人工神经网络简介

监督学习问题分为两大类:分类和回归。回归输出是连续的数字，而分类输出是分类标签。每种类型的问题都可以使用线性或非线性模型。分类问题也可以分为二元或多类分类问题。所有这些类型的问题都可以使用神经网络来解决。也就是说，可以使神经网络产生连续或离散的输出。它可以处理二元或多类问题，并模拟线性和非线性函数。ANN 是一种通用函数逼近器(即 ANN 可以模拟任何线性和非线性函数的运算)。ANN 是一个参数模型，它有一组从问题中学习到的参数，如权重和偏差。它还有许多可以由工程师调整的超参数，例如学习速率和隐藏层数。

人工神经网络实际上由线性模型组成，这些模型被组合在一起以解决复杂的问题。下一小节讨论人工神经网络的基本构件实际上是一个线性模型。

### 线性模型是人工神经网络的基础

对于初学者来说，最简单的模型类型是线性模型。当然，每个人都知道线性模型，这使得接下来的解释更容易。我们可以从一个简单的回归问题开始，我们希望为表 2-1 中所示的样本创建一个线性模型。拟合此类数据的最佳线性模型是什么？让我们看看。

表 2-1

简单回归问题

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

输入(X)

 | 

输出(Y)

 |
| --- | --- |
| Two | six |

“线性模型”是指将每个输入映射到其相应输出的线。我们将从最简单的线性模型开始，如方程 2-1。该模型将输入和输出均衡在一起，方程中没有任何其他参数。

之后，我们创建了第一个模型。有人可能会问，构建任何模型的培训部分在哪里？答案是这个模型是非参数模型。“非参数”意味着模型没有可以从数据中学习的参数。因此，做这项工作不需要培训。在本章的后面，将添加一些参数。

*Y* = *X* (等式 2-1)

在常规的 ML 流水线中，在建立一个模型之后，我们必须测试它。在传统问题中，会有更多的样本，数据会被分成训练集和测试集。在训练模型之后，基于训练数据开始测试。如果它在训练数据上做得很好，那么我们可以在看不见的测试数据上逐步测试它。这是因为，如果一个模型不能很好地处理它所训练的数据，那么对于看不见的数据，情况可能会更糟。总之，我们的例子使我们摆脱了这样的工作，因为它只有一个样本，不需要培训。但是没有训练阶段并不意味着没有测试阶段。让我们基于这样一个样本来测试我们的模型。

测试阶段检查模型预测未知样本而非训练样本输出的准确性。基于 X=2 的示例，当应用于模型时，它也将返回 2。这是因为输入总是等于输出。除了预测和期望输出的位置，线性模型如图 2-1 所示。

![img/473634_1_En_2_Fig1_HTML.jpg](img/473634_1_En_2_Fig1_HTML.jpg)

图 2-1

非参数线性模型的预测和期望输出

我们可以简单地将期望输出和预测输出之间的差值作为等式 2-2。差值为 26 = 4。

*误差* = *预测*—*期望*(等式 2-2)

误差的存在意味着我们必须改变模型中的某些东西以减少误差。回头看看方程 2-1 中的模型，我们看到没有我们可以改变的参数。这个等式只有我们无法改变的输入和输出。因此，我们可以给这个等式添加一个参数**，这有助于输入和输出之间的映射。方程 2-3 显示了修正的模型方程。**

 ***Y* = *aX* (方程式 2-3)

假设 ***a*** 的初始值为 **1.5** 。该方程在 2-3ʹ.方程中给出这样的线性模型如图 2-2 所示。

*Y* = **1.5** *X* (方程式 2-3’)

![img/473634_1_En_2_Fig2_HTML.jpg](img/473634_1_En_2_Fig2_HTML.jpg)

图 2-2

参数线性模型

请注意，添加参数后，模型现在是参数化模型。这是因为至少有一个参数需要从数据中学习。现在，在建立了新的模型之后，我们可以预测样本的输出。预测产量为 *Y* <sub>*预测*</sub> = 1.5(2) = 3。然后我们可以测量误差。根据等式 2-2，误差为 36 = 3。与先前的误差相比，与没有参数的先前模型相比，具有参数 *a* = 1.5 的新模型似乎增强了结果。但是预测仍然有误差，我们需要减少。

我们可以想象方程 2-1 中的第一个模型实际上是用方程 2-3 来表示的，但是参数**总是设置为 **1** 。比较 *a* = 1 时产生的误差和***a =*****1.5**时的误差，也就是***—*****4**时的误差，似乎在***a =*****1.5**为 3 时误差减小了。人们可能想知道值 3 怎么会小于****4**。答案是，误差中的负号只是说预测输出低于期望输出。差异量是误差的绝对值。也就是说，****4**的误差意味着期望输出与预测输出之间存在 4 的差值，并且期望输出低于预测输出，因为误差为负。注意，改变方程 2-2 中预测输出和期望输出的位置将改变误差的符号。现在让我们回到我们的问题上来。******

 ******当 *a* = 1.5 时，结果比 *a* = 1.0 更好，这意味着增加该参数的值将减少误差。因此，我们知道变化的方向。我们试试用 *a* = 2.0。预测输出会是 *Y* <sub>*预测*</sub> = 2.0(2) = 4。这种情况下的误差将等于 46 = 2。误差比以前减少了很多。

根据前面的结果，我们可以推导出参数和误差之间的关系。使用 *a* = 1，误差为 4。将参数( *a* = 1.5)增加 0.5，误差减少 1.0 至 3。参数( *a* = 2.0)再加 0.5，误差减少 1.0，为 2。因此，将参数增加 0.5 会将误差减少 1.0 倍。因此，我们可以给参数加 1.0 来完全消除它，参数将是 *a* = 3.0。这种情况下的预测输出为 *Y* <sub>*预测*</sub> = 3.0(2) = 6。误差将为 66 = 0。误差现在为 0，当 *a* = 3.0 时，我们达到了最佳结果。

让我们对表 2-1 中的示例进行更改，除了使用新的示例之外，还将输出的输出从 6 更改为 6.5。基于等式 2-3，其中 *a* = 3.0，第一个样本的预测输出为 *Y* <sub>*预测*</sub> = (3.0)2 = 6.0，第二个样本的预测输出为*Y**=(3.0)3 = 9.0。因此，总误差等于(6.0 6.5)+(9.0 9.5)= 1.0(表 2-2 )。如何减少这类错误？*

 *表 2-2

双样本回归问题

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

输入(X)

 | 

输出(Y)

 |
| --- | --- |
| Two | Six point five |
| three | Nine point five |

我们遵循的程序是改变参数的值，直到将误差减小到 0。表 2-3 显示了两个参数值的总误差。似乎 3.0 和大于等于 3.0 的值都不能消除误差。对应于 *a* = 2.5、 *a* = 3.0、 *a* = 3.5 的模型以及期望输出如图 2-3 所示。

![img/473634_1_En_2_Fig3_HTML.jpg](img/473634_1_En_2_Fig3_HTML.jpg)

图 2-3

多参数线性模型。虚线对应于 a=2.5 的模型，星号线对应于 a=3.5，实线对应于 a=3.0。

表 2-3

双样本回归问题

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"></colgroup> 
| 

参数

 | 

输出(Y)

 | 

预测

 | 

错误

 | 

总误差

 |
| --- | --- | --- | --- | --- |
| Three point five | Six point five | Seven | 7.0−6.5=1.0 | Two |
| Nine point five | Ten point five | 10.5−9.5=1.0 |
| Two point five | Six point five | Five | 5.0−6.5=−1.5 | −4.0 |
| Nine point five | Seven point five | 7.5−9.5=−2.5 |

事实是，在我们的例子中，没有使误差等于 0 的参数值。我们想要一个乘以 2 得到 6.5，乘以 3 得到 9.5 的值。不可能找到这样的值。满足第一个样本的参数值为 *a* = 3.25，而对于第二个样本，参数值为 *a* = 3.17。因此，在模型的当前形式上，达到 0 的误差是不可能的。由于这个原因，偏差在解决这种情况中起着重要的作用。

我们可以像等式 2-4 一样，在等式 2-3 中添加一个偏差 ***b*** 。这种偏见能够解决我们的问题。

*Y* = *aX* + *b* (等式 2-4)

但是问题的复杂性现在增加了。我们试图找到两个参数(a，b)的值。基于之前的结果，当 *a* = 3.0 时，两个样本的预测输出分别为 6.0 和 9.0。预测输出比正确输出小 0.5。因此， *b* = 0.5 的值就是我们要找的。因此， *a* = 3.0 和 *b* = 0.5 将给出 0 的误差。这就是偏见如此重要的原因。

偏差允许我们在 y 轴上自由移动线性模型，同时增加拟合数据的可能性，而不仅仅是在 x 轴上移动它。请注意，它在我们的示例中非常有用，因为参数较少。当模型参数较多时，偏差可以忽略。

扩展表 2-2 中的示例，有一个新的输入 *Z* 添加到问题中，新数据在表 2-4 中。因为有两个输入和一个输出，等式 2-4 中的先前模型将不起作用，我们必须添加新的输入及其相关参数。等式 2-5 代表了新的模型。

表 2-4

双输入单输出回归问题

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

输入(X)

 | 

输入(Z)

 | 

输出(Y)

 |
| --- | --- | --- |
| Two | One point one | Six point five |
| three | Zero point eight | Nine point five |

*Y*=*aX*+*cZ*+*b*(等式 2-5)

现在，除了偏差 *b* 之外，我们还必须找到两个参数 *a* 和 *c* 的最佳值。之前使用的相同程序将应用于这个问题，以找到这些变量的最佳值。

通过创建简单的线性模型，我们已经成功地了解了人工神经网络的组成部分是如何工作的。人工神经网络由多个这样的线性模型组成，这些模型连接在一起以适应一个问题。以下部分将解释如何通过将线性模型连接在一起来设计网络。下一小节讨论如何为之前创建的模型绘制 ANN。

### 图形人工神经网络

人工神经网络是通过将多个线性模型连接在一起而构建的。随着每个模型中所需参数数量的增加，网络的完整方程变得过于复杂。因此，很难将问题表示为方程，但更简单的方法是将网络可视化为图形。网络图更容易理解和设计。在这里，我们将学习如何构建网络图，从线性模型开始。

ANN 是生物神经网络的人工表示。我们可以这样开始:人工神经网络的基本构件是人工神经元。在本章前面，我们也说过人工神经网络的基础是线性模型。因此，我们可以推断，神经元实际上是一个线性模型。与线性模型一样，神经元接受输入，进行一些处理，如乘法和加法，最后返回输出。图 2-4 显示了等式 2-4 中的线性模型和人工神经元之间的映射。注意，在线性模型中存在的所有变量也存在于 ANN 图中。这种人工神经网络被称为单层感知器。

![img/473634_1_En_2_Fig4_HTML.jpg](img/473634_1_En_2_Fig4_HTML.jpg)

图 2-4

从单输入线性模型到人工神经网络图的映射

我们可以从图形的核心开始，也就是带有文字“Math”的圆圈。这个圆圈代表神经网络的神经元。神经元是一个计算单元，它进行的计算类型是将每个输入乘以其相应的参数，将所有结果相加，然后返回表示乘积和(SOP)的输出。由于这个原因，输入 ***X*** 被连接到那个神经元。

因为每个参数必须与其输入相关联以计算 SOP，所以用于输入 ***X*** 的参数**写在将其连接到神经元的箭头上方。使每个参数靠近其输入有助于找到与每个输入相关联的参数。这是针对输入及其参数的。基于当前的例子，这个想法可能不清楚，因为只有一个输入，但是稍后会更清楚。让我们转向偏见。**

 **在神经元之后使用新的块来将偏置 ***b*** 添加到 SOP。SOP 加上*bT7 后，产生输出 ***Y*** 。到目前为止，一切都很好，但我们仍然可以使图形更简单。*

在之前的讨论中，我们将偏差与输入区别对待。每个参数都乘以其输入，但偏差没有要乘以的输入。我们可以假设偏置有一个输入始终等于 ***+*** **1** 。这大大简化了过程，因为我们可以消除神经元后添加的偏置模块，如图 2-5 所示。神经元将每个参数乘以其相关的输入，并同样对待偏差。它将被视为输入为 ***+*** **1** 的参数。为了使偏差不同于常规参数，可以垂直添加偏差，同时在图表中水平添加其他参数。

![img/473634_1_En_2_Fig5_HTML.jpg](img/473634_1_En_2_Fig5_HTML.jpg)

图 2-5

从具有一个输入的线性模型映射到具有偏差的 ANN 图，通过将偏差与+1 的输入相关联，将偏差视为常规参数

根据前面的例子，我们知道如何从神经网络的角度绘制线性方程。现在，我们可以使用方程 2-5，其中有两个输入。唯一的变化是将新的输入 ***、Z*** 及其相关参数 ***、c*** 添加到图形中，类似于我们对输入 ***X*** 及其参数 ***a*** 所做的操作。新图如图 2-6 所示。对于每个新的输入，该过程重复进行。

![img/473634_1_En_2_Fig6_HTML.jpg](img/473634_1_En_2_Fig6_HTML.jpg)

图 2-6

从具有两个输入的线性模型到 ANN 图的映射

简而言之，人工神经网络中的神经元接受一组输入，将每个输入乘以相关参数，将乘法结果相加，最后返回输出。在人工神经网络中，神经元排列成三种类型的层:输入层、隐藏层和输出层。这种安排在生物神经网络中并不存在，但它有助于我们组织网络。图 2-7 显示了具有这三层的一般全连接(FC)人工神经网络的架构。该网络按照三层来组织。网络只有一个输入和输出层，但它可以有多个隐藏层。注意，每一层内的神经元都是根据它来命名的。也就是说，输入层内的神经元称为输入神经元，而隐藏神经元是隐藏层内的神经元。

![img/473634_1_En_2_Fig7_HTML.jpg](img/473634_1_En_2_Fig7_HTML.jpg)

图 2-7

*通用 FC 人工网络架构*

为简单起见，所有输入被赋予符号 ***X*** ，所有输出被赋予符号 ***O*** ，带有定义输入或输出的索引的下标。网络有 ***n 个*** 输入，其中***X***<sub>**1**</sub>是第一个输入，***X***<sub>**5**</sub>是第五个输入，依此类推，直到***X***<sub>***n***它还有 m 个**输入，其中 ***O 个*** <sub>**1 个**</sub> 是第一个输入， ***O 个*** <sub>**5 个**</sub> 是第五个输入，依此类推直到 ***O 个*** <sub>***m 个***</sub>**</sub>

 ****隐藏层中的神经元被赋予具有两个指数的符号，以反映其层指数以及在其层中的位置。例如，第一个隐藏层有 ***k*** 个神经元，其中![$$ {h}_{\mathbf{1}}^{\mathbf{1}} $$](img/473634_1_En_2_Chapter_TeX_IEq1.png)是第一个隐藏层的第一个隐藏神经元，![$$ {h}_{\mathbf{5}}^{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_IEq2.png)是第二个隐藏层的第五个隐藏神经元，以此类推，直到![$$ {X}_p^r $$](img/473634_1_En_2_Chapter_TeX_IEq3.png) **，**是第 *r* <sup>th</sup> 隐藏层的第 *p* <sup>th</sup> 个隐藏神经元。

在每两层之间，有许多参数等于两层内神经元数量的乘积。例如，如果输入层有 ***n*** 个神经元，第一个隐层有 ***k*** 个神经元，那么连接它们所需的参数个数等于 ***n × k*** **，**其中参数![$$ {W}_{nk}¹ $$](img/473634_1_En_2_Chapter_TeX_IEq4.png)是指输入层中第 *n* <sup>个</sup>神经元和第 *k* <sup>个</sup>神经元之间的参数这个参数也可以称为权重，因为每个参数反映了其相关输入的重要性。参数值越大，其相关输入就越重要。

到目前为止，预计对 ANN 有一个基本的了解，但还需要了解更多。接下来的几节涵盖了一些关于人工神经网络的重要概念，这些概念对于人工神经网络的成功构建至关重要。

## 调整训练人工神经网络的学习率

新手学习 ann 的一个障碍是学习速度。我曾多次被问到学习速度对人工神经网络训练的影响。我们为什么要用学习率？学习率的最佳值是多少？在这一节中，我将用一个例子来说明学习率对于训练一个人工神经网络是多么有用，从而使事情变得简单。让我们从解释使用的例子开始。

### 过滤器示例

一个非常简单的例子可以让我们摆脱复杂性，专注于我们的目标，即学习率。这个例子用等式 2-6 表示。

![$$ Y= activation(X)=\Big\{{\displaystyle \begin{array}{c}\mathbf{250},\kern1.78em X\ge \mathbf{250}\\ {}X,\kern3em X&lt;\mathbf{250}\end{array}} $$](img/473634_1_En_2_Chapter_TeX_IEq5.png)(方程式 2-6)

如果输入等于或小于 250，那么输出将与输入相同。如果输入大于 250，那么它将被剪裁，输出将是 250。它的工作原理就像一个过滤器，只让 250 以下的输入通过，而将其他输入截止到 250。其图形如图 2-8 所示。

![img/473634_1_En_2_Fig8_HTML.jpg](img/473634_1_En_2_Fig8_HTML.jpg)

图 2-8

过滤器示例的激活功能

六个样本的数据如表 2-5 所示。

表 2-5

用于训练网络的数据，以过滤输入，了解学习率如何影响训练过程

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

输入(X)

 | 

输出(Y)

 |
| --- | --- |
| Sixty | Sixty |
| Forty | Forty |
| four hundred | Two hundred and fifty |
| Three hundred | Two hundred and fifty |
| -50 | -50 |
| -10 | -10 |

#### 人工神经网络架构

所用人工神经网络的架构如图 2-9 所示。只有输入层和输出层。输入层只有一个神经元用于我们的单一输入。输出层只有一个神经元来产生输出。输出层神经元负责将输入映射到正确的输出。还有一个偏置施加到输出层神经元，值为 ***b*** ，输入为 ***+1*** 。还有一个用于输入的权重 ***W*** 。

![img/473634_1_En_2_Fig9_HTML.jpg](img/473634_1_En_2_Fig9_HTML.jpg)

图 2-9

与过滤器示例一起使用的 ANN 架构

#### 激活功能

基于前面讨论的网络，我们只能近似线性函数，如图 2-1 。但是我们的问题使用了一个非线性函数，如图 2-8 所示。我们如何使用人工神经网络来表示这种类型的网络？本例中的解决方案是使用一个函数，如 ANN 中的激活函数。

人工神经网络可以逼近线性和非线性函数。人工神经网络将非线性纳入其计算的方式是通过激活函数。激活函数在 ANN 图中的位置是在 SOP 计算之后。在这种情况下，神经元的输出将是激活函数输出，而不仅仅是 SOP。这就是为什么在等式 2-6 中网络输出被设置为等于激活函数输出。

#### Python 实现

清单 2-1 中给出了实现整个网络的 Python 代码。在讨论了它的每个部分并使其尽可能简单之后，我们将集中讨论改变学习率如何影响网络训练。

```py
 1  import numpy
 2
 3  def activation_function(inpt):
 4      if(inpt > 250):
 5          return 250 # clip the result to 250
 6      else:
 7          return inpt # just return the input
 8
 9  def prediction_error(desired, expected):
10      return numpy.abs(numpy.mean(desired-expected)) # absolute error
11
12  def update_weights(weights, predicted, idx):
13      weights = weights + 0.00001*(desired_output[idx] - predicted)*inputs[idx] # updating weights
14      return weights # new updated weights
15
16  weights = numpy.array([0.05, .1]) #bias & weight of input
17  inputs = numpy.array([60, 40, 100, 300, -50, 310]) # training inputs
18  desired_output = numpy.array([60, 40, 150, 250, -50, 250]) # training outputs
19
20  def training_loop(inpt, weights):
21      error = 1
22      idx = 0 # start by the first training sample
23      iteration = 0 #loop iteration variable
24      while(iteration < 2000 or error >= 0.01): #while(error >= 0.1):
25          predicted = activation_function(weights[0]*1+weights[1]*inputs[idx])
26          error = prediction_error(desired_output[idx], predicted)
27          weights = update_weights(weights, predicted, idx)
28          idx = idx + 1 # go to the next sample
29          idx = idx % inputs.shape[0] # restricts the index to the range of our samples
30          iteration = iteration + 1 # next iteration
31      return error, weights
32
33  error, new_weights = training_loop(inputs, weights)
34  print('--------------Final Results----------------')
35  print('Learned Weights : ', new_weights)
36  new_inputs = numpy.array([10, 240, 550, -160])
37  new_outputs = numpy.array([10, 240, 250, -160])
38  for i in range(new_inputs.shape[0]):
39      print('Sample ', i+1, '. Expected = ', new_outputs[i], ' , Predicted = ', activation_function(new_weights[0]*1+new_weights[1]*new_inputs[i]))

Listing 2-1Adjusting Learning Rate for Successful ANN Training

```

第 17 行和第 18 行负责创建两个数组(inputs 和 desired_output ),用于保存我们示例中的训练输入和输出数据。第 16 行创建了一个网络参数数组，它们是输入参数和偏差。它们被随机初始化为 0.05 的偏置和 0.1 的输入。第 3 行到第 7 行使用 activation_function(inpt)方法实现激活函数本身。它接受作为输入的单个参数，并返回作为网络预测输出的单个值。

因为预测可能会有误差，所以我们需要测量一下，才能知道我们离正确的预测有多远。因此，在第 9 行和第 10 行中实现了一个名为 prediction_error(desired，expected)的方法，它接受两个输入:期望的和预测的输出。该方法只是计算每个期望输出和预测输出之间的绝对差值。任何误差的最佳值肯定是 0。这是最佳值。

如果出现预测误差怎么办？在这种情况下，我们必须对网络进行更改。但是到底要改变什么呢？必须改变的是网络参数。为了更新网络参数，在第 13 和 14 行中定义了一个名为 update_weights(weights，predicted，idx)的方法。它接受三个输入:旧权重、预测输出和具有错误预测的输入的索引。等式 2-7 用于更新权重。

![$$ W\left(n+1\right)=W(n)+\eta \left[d(n)-Y(n)\right]X(n) $$](img/473634_1_En_2_Chapter_TeX_IEq6.png)(方程式 2-7)

在哪里

*   *η*–学习率

*   *d*–期望输出

*   *Y*–预测输出

*   *X*–输入

*   *W*(*n*)–当前重量

*   *W*(*n*+1)——更新权重

该等式使用当前步骤 ***n*** 的权重来生成下一步骤的权重( ***n +*** **1** )。这个等式有助于我们理解学习速度是如何影响学习过程的。

最后，我们需要将所有这些连接在一起，使网络能够学习。这是使用从第 20 行到第 31 行定义的 training_loop(inpt，weights)方法完成的。它进入一个训练循环。该循环用于以最小的可能预测误差将输入映射到它们的输出。该循环执行三项操作:

1.  产量预测。

2.  错误计算。

3.  更新权重。

既然我们已经对这个例子和它的 Python 代码有了一个概念，现在让我们来看看学习率对于获得最佳结果是如何有用的。

### 学习率

在前面讨论的清单 2-1 的例子中，第 13 行有权重更新等式，其中使用了学习率。让我们从等式中去掉学习率。具体如下:

```py
    weights = weights + (desired_output[idx] - predicted)*inputs[idx]

```

我们来看看去掉学习率的效果。在训练循环的第一次迭代中，网络的偏差和权重的初始值分别为 0.05 和 0.1。输入是 60，期望输出是 60。第 25 行的预期输出，即激活函数的结果，将是 activation _ function(0.05(+1)+0.1(60))。预测产量为 be 6.05。在第 26 行，通过得到期望输出和预测输出之间的差来计算预测误差。误差为 ABS(60 6.05)= 53.95。然后在第 27 行，权重将根据前面的等式进行更新。新的权重是[0.05，0.1] + (53.95)*60 = [0.05，0.1] + 3237 = [3237.05，3237.1]。似乎新的权重与以前的权重相差太大。每个重量增加了 3，237，这太大了。但是让我们继续做下一个预测。

在下一次迭代中，网络将拥有这些数据(b=3237.05，W=3237.1，输入=40，期望输出= 40)。预期输出将是 activation _ function((3237.05+3237.1(40))= 250。预测误差将为 ABS(40250)= 210。误差非常大。这个误差比之前的 53.95 要大。因此，我们必须再次更新权重。根据上式，新权重为[3237.05，3237.1]+(210)* 40 =[3237.05，3237.1]+8400 =[5162.95，5162.9]。表 2-6 总结了前三次迭代的结果。

表 2-6

训练滤波器网络的前三次迭代的结果

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

预报

 | 

错误

 | 

更新值

 | 

新重量

 |
| --- | --- | --- | --- |
| Six point zero five | Fifty-three point nine five | **3237.0** | [3237.05, 3237.1] |
| Two hundred and fifty | Two hundred and ten | **—8400** | [–5162.95, –5162.9 ] |
| −521452.95 | Five hundred and twenty-one thousand five hundred and fifty-two point nine five | **52155295.0** | [52150132.04999999, 52150132.09999999] |
| −2555356472.95 | Two billion five hundred and fifty-five million three hundred and fifty-six thousand four hundred and twenty-two point nine five | **—127767821147.0** | [–1.27715671 e+11，–1.27715671 e+11] |

随着我们进行更多的迭代，结果会变得更糟。权重的大小变化很快，有时甚至改变符号。它们从非常大的正值变为非常大的负值。我们怎样才能阻止这些巨大而突然的重量变化呢？如何缩小权重更新的值？

如果我们从表 2-6 中查看重量变化的值，该值似乎非常大。这意味着网络高速改变其权重。我们只需要让它慢下来。如果我们能够降低这个值，那么一切都会好的。但是怎么做呢？回到代码，看起来更新等式是生成如此大的值的原因，特别是这一部分:

```py
(desired_output[idx] - predicted)*inputs[idx]

```

我们可以通过将其乘以一个小值(如 0.1)来缩放该部分。因此，在第一次迭代中，不是生成 3237.0 作为更新值，而是减少到 323.7。我们甚至可以将这个值降低到 0.001。使用 0.001，更新值仅为 3.327。

我们现在可以抓住它了。这个值就是学习率。为学习率选择小的值使得权重更新的速率更小，并且避免突然的变化。值越大，变化越快，这会产生不好的结果。

**但是对于学习率来说什么是最好的** **值** **？**

对于学习率来说，没有一个特定的值可以说是最佳值。学习率是一个超参数。超参数的值由实验确定。我们尝试不同的值，并使用给出最佳结果的值。

### 测试网络

对于我们的问题，使用值. 00001 就可以了。用那个学习率训练完网络，就可以做个测试了。表 2-7 显示了四个新测试样本的预测结果。使用学习率后，现在的结果似乎好了很多。

表 2-7

测试样本预测结果

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

投入

 | 

输出量的希望值

 | 

预测产量

 |
| --- | --- | --- |
| Ten | Ten | Ten point eight seven |
| Two hundred and forty | Two hundred and forty | Two hundred and thirty-nine point one three |
| Five hundred and fifty | Two hundred and fifty | Two hundred and fifty |
| –160 | –160 | –157.85 |

现在我们能够理解学习速度决定了我们前进的步伐。步长越大，变化越突然。我们可能接近最佳解，只需要稍微改变我们的参数来达到它，但是忽略或使用学习率的坏值会使我们远离解。

## 使用反向传播的权重优化

在上一节中，我们使用学习率来更新人工神经网络的权重。在本节中，我们将使用反向传播算法来完成这项工作，并推导出它如何优于仅使用学习率。用两个例子对算法进行了数值说明。

本节不会直接深入反向传播算法的细节，而是从训练一个非常简单的网络开始。这是因为反向传播算法意味着在训练后应用于网络。因此，我们应该在应用它之前训练网络，以获得反向传播算法的好处以及如何使用它。读者应该对人工神经网络的工作原理、偏导数和多元链式法则有一个基本的了解。

### 无隐层神经网络的反向传播

从一个简单的例子开始，图 2-10 显示了它的网络结构，我们将用它来解释反向传播算法是如何工作的。它只有两个输入，符号化为***X***<sub>**1**</sub>和***X***<sub>**2**</sub>。输出层只有一个神经元，没有隐藏层。每个输入都有相应的权重其中***W***<sub>**1**</sub>和***W***<sub>**2**</sub>是权重为**<sub>**1**</sub>和***X***<sub>**2**输出层神经元有一个偏置，值为 ***b*** ，固定输入值为 ***+*** **1** 。</sub>**

 **![img/473634_1_En_2_Fig10_HTML.jpg](img/473634_1_En_2_Fig10_HTML.jpg)

图 2-10

训练和应用反向传播的网络结构

输出层神经元使用由等式 2-8 定义的 sigmoid 激活函数:

![$$ f(s)=\frac{\mathbf{1}}{\mathbf{1}+{e}^{-s}} $$](img/473634_1_En_2_Chapter_TeX_IEq7.png)(方程式 2-8)

其中 ***s*** 为每个输入与其对应权重之间的 SOP。 ***s*** 是激活函数的输入，在本例中，如等式 2-9 所定义。

**s**=********【w】********

 ****表 2-8 显示了用作训练数据的单个输入及其相应的期望输出。这个例子的基本目标不是训练网络，而是理解如何使用反向传播来更新权重。现在，为了集中于反向传播，我们将分析单个数据记录。

表 2-8

第一反向传播示例的训练数据

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

*X*?? 1

 | 

*X*2

 | 

*期望输出*

 |
| --- | --- | --- |
| **0.1** | **0.3** | **0.03** |

假设权重和偏差的初始值如表 2-9 所示。

表 2-9

网络的初始参数

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

*W*1

 | 

*W*2

 | 

*b*

 |
| --- | --- | --- |
| **0.5** | **0.2** | **1.83** |

为简单起见，所有输入、权重和偏差的值将被添加到网络图中，如图 2-11 所示。

![img/473634_1_En_2_Fig11_HTML.jpg](img/473634_1_En_2_Fig11_HTML.jpg)

图 2-11

添加了输入和参数的第一反向传播示例的网络

现在，让我们训练网络，看看是否会根据当前的权重和偏差返回所需的输出。激活函数的输入将是每个输入与其权重之间的 SOP。然后，偏差将被添加到总数中，如下所示:

![$$ s={X}_{\mathbf{1}}\ast {W}_{\mathbf{1}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{2}}+b $$](img/473634_1_En_2_Chapter_TeX_Equa.png)

![$$ s=\mathbf{0.1}\ast \mathbf{0.5}+\mathbf{0.3}\ast \mathbf{0.2}+\mathbf{1.83} $$](img/473634_1_En_2_Chapter_TeX_Equb.png)

![$$ s=\mathbf{1.94} $$](img/473634_1_En_2_Chapter_TeX_Equc.png)

激活函数的输出将通过将先前计算的 SOP 应用于所用函数(sigmoid)来计算，如下所示:

![$$ f(s)=\frac{\mathbf{1}}{\mathbf{1}+{e}^{-s}} $$](img/473634_1_En_2_Chapter_TeX_Equd.png)

![$$ f(s)=\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{1.94}}} $$](img/473634_1_En_2_Chapter_TeX_Eque.png)

![$$ f(s)=\frac{\mathbf{1}}{\mathbf{1}+\mathbf{0.143703949}} $$](img/473634_1_En_2_Chapter_TeX_Equf.png)

![$$ f(s)=\frac{\mathbf{1}}{\mathbf{1}.\mathbf{143703949}} $$](img/473634_1_En_2_Chapter_TeX_Equg.png)

![$$ f(s)=\mathbf{0.874352143} $$](img/473634_1_En_2_Chapter_TeX_Equh.png)

激活函数的输出反映了当前输入的预测输出。很明显，期望输出和期望输出之间存在差异。但是这种差异的来源是什么呢？应该如何改变预测的输出以更接近期望的结果？这些问题后面会回答。但至少，让我们看到我们的神经网络基于一个误差函数的误差。

误差函数表明预测输出与期望输出的接近程度。误差的最佳值为零，这意味着根本没有误差，并且期望的和预测的结果是相同的。误差函数之一是平方误差函数，如等式 2-10 所示。

![$$ E=\frac{\mathbf{1}}{\mathbf{2}}{\left( desired- predicted\right)}^{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_IEq8.png)(方程式 2-10)

注意，加在方程上的![$$ \frac{\mathbf{1}}{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_IEq9.png)是为了以后简化导数。我们可以按如下方式测量网络误差:

![$$ E=\frac{\mathbf{1}}{\mathbf{2}}{\left(\mathbf{0.03}-\mathbf{0.874352143}\right)}^{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equi.png)

![$$ E=\frac{\mathbf{1}}{\mathbf{2}}{\left(-\mathbf{0.844352143}\right)}^{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equj.png)

![$$ E=\frac{\mathbf{1}}{\mathbf{2}}\left(\mathbf{0.712930542}\right) $$](img/473634_1_En_2_Chapter_TeX_Equk.png)

![$$ E=\mathbf{0.356465271} $$](img/473634_1_En_2_Chapter_TeX_Equl.png)

结果保证了大误差的存在( **~0.357** )。这是错误所告诉我们的。它只是给了我们一个指示，告诉我们预测的结果离期望的结果有多远。既然我们知道如何测量误差，我们需要找到一种方法来最小化它。我们唯一能玩的参数是重量。我们可以尝试不同的权重，然后测试我们的网络。

### 权重更新方程

权重可以根据等式 2-7(用于上一节)来改变，其中

*   ***n*** :训练步骤(0，1，2，…)。

*   ***W***(***n***):当前训练步的重量。

*   **【w】**(**=***【b】*****【n】-我...。，***【w】***<sub>**【m】**</sub>**)]******

*****   ***η*** :网络学习率。

    *   ***d***(***n***):期望输出。

    *   ***Y***(***n***):预测产量。

    *   ***X***(***n***):网络做出错误预测的当前输入。**** 

 ****对于我们的网络，这些参数具有以下值:

*   ***n*** : 0

*   ***W***(***n***):【1.83，0.5，0.2】

*   ***η*** :超参数。比如我们可以选择 0.01。

*   ***d***(***n***):【0.03】。

*   ***Y***(***n***):【0.874352143】。

*   ***X***(***n***):[+1，0.1，0.3]。第一个值(+1)是偏差。

我们可以基于前面的等式更新我们的神经网络权重:

![$$ W\left(n+\mathbf{1}\right)=W(n)+\eta \left[d(n)-Y(n)\right]X(n) $$](img/473634_1_En_2_Chapter_TeX_Equm.png)

![$$ =\left[1.83,0.5,0.2\right]+0.01\left[0.03-0.874352143\right]\left[+1,0.1,0.3\right] $$](img/473634_1_En_2_Chapter_TeX_Equn.png)

![$$ =\left[\mathbf{1.83},\mathbf{0.5},\mathbf{0.2}\right]+0.01\left[-\mathbf{0.844352143}\right]\left[+\mathbf{1},\mathbf{0.1},\mathbf{0.3}\right] $$](img/473634_1_En_2_Chapter_TeX_Equo.png)

![$$ =\left[\mathbf{1.83},\mathbf{0.5},\mathbf{0.2}\right]+-\mathbf{0.00844352143}\left[+\mathbf{1},\mathbf{0.1},\mathbf{0.3}\right] $$](img/473634_1_En_2_Chapter_TeX_Equp.png)

![$$ =\left[\mathbf{1.83},\mathbf{0.5},\mathbf{0.2}\right]+\left[-\mathbf{0.008443521},-\mathbf{0.000844352},-\mathbf{0.002533056}\right] $$](img/473634_1_En_2_Chapter_TeX_Equq.png)

![$$ =\left[\mathbf{1.821556479},\mathbf{0.499155648},\mathbf{0.197466943}\right] $$](img/473634_1_En_2_Chapter_TeX_Equr.png)

新的重量在表 2-10 中给出。

表 2-10

第一反向传播示例的网络的更新权重

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

*W* <sub>1 ***新***</sub>

 | 

*W* <sub>2 ***新***</sub>

 | 

*b* <sub>***新***</sub>

 |
| --- | --- | --- |
| **0.197466943** | **0.499155648** | **1.821556479** |

基于新的权重，我们将重新计算预测输出，并继续更新权重和计算预测输出，直到达到手头问题的可接受误差值。

这里，我们成功地更新了权重，而没有使用反向传播算法。我们还需要那个算法吗？是的。接下来将解释原因。

### 为什么反向传播算法很重要？

对于最佳情况，假设权重更新方程产生最佳权重；现在还不清楚这个函数实际上做了什么。它就像一个黑匣子，因为我们不了解它的内部运作。我们只知道，万一出现分类错误，我们应该应用这个等式。然后，该函数将生成新的权重，用于接下来的训练步骤。但是为什么新的权重更擅长预测呢？每个权重对预测误差有什么影响？增加或减少一个或多个权重如何影响预测误差？

需要更好地理解如何计算最佳权重。为此，我们应该使用反向传播算法。它帮助我们理解每个权重如何影响 NN 总误差，并告诉我们如何将误差最小化到非常接近零的值。

### 向前传球和向后传球

在训练一个神经网络时，有前向和后向两个通道，如图 2-12 。第一遍永远是正向传递，输入应用到输入层，向输出层移动，计算输入和权重之间的 SOP，应用激活函数生成输出，最后计算预测误差，就知道当前网络的精度有多高。

![img/473634_1_En_2_Fig12_HTML.jpg](img/473634_1_En_2_Fig12_HTML.jpg)

图 2-12

神经网络训练的前向和后向途径

但是如果有预测误差呢？我们应该修改网络以减少错误。这是在反向传递中完成的。在前向传递中，我们从输入开始，直到计算预测误差。但是在反向传递中，我们从错误开始，直到到达输入。这一步的目标是了解每个权重如何影响总误差。知道权重和误差之间的关系允许我们修改网络权重以减小误差。比如在后向传递中，我们可以得到有用的信息，比如将***W***<sub>**1**</sub>的当前值增加 1.0，预测误差就会增加 0.07。这有助于我们理解如何选择***【W】***<sub>**1**</sub>的新值，以使误差最小化(***W***<sub>**1**</sub>不应增加)。

#### 偏导数

向后传递中使用的一个重要操作是计算导数。在开始计算反向传递中的导数之前，我们可以从一个简单的例子开始，让事情变得简单一些。

对于一个多元函数，如*Y*=*X*<sup>2</sup>*Z*+*H*，给定变量 X 的变化对输出 Y 有什么影响？这个问题用偏导数来回答。它是这样写的:

![$$ \frac{\partial Y}{\partial X}=\frac{\partial }{\partial X}\left({X}^{\mathbf{2}}Z+H\right) $$](img/473634_1_En_2_Chapter_TeX_Equs.png)

![$$ \frac{\partial Y}{\partial X}=\mathbf{2} XZ+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equt.png)

![$$ \frac{\partial Y}{\partial X}=\mathbf{2} XZ $$](img/473634_1_En_2_Chapter_TeX_Equu.png)

注意，除了 *X* 之外的一切都被视为常数。这就是为什么 *H* 在计算偏导数后被替换为 0。这里， *∂X* 表示变量 *X* 的微小变化， *∂Y* 表示 *Y* 的微小变化。 *Y* 的变化是 *X* 变化的结果。通过对 *X* 做一个很小的改动，对 *Y* 有什么影响？微小的变化可以是增加或减少一个微小的值，例如 0.01。通过代入不同的 *X* 值，我们可以发现 *Y* 相对于 *X* 如何变化。

将遵循相同的程序，以便了解 NN 预测误差如何相对于网络权重的(wrt)变化而变化。所以，我们的目标是计算![$$ \frac{\partial E}{\partial {W}_{\mathbf{1}}} $$](img/473634_1_En_2_Chapter_TeX_IEq10.png)和![$$ \frac{\partial E}{\partial {W}_{\mathbf{2}}} $$](img/473634_1_En_2_Chapter_TeX_IEq11.png)，因为我们只有两个权重:***W***<sub>**1**</sub>和***W***<sub>**2**</sub>。我们来计算一下。

##### 预测误差权重的变化

看这个等式，***Y = X***<sup>**2**</sup>***Z+H***，计算偏导数![$$ \frac{\partial Y}{\partial X} $$](img/473634_1_En_2_Chapter_TeX_IEq12.png)似乎很简单，因为有一个等式同时关联 *Y* 和 *X* 。但是在预测误差和权重之间没有直接的等式。这就是为什么我们要用多元链式法则来求 *Y* wrt *X* 的偏导数。

##### 重量链的预测误差

让我们试着找出预测误差与权重之间的联系。预测误差是根据等式 2-10 计算的。

但是这个方程没有任何权重。没问题:我们可以按照前一个方程的每个输入进行计算，直到我们得到权重。期望的输出是一个常数，因此不可能通过它达到权重。预测输出基于 sigmoid 函数计算，如等式 2-8 所示。

同样，计算预测产量的等式没有任何权重。但是仍然有变量 ***s*** (SOP)，根据等式 2-11，其计算已经依赖于权重。

**s**=********【w】********

 ****图 2-13 显示了计算重量时应遵循的计算链。

![img/473634_1_En_2_Fig13_HTML.jpg](img/473634_1_En_2_Fig13_HTML.jpg)

图 2-13

从预测误差开始计算权重的计算链

因此，要知道预测误差如何随权重的变化而变化，我们应该做一些中间运算，包括找出预测误差如何随预测输出的变化而变化。然后，我们需要找到预测产量和 SOP 之间的关系。最后，我们将通过改变权重来发现 SOP 是如何变化的。有如下四个中间偏导数:

![$$ \frac{\partial E}{\partial Predicted} $$](img/473634_1_En_2_Chapter_TeX_IEq13.png)、![$$ \frac{\partial Predicted}{\partial s} $$](img/473634_1_En_2_Chapter_TeX_IEq14.png)、![$$ \frac{\partial s}{\partial {W}_{\mathbf{1}}} $$](img/473634_1_En_2_Chapter_TeX_IEq15.png)和![$$ \frac{\partial s}{\partial {W}_{\mathbf{2}}} $$](img/473634_1_En_2_Chapter_TeX_IEq16.png)

这个链将最终告诉预测误差如何随着每个权重的变化而变化，这是我们的目标，通过将所有单独的偏导数相乘，如下所示:

![$$ \frac{\partial E}{\partial {W}_{\mathbf{1}}}=\frac{\partial E}{\partial Predicted}\ast \frac{\partial Predicted}{\partial s}\ast \frac{\partial s}{\partial {W}_{\mathbf{1}}} $$](img/473634_1_En_2_Chapter_TeX_Equv.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{2}}}=\frac{\partial E}{\partial Predicted}\ast \frac{\partial Predicted}{\partial s}\ast \frac{\partial s}{\partial {W}_{\mathbf{2}}} $$](img/473634_1_En_2_Chapter_TeX_Equw.png)

### 重要说明

目前，还没有将预测误差与网络权重直接联系起来的等式，但是我们可以创建一个将它们联系起来的等式，并对其直接应用偏导数。这是等式 2-12。

![$$ E=\frac{\mathbf{1}}{\mathbf{2}}{\left( desired-\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\left({X}_{\mathbf{1}}\ast {W}_{\mathbf{1}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{2}}+b\right)}}\right)}^{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_IEq17.png)(方程式 2-12)

因为这个方程看起来很复杂，为了简单起见，我们可以使用多元链式法则。

#### 计算链偏导数

让我们计算之前创建的链的每个部分的偏导数。

**误差预测输出偏导数:**

![$$ \frac{\partial E}{\partial Predicted}=\frac{\partial }{\partial Predicted}\left(\frac{\mathbf{1}}{\mathbf{2}}{\left( desired- predicted\right)}^{\mathbf{2}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equx.png)

![$$ =\mathbf{2}\ast \frac{\mathbf{1}}{\mathbf{2}}{\left( desired- predicted\right)}^{\mathbf{2}-\mathbf{1}}\ast \left(\mathbf{0}-\mathbf{1}\right) $$](img/473634_1_En_2_Chapter_TeX_Equy.png)

![$$ =\left( desired- predicted\right)\ast \left(-\mathbf{1}\right) $$](img/473634_1_En_2_Chapter_TeX_Equz.png)

![$$ = predicted- desired $$](img/473634_1_En_2_Chapter_TeX_Equaa.png)

通过值替换，

![$$ \frac{\partial E}{\partial Predicted}= predicted- desired=\mathbf{0.874352143}-\mathbf{0.03} $$](img/473634_1_En_2_Chapter_TeX_Equab.png)

![$$ \frac{\partial E}{\partial Predicted}=\mathbf{0.844352143} $$](img/473634_1_En_2_Chapter_TeX_Equac.png)

**预测产量- SOP 偏导数:**

![$$ \frac{\partial Predicted}{\partial s}=\frac{\partial }{\partial s}\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{-s}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equad.png)

请记住，商法则可用于计算 sigmoid 函数的导数，如下所示:

![$$ \frac{\partial Predicted}{\partial s}=\frac{\mathbf{1}}{\mathbf{1}+{e}^{-s}}\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+{e}^{-s}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equae.png)

通过值替换，

![$$ \frac{\partial Predicted}{\partial s}=\frac{\mathbf{1}}{\mathbf{1}+{e}^{-s}}\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+{e}^{-s}}\right)=\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{1.94}}}\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{1.94}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equaf.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}+\mathbf{0.143703949}}\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+\mathbf{0.143703949}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equag.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}.\mathbf{143703949}}\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}.\mathbf{143703949}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equah.png)

![$$ =\mathbf{0.874352143}\left(\mathbf{1}-\mathbf{0.874352143}\right) $$](img/473634_1_En_2_Chapter_TeX_Equai.png)

![$$ =\mathbf{0.874352143}\left(\mathbf{0.125647857}\right) $$](img/473634_1_En_2_Chapter_TeX_Equaj.png)

![$$ \frac{\partial Predicted}{\partial s}=\mathbf{0.109860473} $$](img/473634_1_En_2_Chapter_TeX_Equak.png)

**SOP-*****W***<sub>**1**</sub>**偏导数:**

![$$ \frac{\partial s}{\partial {W}_{\mathbf{1}}}=\frac{\partial }{\partial {W}_{\mathbf{1}}}\left({X}_{\mathbf{1}}\ast {W}_{\mathbf{1}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{2}}+b\right) $$](img/473634_1_En_2_Chapter_TeX_Equal.png)

![$$ =\mathbf{1}\ast {X}_{\mathbf{1}}\ast {\left({W}_{\mathbf{1}}\right)}^{\left(\mathbf{1}-\mathbf{1}\right)}+\mathbf{0}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equam.png)

![$$ ={X}_{\mathbf{1}}\ast {\left({W}_{\mathbf{1}}\right)}^{\left(\mathbf{0}\right)} $$](img/473634_1_En_2_Chapter_TeX_Equan.png)

![$$ ={X}_{\mathbf{1}}\left(\mathbf{1}\right) $$](img/473634_1_En_2_Chapter_TeX_Equao.png)

![$$ \frac{\partial s}{\partial {W}_{\mathbf{1}}}={X}_{\mathbf{1}} $$](img/473634_1_En_2_Chapter_TeX_Equap.png)

通过值替换，

![$$ \frac{\partial s}{\partial {W}_{\mathbf{1}}}={X}_{\mathbf{1}}=\mathbf{0.1} $$](img/473634_1_En_2_Chapter_TeX_Equaq.png)

**SOP-*****W***<sub>**2**</sub>**偏导数:**

![$$ \frac{\partial s}{\partial {W}_{\mathbf{2}}}=\frac{\partial }{\partial {W}_{\mathbf{2}}}\left({X}_{\mathbf{1}}\ast {W}_{\mathbf{1}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{2}}+b\right) $$](img/473634_1_En_2_Chapter_TeX_Equar.png)

![$$ =\mathbf{0}+\mathbf{1}\ast {X}_{\mathbf{2}}\ast {\left({W}_{\mathbf{2}}\right)}^{\left(\mathbf{1}-\mathbf{1}\right)}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equas.png)

![$$ ={X}_{\mathbf{2}}\ast {\left({W}_{\mathbf{2}}\right)}^{\left(\mathbf{0}\right)} $$](img/473634_1_En_2_Chapter_TeX_Equat.png)

![$$ ={X}_{\mathbf{2}}\left(\mathbf{1}\right) $$](img/473634_1_En_2_Chapter_TeX_Equau.png)

![$$ \frac{\partial s}{\partial {W}_{\mathbf{2}}}={X}_{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equav.png)

通过值替换，

![$$ \frac{\partial s}{\partial {W}_{\mathbf{2}}}={X}_{\mathbf{2}}=\mathbf{0.3} $$](img/473634_1_En_2_Chapter_TeX_Equaw.png)

在计算出每个单独的导数后，我们可以将它们相乘，从而得到预测误差和每个权重之间的关系。

**预测误差-*****W***<sub>**1**</sub>**偏导数:**

![$$ \frac{\partial E}{\partial {W}_{\mathbf{1}}}=\mathbf{0.844352143}\ast \mathbf{0.1}\mathbf{09860473}\ast \mathbf{0.1} $$](img/473634_1_En_2_Chapter_TeX_Equax.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{1}}}=\mathbf{0.009276093} $$](img/473634_1_En_2_Chapter_TeX_Equay.png)

**预测误差-*****W***<sub>**2**</sub>**偏导数:**

![$$ \frac{\partial E}{\partial {W}_{\mathbf{2}}}=\mathbf{0.844352143}\ast \mathbf{0.109860473}\ast \mathbf{0.3} $$](img/473634_1_En_2_Chapter_TeX_Equaz.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{2}}}=\mathbf{0.027828278} $$](img/473634_1_En_2_Chapter_TeX_Equba.png)

最后，有两个值反映预测误差如何相对于权重变化(对于 W <sub>1</sub> 为 0.009276093，对于 W <sub>2</sub> 为 0.027828278)。但这意味着什么呢？结果需要解释。

#### 解释反向传播的结果

从下面得到的最后两个导数中的每一个都有两个有用的结论:

*   导数符号

*   导数大小

如果导数为正，这意味着增加权重会增加误差，同样，减少权重会减少误差。如果导数是负的，那么增加权重将减少误差，相应地，减少权重将增加误差。

但是误差会增加或减少多少呢？DM 可以告诉我们。对于正导数，增加 *p* 的权重会增加*DM*∫*p*的误差。对于负导数，增加权重 *p* 将减少误差*DM*∫*p*。

因为![$$ \frac{\partial E}{\partial {W}_{\mathbf{1}}} $$](img/473634_1_En_2_Chapter_TeX_IEq18.png)求导的结果是正的，这意味着如果***W***<sub>**1**</sub>增加 1 那么总误差将增加 0.009276093。同样，因为![$$ \frac{\partial E}{\partial {W}_{\mathbf{2}}} $$](img/473634_1_En_2_Chapter_TeX_IEq19.png)导数的结果是正的，这意味着如果**<sub>**2**</sub>增加 1，那么总误差将增加 0.027828278。**

 **#### 更新权重

在成功计算出误差相对于每个单独权重的导数之后，我们可以更新权重以增强预测。每个权重将根据其导数进行更新，如下所示:

![$$ {W}_{\mathbf{1} new}={W}_{\mathbf{1}}-\eta \ast \frac{\partial E}{\partial {W}_{\mathbf{1}}} $$](img/473634_1_En_2_Chapter_TeX_Equbb.png)

![$$ =\mathbf{0.5}-0.01\ast \mathbf{0.009276093} $$](img/473634_1_En_2_Chapter_TeX_Equbc.png)

![$$ {W}_{\mathbf{1} new}=\mathbf{0.49990723907} $$](img/473634_1_En_2_Chapter_TeX_Equbd.png)

对于第二重量，

![$$ {W}_{\mathbf{2} new}={W}_{\mathbf{2}}-\eta \ast \frac{\partial E}{\partial {W}_{\mathbf{2}}} $$](img/473634_1_En_2_Chapter_TeX_Eqube.png)

![$$ =\mathbf{0.2}-0.01\ast \mathbf{0.027828278} $$](img/473634_1_En_2_Chapter_TeX_Equbf.png)

![$$ {W}_{\mathbf{2} new}=\mathbf{0.1997217172} $$](img/473634_1_En_2_Chapter_TeX_Equbg.png)

注意，导数是减去的，而不是加到重量上，因为它是正的。

然后，继续预测和更新权重的过程，直到产生具有可接受误差的期望输出。

### 隐层神经网络的反向传播

为了使思路更加清晰，我们可以在添加一个具有两个神经元的隐藏层之后，在下面的 NN 上应用反向传播算法。新网络如图 2-14 所示。

![img/473634_1_En_2_Fig14_HTML.jpg](img/473634_1_En_2_Fig14_HTML.jpg)

图 2-14

第二反向传播示例的网络架构

先前使用的相同输入、输出、激活函数和学习率也将应用于本例。以下是网络的完整权重:

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"> <col class="tcol8 align-left"> <col class="tcol9 align-left"></colgroup> 
| 

***W***<sub>1</sub>

 | 

***W***<sub>2</sub>

 | 

***W***<sub>3</sub>

 | 

***W***<sub>4</sub>

 | 

***W***<sub>5</sub>

 | 

***W***<sub>6</sub>

 | 

***b***<sub>1</sub>

 | 

***b***<sub>2</sub>

 | 

***b***<sub>3</sub>

 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **0.5** | **0.1** | **0.62** | **0.2** | ***—*****0.2** | **0.3** | **0.4** | ***—*****0.1** | **1.83** |

图 2-15 显示了添加了所有输入和权重的先前网络。

![img/473634_1_En_2_Fig15_HTML.jpg](img/473634_1_En_2_Fig15_HTML.jpg)

图 2-15

在添加输入和参数值之后的第二反向传播示例的网络架构

首先，我们应该通过正向传递来获得预测的输出。如果预测有错误，那么我们应该根据反向传播算法通过反向传递来更新权重。让我们计算隐层中第一个神经元的输入(**<sub>**1**</sub>):**

**![$$ {h}_{\mathbf{1} in}={X}_{\mathbf{1}}\ast {W}_{\mathbf{1}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{2}}+{b}_{\mathbf{1}} $$](img/473634_1_En_2_Chapter_TeX_Equbh.png)**

 **![$$ =\mathbf{0.1}\ast \mathbf{0.5}+\mathbf{0.3}\ast \mathbf{0.1}+\mathbf{0.4} $$](img/473634_1_En_2_Chapter_TeX_Equbi.png)

![$$ {h}_{\mathbf{1} in}=\mathbf{0.48} $$](img/473634_1_En_2_Chapter_TeX_Equbj.png)

对隐层中第二个神经元的输入(**<sub>**2**</sub>):**

**![$$ {h}_{\mathbf{2} in}={X}_{\mathbf{1}}\ast {W}_{\mathbf{3}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{4}}+{b}_{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equbk.png)**

 **![$$ =\mathbf{0.1}\ast \mathbf{0.62}+\mathbf{0.3}\ast \mathbf{0.2}-\mathbf{0.1} $$](img/473634_1_En_2_Chapter_TeX_Equbl.png)

![$$ =\mathbf{0.022} $$](img/473634_1_En_2_Chapter_TeX_Equbm.png)

隐藏层的第一个神经元的输出:

![$$ {h}_{\mathbf{1} ou}=\frac{\mathbf{1}}{\mathbf{1}+{e}^{-{h}_{\mathbf{1} in}}} $$](img/473634_1_En_2_Chapter_TeX_Equbn.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{0.48}}} $$](img/473634_1_En_2_Chapter_TeX_Equbo.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}+\mathbf{0.619}} $$](img/473634_1_En_2_Chapter_TeX_Equbp.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}.\mathbf{619}} $$](img/473634_1_En_2_Chapter_TeX_Equbq.png)

![$$ {h}_{\mathbf{1} out}=\mathbf{0.618} $$](img/473634_1_En_2_Chapter_TeX_Equbr.png)

以及隐藏层的第二神经元的输出:

![$$ {h}_{\mathbf{2} out}=\frac{\mathbf{1}}{\mathbf{1}+{e}^{-{h}_{\mathbf{2} in}}} $$](img/473634_1_En_2_Chapter_TeX_Equbs.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{0.022}}} $$](img/473634_1_En_2_Chapter_TeX_Equbt.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}+\mathbf{0.978}} $$](img/473634_1_En_2_Chapter_TeX_Equbu.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}.\mathbf{978}} $$](img/473634_1_En_2_Chapter_TeX_Equbv.png)

![$$ {h}_{\mathbf{2} out}=\mathbf{0.506} $$](img/473634_1_En_2_Chapter_TeX_Equbw.png)

下一步是计算输出神经元的输入:

![$$ ou{t}_{in}={h}_{\mathbf{1} out}\ast {W}_{\mathbf{5}}+{h}_{\mathbf{2} out}\ast {W}_{\mathbf{6}}+{b}_{\mathbf{3}} $$](img/473634_1_En_2_Chapter_TeX_Equbx.png)

![$$ =\mathbf{0.618}\ast -\mathbf{0.2}+\mathbf{0.506}\ast \mathbf{0.3}+\mathbf{1.83} $$](img/473634_1_En_2_Chapter_TeX_Equby.png)

![$$ ou{t}_{in}=\mathbf{1.858} $$](img/473634_1_En_2_Chapter_TeX_Equbz.png)

输出神经元的输出:

![$$ ou{t}_{out}=\frac{\mathbf{1}}{\mathbf{1}+{e}^{- ou{t}_{in}}} $$](img/473634_1_En_2_Chapter_TeX_Equca.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{1.858}}} $$](img/473634_1_En_2_Chapter_TeX_Equcb.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}+\mathbf{0.156}} $$](img/473634_1_En_2_Chapter_TeX_Equcc.png)

![$$ =\frac{\mathbf{1}}{\mathbf{1}.\mathbf{156}} $$](img/473634_1_En_2_Chapter_TeX_Equcd.png)

![$$ ou{t}_{out}=\mathbf{0.865} $$](img/473634_1_En_2_Chapter_TeX_Equce.png)

因此，基于当前权重，我们的神经网络的预期输出是 0.865。然后，我们可以根据以下等式计算预测误差:

![$$ E=\frac{\mathbf{1}}{\mathbf{2}}{\left( desired- ou{t}_{out}\right)}^{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equcf.png)

![$$ =\frac{\mathbf{1}}{\mathbf{2}}{\left(\mathbf{0.03}-\mathbf{0.865}\right)}^{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equcg.png)

![$$ =\frac{\mathbf{1}}{\mathbf{2}}{\left(-\mathbf{0.835}\right)}^{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equch.png)

![$$ =\frac{\mathbf{1}}{\mathbf{2}}\left(\mathbf{0.697}\right) $$](img/473634_1_En_2_Chapter_TeX_Equci.png)

![$$ E=\mathbf{0.349} $$](img/473634_1_En_2_Chapter_TeX_Equcj.png)

误差似乎非常大，因此我们应该使用反向传播算法来更新网络权重。

#### 偏导数

我们的目标是得到总误差 ***E*** 如何改变 wrt 的六个权重(***W***<sub>**1**</sub>***:W***<sub>**6**</sub>):

![$$ \frac{\partial E}{\partial {W}_{\mathbf{1}}} $$](img/473634_1_En_2_Chapter_TeX_IEq20.png)、![$$ \frac{\partial E}{\partial {W}_{\mathbf{2}}} $$](img/473634_1_En_2_Chapter_TeX_IEq21.png)、![$$ \frac{\partial E}{\partial {W}_{\mathbf{3}}} $$](img/473634_1_En_2_Chapter_TeX_IEq22.png)、![$$ \frac{\partial E}{\partial {W}_{\mathbf{4}}} $$](img/473634_1_En_2_Chapter_TeX_IEq23.png)、![$$ \frac{\partial E}{\partial {W}_{\mathbf{5}}} $$](img/473634_1_En_2_Chapter_TeX_IEq24.png)、![$$ \frac{\partial E}{\partial {W}_{\mathbf{6}}} $$](img/473634_1_En_2_Chapter_TeX_IEq25.png)

让我们从计算隐藏输出层权重的输出的偏导数开始(***W***<sub>**5**</sub>和***W***<sub>**6**</sub>)。

**E***WT5<sub>5</sub>**偏导数:***

从***W***<sub>**5**</sub>开始，我们将遵循这样的链条:

![$$ \frac{\partial E}{\partial {W}_{\mathbf{5}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial {W}_{\mathbf{5}}} $$](img/473634_1_En_2_Chapter_TeX_Equck.png)

我们可以首先计算每个单独的部分，然后将它们组合起来得到所需的导数。

对于一阶导数![$$ \frac{\partial E}{\partial ou{t}_{out}} $$](img/473634_1_En_2_Chapter_TeX_IEq26.png):

![$$ \frac{\partial E}{\partial ou{t}_{out}}=\frac{\partial }{\partial ou{t}_{out}}\left(\frac{\mathbf{1}}{\mathbf{2}}{\left( desired- ou{t}_{out}\right)}^{\mathbf{2}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equcl.png)

![$$ =\mathbf{2}\ast \frac{\mathbf{1}}{\mathbf{2}}{\left( desired- ou{t}_{out}\right)}^{\mathbf{2}-\mathbf{1}}\ast \left(\mathbf{0}-\mathbf{1}\right) $$](img/473634_1_En_2_Chapter_TeX_Equcm.png)

![$$ = desired- ou{t}_{out}\ast \left(-\mathbf{1}\right) $$](img/473634_1_En_2_Chapter_TeX_Equcn.png)

![$$ = ou{t}_{out}- desired $$](img/473634_1_En_2_Chapter_TeX_Equco.png)

通过替换这些变量的值，

![$$ = ou{t}_{out}- desired=\mathbf{0.865}-\mathbf{0.03} $$](img/473634_1_En_2_Chapter_TeX_Equcp.png)

![$$ \frac{\partial E}{\partial ou{t}_{out}}=\mathbf{0.835} $$](img/473634_1_En_2_Chapter_TeX_Equcq.png)

对于二阶导数![$$ \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}} $$](img/473634_1_En_2_Chapter_TeX_IEq27.png):

![$$ \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}=\frac{\partial }{\partial ou{t}_{in}}\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{- ou{t}_{in}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equcr.png)

![$$ =\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{- ou{t}_{in}}}\right)\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+{e}^{- ou{t}_{in}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equcs.png)

![$$ =\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{1.858}}}\right)\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{1.858}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equct.png)

![$$ =\left(\frac{\mathbf{1}}{\mathbf{1}.\mathbf{56}}\right)\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}.\mathbf{56}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equcu.png)

![$$ =\left(\mathbf{0.865}\right)\left(\mathbf{1}-\mathbf{0.865}\right)=\left(\mathbf{0.865}\right)\left(\mathbf{0.135}\right) $$](img/473634_1_En_2_Chapter_TeX_Equcv.png)

![$$ \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}=\mathbf{0.117} $$](img/473634_1_En_2_Chapter_TeX_Equcw.png)

对于最后一个导数![$$ \frac{\partial ou{t}_{in}}{\partial {W}_5} $$](img/473634_1_En_2_Chapter_TeX_IEq28.png):

![$$ \frac{\partial ou{t}_{in}}{\partial {W}_{\mathbf{5}}}=\frac{\partial }{\partial {W}_{\mathbf{5}}}\left({h}_{\mathbf{1} out}\ast {W}_{\mathbf{5}}+{h}_{\mathbf{2} out}\ast {W}_{\mathbf{6}}+{b}_{\mathbf{3}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equcx.png)

![$$ =\mathbf{1}\ast {h}_{\mathbf{1} out}\ast {\left({W}_{\mathbf{5}}\right)}^{\mathbf{1}-\mathbf{1}}+\mathbf{0}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equcy.png)

![$$ ={h}_{\mathbf{1} out} $$](img/473634_1_En_2_Chapter_TeX_Equcz.png)

![$$ \frac{\partial ou{t}_{in}}{\partial {W}_{\mathbf{5}}}=\mathbf{0.618} $$](img/473634_1_En_2_Chapter_TeX_Equda.png)

计算完所有三个所需的导数后，我们可以计算目标导数，如下所示:

![$$ \frac{\partial E}{\partial {W}_{\mathbf{5}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial {W}_{\mathbf{5}}} $$](img/473634_1_En_2_Chapter_TeX_Equdb.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{5}}}=\mathbf{0.835}\ast \mathbf{0.23}\ast \mathbf{0.618} $$](img/473634_1_En_2_Chapter_TeX_Equdc.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{5}}}=\mathbf{0.119} $$](img/473634_1_En_2_Chapter_TeX_Equdd.png)

**E*****—W***<sub>**6**</sub>**偏导数**:

为了计算![$$ \frac{\partial E}{\partial {W}_6} $$](img/473634_1_En_2_Chapter_TeX_IEq29.png)，我们将使用以下链:

![$$ \frac{\partial E}{\partial {W}_{\mathbf{6}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial {W}_{\mathbf{6}}} $$](img/473634_1_En_2_Chapter_TeX_Equde.png)

将重复相同的计算，仅改变最后一个导数![$$ \frac{\partial ou{t}_{in}}{\partial {W}_6} $$](img/473634_1_En_2_Chapter_TeX_IEq30.png)。它可以计算如下:

![$$ \frac{\partial ou{t}_{in}}{\partial {W}_{\mathbf{6}}}=\frac{\partial }{\partial {W}_{\mathbf{6}}}\left({h}_{\mathbf{1} out}\ast {W}_{\mathbf{5}}+{h}_{\mathbf{2} out}\ast {W}_{\mathbf{6}}+{b}_{\mathbf{3}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equdf.png)

![$$ =\mathbf{0}+\mathbf{1}\ast {h}_{\mathbf{2} out}\ast {\left({W}_{\mathbf{6}}\right)}^{\mathbf{1}-\mathbf{1}}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equdg.png)

![$$ ={h}_{\mathbf{2} out} $$](img/473634_1_En_2_Chapter_TeX_Equdh.png)

![$$ \frac{\partial ou{t}_{in}}{\partial {W}_{\mathbf{6}}}=\mathbf{0.506} $$](img/473634_1_En_2_Chapter_TeX_Equdi.png)

最后，可以计算导数![$$ \frac{\partial E}{\partial {W}_6} $$](img/473634_1_En_2_Chapter_TeX_IEq31.png):

![$$ \frac{\partial E}{\partial {W}_{\mathbf{6}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial {W}_{\mathbf{6}}} $$](img/473634_1_En_2_Chapter_TeX_Equdj.png)

![$$ =\mathbf{0.835}\ast \mathbf{0.23}\ast \mathbf{0.506} $$](img/473634_1_En_2_Chapter_TeX_Equdk.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{6}}}=\mathbf{0.097} $$](img/473634_1_En_2_Chapter_TeX_Equdl.png)

这是给 *W* <sub>5</sub> 和*W*6 的。我们来计算一下对*W*1 到*W*4 的导数 wrt。

**E***—WT5<sub>1</sub>**偏导数:***

从 *W* <sub>1</sub> 开始，我们将遵循这个链条:

![$$ \frac{\partial E}{\partial {W}_{\mathbf{1}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial h{\mathbf{1}}_{out}}\ast \frac{\partial h{\mathbf{1}}_{out}}{\partial h{\mathbf{1}}_{in}}\ast \frac{\partial h{\mathbf{1}}_{in}}{\partial {W}_{\mathbf{1}}} $$](img/473634_1_En_2_Chapter_TeX_Equdm.png)

我们将按照前面的程序，计算每个单独的导数，最后将它们全部组合起来。前面已经计算了前两个导数![$$ \frac{\partial E}{\partial ou{t}_{out}}\ \mathrm{and}\ \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}} $$](img/473634_1_En_2_Chapter_TeX_IEq32.png)，结果如下:

![$$ \frac{\partial E}{\partial ou{t}_{out}}=\mathbf{0.835} $$](img/473634_1_En_2_Chapter_TeX_Equdn.png)

![$$ \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}=\mathbf{0.23} $$](img/473634_1_En_2_Chapter_TeX_Equdo.png)

对于下一个导数![$$ \frac{\partial ou{t}_{in}}{\partial h{1}_{out}} $$](img/473634_1_En_2_Chapter_TeX_IEq33.png):

![$$ \frac{\partial ou{t}_{in}}{\partial h{\mathbf{1}}_{out}}=\frac{\partial }{\partial h{\mathbf{1}}_{out}}\left({h}_{\mathbf{1} out}\ast {W}_{\mathbf{5}}+{h}_{\mathbf{2} out}\ast {W}_{\mathbf{6}}+{b}_{\mathbf{3}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equdp.png)

![$$ ={\left({h}_{\mathbf{1} out}\right)}^{\mathbf{1}-\mathbf{1}}\ast {W}_{\mathbf{5}}+\mathbf{0}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equdq.png)

![$$ ={W}_{\mathbf{5}} $$](img/473634_1_En_2_Chapter_TeX_Equdr.png)

![$$ \frac{\partial ou{t}_{in}}{\partial h{\mathbf{1}}_{out}}=-\mathbf{0.2} $$](img/473634_1_En_2_Chapter_TeX_Equds.png)

对于![$$ \frac{\partial h{1}_{out}}{\partial h{1}_{in}} $$](img/473634_1_En_2_Chapter_TeX_IEq34.png):

![$$ \frac{\partial h{\mathbf{1}}_{out}}{\partial h{\mathbf{1}}_{in}}=\frac{\partial }{\partial {h}_{\mathbf{1} in}}\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{-{h}_{\mathbf{1} in}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equdt.png)

![$$ =\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{-{h}_{\mathbf{1} in}}}\right)\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+{e}^{-{h}_{\mathbf{1} in}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equdu.png)

![$$ =\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{0.48}}}\right)\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{0.48}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equdv.png)

![$$ =\left(\frac{\mathbf{1}}{\mathbf{1}.\mathbf{619}}\right)\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}.\mathbf{619}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equdw.png)

![$$ =\left(\mathbf{0.618}\right)\left(\mathbf{1}-\mathbf{0.618}\right)=\mathbf{0.618}\ast \mathbf{0.382} $$](img/473634_1_En_2_Chapter_TeX_Equdx.png)

![$$ \frac{\partial {h}_{\mathbf{2} out}}{\partial {h}_{\mathbf{2} in}}=\mathbf{0.236} $$](img/473634_1_En_2_Chapter_TeX_Equdy.png)

对于![$$ \frac{\partial h{1}_{in}}{\partial {W}_1} $$](img/473634_1_En_2_Chapter_TeX_IEq35.png):

![$$ \frac{\partial h{\mathbf{1}}_{in}}{\partial {W}_{\mathbf{1}}}=\frac{\partial }{\partial {W}_{\mathbf{1}}}\left({X}_{\mathbf{1}}\ast {W}_{\mathbf{1}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{2}}+{b}_{\mathbf{1}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equdz.png)

![$$ ={X}_{\mathbf{1}}\ast {\left({W}_{\mathbf{1}}\right)}^{\mathbf{1}-\mathbf{1}}+\mathbf{0}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equea.png)

![$$ ={X}_{\mathbf{1}} $$](img/473634_1_En_2_Chapter_TeX_Equeb.png)

![$$ \frac{\partial h{\mathbf{1}}_{in}}{\partial {W}_{\mathbf{1}}}=\mathbf{0.1} $$](img/473634_1_En_2_Chapter_TeX_Equec.png)

最后，可以计算目标导数:

![$$ \frac{\partial E}{\partial {W}_{\mathbf{1}}}=\mathbf{0.835}\ast \mathbf{0.23}\ast -\mathbf{0.2}\ast \mathbf{0.23}\mathbf{6}\ast \mathbf{0.1} $$](img/473634_1_En_2_Chapter_TeX_Equed.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{1}}}=-\mathbf{0.001} $$](img/473634_1_En_2_Chapter_TeX_Equee.png)

**E*****—W***<sub>**2**</sub>**偏导数**:

类似于计算![$$ \frac{\partial E}{\partial {W}_1} $$](img/473634_1_En_2_Chapter_TeX_IEq36.png)的方法，我们可以计算![$$ \frac{\partial E}{\partial {W}_2} $$](img/473634_1_En_2_Chapter_TeX_IEq37.png)。唯一的变化将是在最后一个衍生物![$$ \frac{\partial h{1}_{in}}{\partial {W}_2} $$](img/473634_1_En_2_Chapter_TeX_IEq38.png)。

![$$ \frac{\partial E}{\partial {W}_{\mathbf{2}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial h{\mathbf{1}}_{out}}\ast \frac{\partial h{\mathbf{1}}_{out}}{\partial h{\mathbf{1}}_{in}}\ast \frac{\partial h{\mathbf{1}}_{in}}{\partial {W}_{\mathbf{2}}} $$](img/473634_1_En_2_Chapter_TeX_Equef.png)

![$$ \frac{\partial h{\mathbf{1}}_{in}}{\partial {W}_{\mathbf{2}}}=\frac{\partial }{\partial {W}_{\mathbf{2}}}\left({X}_{\mathbf{1}}\ast {W}_{\mathbf{1}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{2}}+{b}_{\mathbf{1}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equeg.png)

![$$ =\mathbf{0}+{X}_{\mathbf{2}}\ast {\left({W}_{\mathbf{2}}\right)}^{\mathbf{1}-\mathbf{1}}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equeh.png)

![$$ ={X}_{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equei.png)

![$$ \frac{\partial h{\mathbf{1}}_{in}}{\partial {W}_{\mathbf{2}}}=\mathbf{0.3} $$](img/473634_1_En_2_Chapter_TeX_Equej.png)

然后:

![$$ \frac{\partial E}{\partial {W}_{\mathbf{2}}}=\mathbf{0.835}\ast \mathbf{0.23}\ast -\mathbf{0.2}\ast \mathbf{0.23}\mathbf{6}\ast \mathbf{0.3} $$](img/473634_1_En_2_Chapter_TeX_Equek.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{2}}}=-.\mathbf{003} $$](img/473634_1_En_2_Chapter_TeX_Equel.png)

后两个权重( *W* <sub>3</sub> 和 *W* <sub>4</sub> )的计算方法与 *W* <sub>1</sub> 和 *W* <sub>2</sub> 类似。

**E***—WT5<sub>3</sub>**偏导数:***

从 *W* <sub>3</sub> 开始，我们应该遵循这个链条:

![$$ \frac{\partial E}{\partial {W}_{\mathbf{3}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial h{\mathbf{2}}_{out}}\ast \frac{\partial h{\mathbf{2}}_{out}}{\partial h{\mathbf{2}}_{in}}\ast \frac{\partial h{\mathbf{2}}_{in}}{\partial {W}_{\mathbf{3}}} $$](img/473634_1_En_2_Chapter_TeX_Equem.png)

需要计算的缺失导数是![$$ \frac{\partial ou{t}_{in}}{\partial h{2}_{out}},\frac{\partial h{2}_{out}}{\partial h{2}_{in}}\mathrm{and}\frac{\partial h{2}_{in}}{\partial {W}_3} $$](img/473634_1_En_2_Chapter_TeX_IEq39.png)。

![$$ \frac{\partial ou{t}_{in}}{\partial h{\mathbf{2}}_{out}}=\frac{\partial }{\partial h{\mathbf{2}}_{out}}\left({h}_{\mathbf{1} out}\ast {W}_{\mathbf{5}}+{h}_{\mathbf{2} out}\ast {W}_{\mathbf{6}}+{b}_{\mathbf{3}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equen.png)

![$$ =\mathbf{0}+{\left({h}_{\mathbf{2} out}\right)}^{\mathbf{1}-\mathbf{1}}\ast {W}_{\mathbf{6}}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equeo.png)

![$$ ={W}_{\mathbf{6}} $$](img/473634_1_En_2_Chapter_TeX_Equep.png)

![$$ \frac{\partial ou{t}_{in}}{\partial h{\mathbf{2}}_{out}}=\mathbf{0.3} $$](img/473634_1_En_2_Chapter_TeX_Equeq.png)

对于![$$ \frac{\partial h{2}_{out}}{\partial h{2}_{in}} $$](img/473634_1_En_2_Chapter_TeX_IEq40.png):

![$$ \frac{\partial h{\mathbf{2}}_{out}}{\partial h{\mathbf{2}}_{in}}=\frac{\partial }{\partial {h}_{\mathbf{2} in}}\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{-{h}_{\mathbf{2} in}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equer.png)

![$$ =\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{-{h}_{\mathbf{2} in}}}\right)\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+{e}^{-{h}_{\mathbf{2} in}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Eques.png)

![$$ =\left(\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{0.022}}}\right)\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}+{e}^{-\mathbf{0.022}}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equet.png)

![$$ =\left(\frac{\mathbf{1}}{\mathbf{1}.\mathbf{978}}\right)\left(\mathbf{1}-\frac{\mathbf{1}}{\mathbf{1}.\mathbf{978}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equeu.png)

![$$ =\left(\mathbf{0.506}\right)\left(\mathbf{1}-\mathbf{0.506}\right) $$](img/473634_1_En_2_Chapter_TeX_Equev.png)

![$$ \frac{\partial {h}_{\mathbf{2} out}}{\partial {h}_{\mathbf{2} in}}=\mathbf{0.25} $$](img/473634_1_En_2_Chapter_TeX_Equew.png)

对于![$$ \frac{\partial h{2}_{in}}{\partial {W}_3} $$](img/473634_1_En_2_Chapter_TeX_IEq41.png):

![$$ \frac{\partial h{\mathbf{2}}_{in}}{\partial {W}_{\mathbf{3}}}=\frac{\partial }{\partial {W}_{\mathbf{3}}}\left({X}_{\mathbf{1}}\ast {W}_{\mathbf{3}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{4}}+{b}_{\mathbf{2}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equex.png)

![$$ ={X}_{\mathbf{1}}\ast {W}_{\mathbf{3}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{4}}+{b}_{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equey.png)

![$$ ={\left({X}_{\mathbf{1}}\right)}^{\mathbf{1}-\mathbf{1}}\ast {X}_{\mathbf{1}}+\mathbf{0}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equez.png)

![$$ ={X}_{\mathbf{1}} $$](img/473634_1_En_2_Chapter_TeX_Equfa.png)

![$$ =\mathbf{0.1} $$](img/473634_1_En_2_Chapter_TeX_Equfb.png)

最后，我们可以计算所需的导数，如下所示:

![$$ \frac{\partial E}{\partial {W}_{\mathbf{3}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial h{\mathbf{2}}_{out}}\ast \frac{\partial h{\mathbf{2}}_{out}}{\partial h{\mathbf{2}}_{in}}\ast \frac{\partial h{\mathbf{2}}_{in}}{\partial {W}_{\mathbf{3}}} $$](img/473634_1_En_2_Chapter_TeX_Equfc.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{3}}}=\mathbf{0.835}\ast \mathbf{0.23}\ast \mathbf{0.3}\ast \mathbf{0.25}\ast \mathbf{0.1} $$](img/473634_1_En_2_Chapter_TeX_Equfd.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{3}}}=\mathbf{0.00014} $$](img/473634_1_En_2_Chapter_TeX_Equfe.png)

**E***WT5<sub>4</sub>**偏导数:***

我们现在可以类似地计算![$$ \frac{\partial E}{\partial {W}_4} $$](img/473634_1_En_2_Chapter_TeX_IEq42.png):

![$$ \frac{\partial E}{\partial {W}_{\mathbf{4}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial h{\mathbf{2}}_{out}}\ast \frac{\partial h{\mathbf{2}}_{out}}{\partial h{\mathbf{2}}_{in}}\ast \frac{\partial h{\mathbf{2}}_{in}}{\partial {W}_{\mathbf{4}}} $$](img/473634_1_En_2_Chapter_TeX_Equff.png)

我们应该计算缺失的导数![$$ \frac{\partial h{2}_{in}}{\partial {W}_4} $$](img/473634_1_En_2_Chapter_TeX_IEq43.png):

![$$ \frac{\partial h{\mathbf{2}}_{in}}{\partial {W}_{\mathbf{4}}}=\frac{\partial }{\partial {W}_{\mathbf{4}}}\left({X}_{\mathbf{1}}\ast {W}_{\mathbf{3}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{4}}+{b}_{\mathbf{2}}\right) $$](img/473634_1_En_2_Chapter_TeX_Equfg.png)

![$$ ={X}_{\mathbf{1}}\ast {W}_{\mathbf{3}}+{X}_{\mathbf{2}}\ast {W}_{\mathbf{4}}+{b}_{\mathbf{2}} $$](img/473634_1_En_2_Chapter_TeX_Equfh.png)

![$$ =\mathbf{0}+{\left({X}_{\mathbf{2}}\right)}^{\mathbf{1}-\mathbf{1}}\ast {W}_{\mathbf{4}}+\mathbf{0} $$](img/473634_1_En_2_Chapter_TeX_Equfi.png)

![$$ ={W}_{\mathbf{4}} $$](img/473634_1_En_2_Chapter_TeX_Equfj.png)

![$$ =\mathbf{0.2} $$](img/473634_1_En_2_Chapter_TeX_Equfk.png)

然后计算![$$ \frac{\partial E}{\partial {W}_4} $$](img/473634_1_En_2_Chapter_TeX_IEq44.png):

![$$ \frac{\partial E}{\partial {W}_{\mathbf{4}}}=\frac{\partial E}{\partial ou{t}_{out}}\ast \frac{\partial ou{t}_{out}}{\partial ou{t}_{in}}\ast \frac{\partial ou{t}_{in}}{\partial h{\mathbf{2}}_{out}}\ast \frac{\partial h{\mathbf{2}}_{out}}{\partial h{\mathbf{2}}_{in}}\ast \frac{\partial h{\mathbf{2}}_{in}}{\partial {W}_{\mathbf{4}}} $$](img/473634_1_En_2_Chapter_TeX_Equfl.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{4}}}=\mathbf{0.835}\ast \mathbf{0.2}\mathbf{3}\ast \mathbf{0.3}\ast \mathbf{0.2}\mathbf{5}\ast \mathbf{0.2} $$](img/473634_1_En_2_Chapter_TeX_Equfm.png)

![$$ \frac{\partial E}{\partial {W}_{\mathbf{4}}}=.\mathbf{003} $$](img/473634_1_En_2_Chapter_TeX_Equfn.png)

#### 更新权重

至此，我们已经成功地根据网络中的每个权重计算出了总误差的导数。下一步是根据导数更新权重并重新训练网络。更新后的权重将按如下方式计算:

![$$ {W}_{\mathbf{1} new}={W}_{\mathbf{1}}-\eta \ast \frac{\partial E}{\partial {W}_{\mathbf{1}}}=\mathbf{0.5}-.\mathbf{01}\ast -\mathbf{0.001}=\mathbf{0.5}\mathbf{0001} $$](img/473634_1_En_2_Chapter_TeX_Equfo.png)

![$$ {W}_{\mathbf{2} new}={W}_{\mathbf{2}}-\eta \ast \frac{\partial E}{\partial {W}_{\mathbf{2}}}=\mathbf{0.1}-.\mathbf{01}\ast -\mathbf{0.003}=\mathbf{0.1}\mathbf{0003} $$](img/473634_1_En_2_Chapter_TeX_Equfp.png)

![$$ {W}_{\mathbf{3} new}={W}_{\mathbf{3}}-\eta \ast \frac{\partial E}{\partial {W}_{\mathbf{3}}}=\mathbf{0.62}-.\mathbf{01}\ast \mathbf{0.00014}=\mathbf{0.6199} $$](img/473634_1_En_2_Chapter_TeX_Equfq.png)

![$$ {W}_{\mathbf{4} new}={W}_{\mathbf{4}}-\eta \ast \frac{\partial E}{\partial {W}_{\mathbf{4}}}=\mathbf{0.2}-.\mathbf{01}\ast \mathbf{0.003}=\mathbf{0.1997} $$](img/473634_1_En_2_Chapter_TeX_Equfr.png)

![$$ {W}_{\mathbf{5} new}={W}_{\mathbf{5}}-\eta \ast \frac{\partial E}{\partial {W}_{\mathbf{5}}}=-\mathbf{0.2}-.\mathbf{01}\ast \mathbf{0.618}=-\mathbf{0.2}\mathbf{0618} $$](img/473634_1_En_2_Chapter_TeX_Equfs.png)

![$$ {W}_{\mathbf{6} new}={W}_{\mathbf{6}}-\eta \ast \frac{\partial E}{\partial {W}_{\mathbf{6}}}=\mathbf{0.3}-.\mathbf{01}\ast \mathbf{0.097}=\mathbf{0.29903} $$](img/473634_1_En_2_Chapter_TeX_Equft.png)

## 过度拟合

您是否曾经创建过一个 ML 模型，它对于训练样本来说是完美的，但是对于看不见的样本却给出了非常糟糕的预测？你想过为什么会这样吗？原因可能是过度拟合。有过拟合问题的模型对训练样本的预测很好，但对验证数据的预测很差。这是因为该模型使其自身适应训练数据中的每条信息，直到收集到只能在训练数据中找到的一些属性。让我们试着理解这个问题。

ML 的重点是用训练数据训练算法，以便创建能够对看不见的数据(测试数据)做出正确预测的模型。例如，为了创建分类器，人类专家将从收集训练 ML 算法所需的数据开始。人类负责寻找最佳类型的特征，这些特征能够区分不同的类别，以便代表每个类别。这些特征将用于训练 ML 算法。假设我们要建立一个 ML 模型，将图 2-16 中的图像分类为包含或不包含猫。

![img/473634_1_En_2_Fig16_HTML.jpg](img/473634_1_En_2_Fig16_HTML.jpg)

图 2-16

训练模型的猫的图像

我们要回答的第一个问题是“使用什么功能最好？”这是 ML 中的一个关键问题，因为使用的特征越好，训练的 ML 模型做出的预测就越好，反之亦然。让我们试着将这些图像形象化，并提取一些代表猫的特征。一些代表性的特征可能是存在两个深色的瞳孔和两个对角线方向的耳朵。让我们假设我们已经以某种方式从前面的训练图像中提取了特征，并且已经创建了训练的 ML 模型。这个模型可以处理各种各样的猫图像，因为所使用的特征存在于大多数猫中。我们可以使用一些看不见的数据来测试模型，如图 2-17 所示。假设测试数据的分类准确率为 **x%** 。

![img/473634_1_En_2_Fig17_HTML.jpg](img/473634_1_En_2_Fig17_HTML.jpg)

图 2-17

猫的测试图像

人们可能希望提高分类精度。首先要考虑的是使用比以前更多的功能。这是因为使用的区别特征越多，准确度就越高。通过再次检查训练数据，我们可以发现更多的特征，例如整体图像颜色，因为所有训练猫样本都是白色的，而训练数据中的虹膜颜色是黄色的。特征向量将具有这四个特征:

1.  深色瞳孔

2.  对角耳朵

3.  白色毛皮

4.  黄色鸢尾花

它们将用于重新训练 ML 模型。

创建训练好的模型后，下一步是测试它。使用新的特征向量后的预期结果是分类精度将下降到小于 **x%** 。但是为什么呢？准确性下降的原因是使用了一些已经存在于训练数据中但并非普遍存在于所有 cat 图像中的特征。这些特征并不是所有猫图像的共性。在检测数据中，有些猫的皮毛是黑色或黄色的，而不是训练中使用的白色皮毛。

在我们的例子中，所使用的特征对于训练样本来说是强有力的，但是对于测试样本来说是非常差的，这可以被描述为过度拟合。该模型用一些特征来训练，这些特征是训练数据所独有的，但不存在于测试数据中。

前面讨论的目的是通过使用一个高层次的例子来简化过度拟合的概念。要了解细节，最好用一个更简单的例子。这就是为什么接下来的讨论将基于回归示例。

### 基于回归示例理解正则化

假设我们想要创建一个回归模型来拟合图 2-18 中所示的数据。我们可以使用多项式回归。

![img/473634_1_En_2_Fig18_HTML.png](img/473634_1_En_2_Fig18_HTML.png)

图 2-18

拟合回归模型的数据

我们可以从一个一次多项式方程的线性模型开始，如方程 2-13 所示。

y<sub>1</sub>= f<sub>1</sub>(x)=θ<sub>1</sub>x+θ<sub>0</sub>(等式 2-13)

其中θ<sub>0</sub>和θ<sub>1</sub>是模型参数， *x* 是唯一使用的特征。

前一型号的图如图 2-19 所示。

![img/473634_1_En_2_Fig19_HTML.png](img/473634_1_En_2_Fig19_HTML.png)

图 2-19

使用一级模型拟合数据的初始模型

根据损失函数，如等式 2-14 中的损失函数，我们可以得出结论，该模型不适合数据。

![$$ L=\frac{\sum_{i=0}^N\mid {f}_1\left({x}_i\right)-{d}_i\mid }{N} $$](img/473634_1_En_2_Chapter_TeX_IEq45.png)(方程式 2-14)

其中 f<sub>*I*</sub>(x<sub>*I*</sub>是样本 *i* 的期望输出，d <sub>*i*</sub> 是同一样本的期望输出。

模型过于简单，有很多预测都不准确。由于这个原因，我们应该创建一个更复杂的模型，它可以很好地拟合数据，我们可以将方程的次数从一次增加到二次，如方程 2-15 所示。

y<sub>2</sub>= f<sub>1</sub>(x)=θ<sub>2</sub>x<sup>2</sup>+θ<sub>1</sub>x+θ<sub>0</sub>(等式 2-15)

通过使用相同的特征 x 的 2 次方(x <sup>2</sup> )，我们创建了一个新的特征，我们将不仅捕获数据的线性属性，还捕获一些非线性属性。新模型的图形如图 2-20 所示。

![img/473634_1_En_2_Fig20_HTML.png](img/473634_1_En_2_Fig20_HTML.png)

图 2-20

使用更多特征来创建二次模型

该图显示，二次多项式比一次多项式更适合数据。但是二次方程也不太适合一些数据样本。这就是为什么我们可以用方程 2-16 建立一个更复杂的三次模型。该图如图 2-21 所示。

![img/473634_1_En_2_Fig21_HTML.png](img/473634_1_En_2_Fig21_HTML.png)

图 2-21

三次模型

y<sub>3</sub>= f<sub>3</sub>(x)=θ<sub>3</sub>x<sup>3</sup>+2x2

可以注意到，在添加了捕获三次数据属性的新特征之后，模型更好地拟合了数据。为了比以前更好地拟合数据，我们可以将方程的次数增加到四次，如方程 2-17 所示。该图如图 2-22 所示。

y<sub>4</sub>= f<sub>4</sub>(x)=θ<sub>4</sub>x<sup>4</sup>+3x3

![img/473634_1_En_2_Fig22_HTML.png](img/473634_1_En_2_Fig22_HTML.png)

图 2-22

四阶模型

似乎多项式方程的次数越高，就越符合数据。但是有一些重要的问题需要回答。如果通过添加新特征来增加多项式方程的次数可以增强结果，为什么不应该使用非常高的次数，例如 100 次<sup>次</sup>次？对于一个问题，用什么度最好？

### 模型容量/复杂性

术语“模型容量/复杂性”指的是模型可以处理的变化程度。容量越高，模型能够应对的变化就越多。第一款 y <sub>1</sub> 据说比 y <sub>4</sub> 容量小。在我们的例子中，容量随着多项式次数的增加而增加。

当然，多项式方程的次数越高，它就越适合数据。但是请记住，增加多项式次数会增加模型的复杂性。使用容量高于所需容量的模型可能会导致过度拟合。该模型变得非常复杂，并且非常适合训练数据，但是不幸的是对于看不见的数据来说非常弱。ML 的目标是创建一个模型，该模型不仅对训练数据而且对看不见的数据样本都是健壮的。

四度(y <sub>4</sub> 的模型很复杂。是的，它很好地适应了可见的数据，但是对于不可见的数据却不是这样。对于这种情况，y <sub>4</sub> 中新使用的特征，即*x*4，捕获了比所需更多的细节。因为这个新特性使得模型过于复杂，我们应该去掉它。

在这个例子中，我们实际上知道要删除哪些特性。所以，我们可以去掉它们，回到之前的三次模型(θ<sub>4</sub>x<sup>4</sup>+θ<sub>3</sub>x<sup>3</sup>+θ<sub>2</sub>x<sup>2</sup>+θ<sub>1</sub>x+θ<sub>0</sub>)。但是在实际工作中，我们不知道要删除哪些特征。此外，假设新特性不太糟糕，我们不想完全删除它，只想惩罚它。我们做什么呢

回头看损失函数，唯一的目标是最小化/惩罚预测误差。我们可以设定一个新的目标，尽可能地最小化/惩罚新特性*x*?? 4 的影响。在修改损失函数来惩罚 x <sup>3</sup> 之后，新的在等式 2-18 中。

![$$ {L}_{new}=\frac{\left[{\sum}_{i=0}^N|{f}_4\left({x}_i\right)-{d}_i|+{\varTheta}_4{x}⁴\right]}{N} $$](img/473634_1_En_2_Chapter_TeX_IEq46.png)(方程式 2-18)

我们现在的目标是最小化损失函数。我们现在只对最小化这一项θ<sub>4</sub>x<sup>4</sup>感兴趣。显然，为了最小化θ<sub>4</sub>x<sup>4</sup>，我们应该最小化θ<sub>4</sub>，因为它是我们唯一可以改变的自由参数。如果我们想完全去掉这个特征，以防它是一个非常糟糕的特征，我们可以把它的值设置为零，如等式 2-19 所示。

![$$ {L}_{new}=\frac{\left[{\sum}_{i=0}^N|{f}_4\left({x}_i\right)-{d}_i|+0\ast {x}⁴\right]}{N} $$](img/473634_1_En_2_Chapter_TeX_IEq47.png)(方程式 2-19)

去掉它，我们回到三次多项式方程(y <sub>3</sub> )。y <sub>3</sub> 并不像 y <sub>4</sub> 那样完美地拟合可见数据，但一般来说，它会比 y <sub>4</sub> 对不可见数据给出更好的性能。

但是如果 x <sup>4</sup> 是一个相对较好的特征，我们只是想惩罚它而不是完全删除它，我们可以将它设置为一个接近但不为零的值(比如 0.1)，如等式 2-20 所示。通过这样做，我们限制了 x <sup>4</sup> 的影响。因此，新模型不会像以前那样复杂。

![$$ {L}_{new}=\frac{\left[{\sum}_{i=0}^N|{f}_4\left({x}_i\right)-{d}_i|+0.1\ast {x}⁴\right]}{N} $$](img/473634_1_En_2_Chapter_TeX_IEq48.png)(方程式 2-20)

回到 y <sub>2</sub> ，好像比 y <sub>3</sub> 简单。它可以很好地处理可见和不可见的数据样本。所以，我们应该删除 y <sub>3</sub> 中使用的新功能，也就是 x <sup>3</sup> ，或者如果它做得相对好就惩罚它。我们可以修改损失函数来做到这一点，如方程 2-21 所示。

![$$ {L}_{new}=\frac{\left[{\sum}_{i=0}^N|{f}_4\left({x}_i\right)-{d}_i|+0.1\ast {x}⁴+{\varTheta}_3{x}³\right]}{N} $$](img/473634_1_En_2_Chapter_TeX_Equfu.png)

![$$ {L}_{new}=\frac{\left[{\sum}_{i=0}^N|{f}_4\left({x}_i\right)-{d}_i|+0.1\ast {x}⁴+0.04\ast {x}³\right]}{N} $$](img/473634_1_En_2_Chapter_TeX_IEq49.png)(方程式 2-21)

### L1 正则化

注意，我们实际上知道 y <sub>2</sub> 是拟合数据的最佳模型，因为数据图对我们来说是可用的。这是一个非常简单的任务，我们可以手动解决。但是，如果我们无法获得这些信息，随着样本数量和数据复杂性的增加，我们将无法轻易得出这样的结论。必须有一些自动的东西来告诉我们哪种程度适合数据，并告诉我们要惩罚哪些特征来获得对看不见的数据的最佳预测。这就是正规化。

正则化有助于我们选择适合数据的模型复杂度。自动惩罚使模型过于复杂的特征是很有用的。请记住，如果特征不差，正则化是有用的，它将帮助我们在相对意义上获得良好的预测；我们只需要惩罚他们，而不是完全消除他们。正则化会惩罚所有使用的要素，而不是选定的子集。之前，我们只惩罚了两个特征，x <sup>4</sup> 和 x <sup>3</sup> ，而不是所有的特征。但正规化就不是这样了。

使用正则化，一个新的项被添加到损失函数中以惩罚特征，因此损失函数将如等式 2-22 所示。

![$$ {L}_{new}=\frac{\left[{\sum}_{i=0}^N|{f}_4\left({x}_i\right)-{d}_i|+{\sum}_{j=1}^N\lambda {\varTheta}_j\right]}{N} $$](img/473634_1_En_2_Chapter_TeX_IEq50.png)(方程式 2-22)

将λ移出总和后，也可以写成等式 2-23。

![$$ {L}_{new}=\frac{\left[{\sum}_{i=0}^N|{f}_4\left({x}_i\right)-{d}_i|+\lambda {\sum}_{j=1}^N{\varTheta}_j\right]}{N} $$](img/473634_1_En_2_Chapter_TeX_IEq51.png)(方程式 2-23)

新增加的术语![$$ \lambda \sum \limits_{j=1}^N{\varTheta}_j $$](img/473634_1_En_2_Chapter_TeX_IEq52.png)用于惩罚特征，以控制模型的复杂程度。在加入正则项之前，我们之前的目标是尽可能减小预测误差。现在我们的目标是最小化误差，但要小心不要让模型太复杂，避免过度拟合。

有一个称为 lambda (λ)的正则化参数，用于控制如何惩罚要素。它是一个没有固定值的超参数。它的值根据手头的任务是可变的。随着其值的增加，将有更高的惩罚功能。因此，模型变得更简单。当它的值降低时，特征的惩罚将降低，因此模型复杂性增加。值为零表示完全不移除特征。

当λ为零时，那么θ<sub>*j*</sub>的值根本不会被罚，如下式所示。这是因为将λ设置为零意味着去除正则化项，只留下误差项。因此，我们的目标将返回到将误差最小化到接近于零。当以误差最小化为目标时，模型可能会过拟合。

![$$ {L}_{new}=\frac{\left[{\sum}_{i=0}^N|{f}_4\left({x}_i\right)-{d}_i|+0\ast {\sum}_{j=1}^N{\varTheta}_j\right]}{N} $$](img/473634_1_En_2_Chapter_TeX_Equfv.png)

![$$ {L}_{new}=\frac{\left[{\sum}_{i=0}^N|{f}_4\left({x}_i\right)-{d}_i|+0\right]}{N} $$](img/473634_1_En_2_Chapter_TeX_Equfw.png)

![$$ {L}_{new}=\frac{\sum_{i=0}^N\mid {f}_4\left({x}_i\right)-{d}_i\mid }{N} $$](img/473634_1_En_2_Chapter_TeX_Equfx.png)

但是当惩罚参数λ的值很高时(比如 10 <sup>9</sup> ，那么为了使损失保持在最小值，参数θ<sub>*j*</sub>必须有很高的惩罚。因此，参数θ<sub>*j*</sub>将为零。因此，模型(y <sub>4</sub> )的θ<sub>*I*</sub>将被修剪，如下所示。

![$$ {y}_4={f}_4(x)={\varTheta}_4{x}⁴+{\varTheta}_3{x}³+{\varTheta}_2{x}²+{\varTheta}_1x+{\varTheta}_0 $$](img/473634_1_En_2_Chapter_TeX_Equfy.png)

![$$ {y}_4=0\ast {x}⁴+0\ast {x}³+0\ast {x}²+0\ast x+{\varTheta}_0 $$](img/473634_1_En_2_Chapter_TeX_Equfz.png)

![$$ {y}_4={\varTheta}_0 $$](img/473634_1_En_2_Chapter_TeX_Equga.png)

请注意，正则项从 1 开始其索引 *j* 不为零。实际上，我们用正则项来惩罚特征(x<sub>IT5)。因为θ<sub>0</sub>没有关联特征，所以没有理由惩罚它。这种情况下，模型为 y<sub>4</sub>=θ<sub>0</sub>，图形如图 2-23 所示。</sub>

![img/473634_1_En_2_Fig23_HTML.png](img/473634_1_En_2_Fig23_HTML.png)

图 2-23

惩罚所有特征后平行于 x 轴的模型

## 设计人工神经网络

人工神经网络的初学者可能会问一些问题，包括以下问题:使用多少个隐藏层是正确的？每个隐藏层有多少个隐藏神经元？使用隐藏层/神经元的目的是什么？增加隐藏层/神经元的数量是否总能得到更好的结果？我很高兴地说，我们可以回答这些问题。明确地说，如果要解决的问题很复杂，回答这样的问题可能会太复杂。在本节结束时，你至少可以知道如何回答这些问题，并能够通过简单的例子来测试自己。我们开始吧。

ANN 的灵感来自于生物神经网络。为简单起见，在计算机科学中，它被表示为一组层。这些层分为三类:输入、隐藏和输出。

知道输入和输出层的数量及其神经元的数量是最容易的部分。每个网络都有单一的输入和输出层。输入层中神经元的数量等于正在处理的数据中输入变量的数量。输出层中神经元的数量等于与每个输入相关联的输出的数量。但是挑战在于知道隐藏层及其神经元的数量。

以下是学习分类问题中隐藏层和每个隐藏层中神经元数量的一些指导原则:

*   根据这些数据，画出一个预期的决策边界来分隔这些类。

*   将决策边界表示为一组线。请注意，这些线的组合必须服从决策边界。

*   选定线的数量表示第一个隐藏层中隐藏神经元的数量。

*   为了连接由前一层创建的线，添加了一个新的隐藏层。请注意，每次需要在前一个隐藏层中的线条之间创建连接时，都会添加一个新的隐藏层。

*   每个新隐藏层中隐藏神经元的数量等于要建立的连接的数量。

为了让事情更清楚，让我们将前面的准则应用到几个例子中。

### 示例 1:无隐藏层的人工神经网络

先说一个简单的两类分类问题的例子，如图 2-24 所示。每个样本有两个输入和一个表示类标签的输出。这与异或问题非常相似。

![img/473634_1_En_2_Fig24_HTML.png](img/473634_1_En_2_Fig24_HTML.png)

图 2-24

两类分类问题

要回答的第一个问题是是否需要隐藏层。确定这一点要遵循的规则如下:

*   在人工神经网络中，当且仅当数据必须非线性分离时，才需要隐藏层。

查看图 2-25 ，似乎类别必须非线性分离。单行不行。因此，我们必须使用隐藏层，以获得最佳决策边界。在这种情况下，我们可能仍然不使用隐藏层，但这会影响分类精度。所以，最好使用隐藏层。

知道我们需要隐藏层之后，我们需要回答两个重要的问题。这些问题如下:

1.  所需的隐藏层数是多少？

2.  每个隐藏层中隐藏神经元的数量是多少？

按照前面的过程，第一步是绘制一个划分两个类的决策边界。正确拆分数据的可能决策边界不止一个，如图 2-25 所示。我们将用于进一步讨论的是图 2-25(a) 。

![img/473634_1_En_2_Fig25_HTML.png](img/473634_1_En_2_Fig25_HTML.png)

图 2-25

非线性分类问题不能用一条线来解决

按照指导方针，下一步是用一组线来表达决策边界。

使用一组线来表示决策边界的想法来自于这样一个事实，即任何人工神经网络都是使用单层感知器作为构建块来构建的。单层感知器是一个线性分类器，它使用根据等式 2-24 创建的线来分离类别。

**和**=***【w】******x*****

 ****其中***x***<sub>***I***</sub>是输入的****，***w***<sub>***I***</sub>是其权重， ***b*** 是偏差，而因为每个添加的隐藏神经元都会增加权重的数量，所以建议使用完成任务的最小数量的隐藏神经元。使用比所需更多的隐藏神经元将增加更多的复杂性。****

 **回到我们的例子，说 ANN 是使用多个感知器网络构建的，等同于说网络是使用多条线构建的。

在这个例子中，判定边界由一组线代替。这些线从边界曲线改变方向的点开始。此时，放置了两条线，每条线的方向都不同。

因为边界曲线只有一个点改变方向，如图 2-26 中灰色圆圈所示，那么就只需要两条线。换句话说，有两个单层感知器网络。每个感知器产生一条线。

![img/473634_1_En_2_Fig26_HTML.png](img/473634_1_En_2_Fig26_HTML.png)

图 2-26

对问题进行分类需要两行

知道只需要两条线来表示决策边界告诉我们，第一个隐藏层将有两个隐藏神经元。

到目前为止，我们有一个带有两个隐藏神经元的隐藏层。每个隐藏的神经元可以被视为一个线性分类器，用一条线来表示，如图 2-26 所示。将有两个输出，一个来自每个分类器(即，隐藏神经元)。但是我们要构建一个单一的分类器，用一个输出表示类标签，而不是两个分类器。结果，两个隐藏神经元的输出将被合并成单个输出。换句话说，这两条线将由另一个神经元连接。结果如图 2-27 所示。

![img/473634_1_En_2_Fig27_HTML.png](img/473634_1_En_2_Fig27_HTML.png)

图 2-27

用一个隐藏的神经元把两条线连接起来

幸运的是，我们不需要添加另一个具有单个神经元的隐藏层来完成这项工作。输出层神经元将完成这项任务。这个神经元将合并先前生成的两条线，以便网络只有一个输出。

学习了隐含层及其神经元的数目后，网络架构现在就完成了，如图 2-28 所示。

![img/473634_1_En_2_Fig28_HTML.png](img/473634_1_En_2_Fig28_HTML.png)

图 2-28

分类问题的网络结构，其中曲线是通过连接两条线创建的，每条线都是使用隐藏层神经元创建的

### 示例 2:具有单一隐藏层的人工神经网络

另一个分类示例如图 2-29 所示。它类似于前面的例子，其中有两个类，每个样本有两个输入和一个输出。区别在于决策边界。本例中的边界比上例中的边界更复杂。

![img/473634_1_En_2_Fig29_HTML.png](img/473634_1_En_2_Fig29_HTML.png)

图 2-29

寻找最佳网络架构的更复杂的分类问题

根据指导方针，第一步是划定决策边界。我们讨论中使用的决策边界如图 2-30(a) 所示。

下一步是将决策边界分割成一组线；每条线将被模拟为人工神经网络中的感知器。在画线之前，边界改变方向的点要做好标记，如图 2-30(b) 所示。

![img/473634_1_En_2_Fig30_HTML.png](img/473634_1_En_2_Fig30_HTML.png)

图 2-30

对第二个示例进行分类的决策边界

问题是需要多少行。每个顶点和底点将有两条线与之相关联，总共四条线。中间点的两条线将与其他点共享。要创建的线如图 2-31 所示。

![img/473634_1_En_2_Fig31_HTML.png](img/473634_1_En_2_Fig31_HTML.png)

图 2-31

创建第二个示例的决策边界所需的行

因为第一隐藏层将具有与行数相等的隐藏层神经元，所以第一隐藏层将具有四个神经元。换句话说，有四个分类器，每个都由一个单层感知器创建。目前，网络将产生四个输出，每个分类器一个。下一步是将这些分类器连接在一起，以使网络只生成一个输出。换句话说，这些线将通过其他隐藏层连接在一起，以生成一条曲线。

由模型设计者来选择网络的布局。一种可行的网络架构是建立具有两个隐藏神经元的第二隐藏层。第一个隐藏神经元将连接前两条线，最后一个隐藏神经元将连接后两条线。第二次隐藏层的结果如图 2-32 所示。

![img/473634_1_En_2_Fig32_HTML.png](img/473634_1_En_2_Fig32_HTML.png)

图 2-32

连线以创建单个决策边界

到目前为止，已经有两条独立的曲线。因此，网络有两个输出。下一步是将这些曲线连接在一起，以便整个网络只有一个输出。在这种情况下，输出层神经元可以用于进行最终连接，而不是添加新的隐藏层。最终结果如图 2-33 所示。

![img/473634_1_En_2_Fig33_HTML.png](img/473634_1_En_2_Fig33_HTML.png)

图 2-33

使用输出层连接隐藏层的输出

网络设计现已完成，完整的网络架构如图 2-34 所示。

![img/473634_1_En_2_Fig34_HTML.png](img/473634_1_En_2_Fig34_HTML.png)

图 2-34

对第二个例子进行分类的网络架构*****************************************